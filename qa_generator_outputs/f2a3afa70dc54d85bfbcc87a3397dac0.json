[
    {
        "question_answer": {
            "question": "What is Cribl Stream and what does it do?",
            "answer": "Cribl Stream is a tool that allows you to capture, refine, and shrink log data. It provides built-in functions for redacting, parsing, and refining data. It's designed to help you manage and optimize your data flow, with features like Sources, Routes, Pipelines, and Destinations.",
            "follow_up_question_1": "What are these features like Sources, Routes, Pipelines, and Destinations?",
            "follow_up_answer_1": "In Cribl Stream, a Source represents a data input, which could be from various platforms like Splunk, Elastic Beats, Kinesis, Kafka, syslog, HTTP, TCP JSON, and others. A Route is a path that data takes from a Source to a Destination. A Pipeline is a stack of functions that process data, and a Destination represents a data output, which could be various platforms like Splunk, Kafka, Kinesis, InfluxDB, Snowflake, Databricks, TCP JSON, and others.",
            "follow_up_question_2": "Can you explain more about how Pipelines work?",
            "follow_up_answer_2": "Sure, a Pipeline in Cribl Stream is a stack of functions that process data. These functions are central to refining your data. When you set up a Pipeline, you can preview data, add functions, and attach a Route. The functions in a Pipeline process the data in sequence, allowing you to refine the raw data as it flows through.",
            "follow_up_question_3": "What kind of functions can I add to a Pipeline?",
            "follow_up_answer_3": "Cribl Stream provides a variety of built-in functions that you can add to a Pipeline. These functions allow you to perform various operations on your data, such as redacting sensitive information, parsing complex data structures, refining data to improve its quality, and shrinking data to reduce its size.",
            "url": "https://docs.cribl.io/docs/getting-started-guide"
        },
        "reference": "url,title,text_chunks https://docs.cribl.io/docs/getting-started-guide,Getting Started Guide | Cribl Docs,\"This guide walks you through planning, (optionally) installing, and configuring a basic deployment of Cribl\u00c2 Stream. You\u2019ll capture some realistic sample log data, and then use Cribl\u00c2 Stream\u2019s built-in Functions to redact, parse, refine, and shrink the data. By the end of this guide, you\u2019ll have assembled all of Cribl\u00c2 Stream\u2019s basic building blocks: a Source, Route, Pipeline, several Functions, and a Destination. You can complete this tutorial using Cribl\u00c2 Stream\u2019s included sample data, without connections to \u00e2\u0080\u0093 or licenses on \u00e2\u0080\u0093 any inbound or outbound services. Assuming a cold start (from first-time setup of a Cribl.Cloud or self-hosted instance), this guide might take about an hour. But you\u00c2 can work through it in chunks, and Cribl\u00c2 Stream will persist your work between sessions. If you\u2019ve already launched a Cribl\u00c2 Stream instance (either Cribl.Cloud or self-hosted), skip ahead to Get\u00c2 Data\u00c2 Flowing. If you\u2019d prefer a similar tutorial based on Cribl\u00c2 Stream\u2019s simplified, visual QuickConnect UI, see our CrowdStream setup guide. All the steps shown there work in Cribl.Cloud as well. Once you\u2019ve mastered all the techniques in this tutorial, check out its Distributed\u00c2 Quick\u00c2 Start successor. To quote Robert Frost and Robert Plant (of Led\u00c2 Zeppelin), there are two paths that you can go by: Do this tutorial with a free Cribl.Cloud instance, hosted by Cribl. Follow the registration instructions in the section just below. This\u00c2 skips you past all the requirements and installation sections below \u00e2\u0080\u0093 you\u2019ll have nothing to install or host. Because Cribl.Cloud always runs in distributed\u00c2 mode, this will require a few extra clicks (all clearly labeled) later in the tutorial\u2019s body. But it will give you immediate experience with Cribl\u00c2 Stream\u2019s typical production mode. Do this tutorial by downloading and installing Cribl\u00c2 Stream software on your own hardware or virtual machine. Follow the Requirements and Download and Install instructions below. You\u2019ll need to provide your own infrastructure. But if you\u2019re planning to use self-hosted/on-prem Cribl\u00c2 Stream in production, this will walk you through its realistic setup. To fully experience a real-world, self-hosted production setup, you can switch your Cribl\u00c2 Stream installation to distributed\u00c2 mode. And then chase this tutorial with our Distributed\u00c2 Quick\u00c2 Start, which fully exercises distributed mode\u2019s multiple Workers and Worker\u00c2 Groups. As indicated just above, you can skip installing Cribl\u00c2 Stream software \u00e2\u0080\u0093 and skip this tutorial\u2019s next several sections \u00e2\u0080\u0093 by registering a free Cribl.Cloud instance. Cribl will quickly spin up a fully functioning copy of Cribl\u00c2 Stream for you, and manage it on your behalf. To use this fastest option: Really, skip all the other sections of the linked Cloud Guide, and all this tutorial\u2019s sections between here and Get\u00c2 Data\u00c2 Flowing. We\u00c2 told you this was a quick start! The minimum requirements for running this tutorial are the same as for a Cribl\u00c2 Stream production single-instance deployment. Example distributions: Ubuntu\u00c2 16.04, Debian\u00c2 9, RHEL\u00c2 7, CentOS\u00c2 Linux 7, 8, or CentOS Stream 9, SUSE\u00c2 Linux Enterprise\u00c2 Server 12, Amazon\u00c2 Linux 2. Tested so far on Ubuntu (14.04, 16.04, 18.04, and 20.04), CentOS 7.9, and Amazon Linux 2. We assume that 1 physical core is equivalent to 2\u00c2 virtual/\u00e2\u0080\u008bhyperthreaded CPUs (vCPUs) on Intel/Xeon or AMD processors; and to 1\u00c2 (higher-throughput) vCPU on Graviton2/ARM64 processors. By default, Cribl\u00c2 Stream listens on the following ports: You can override these defaults as needed. For higher processing volumes, users typically enable Cribl\u00c2 Stream\u2019s Distributed\u00c2 Deployment option. While beyond the scope of this tutorial, that option has a few additional requirements, which we list here for planning purposes: See Sizing and Scaling for further details about configuring Cribl\u00c2 Stream to handle large data streams. To avoid permissions errors, you should both install and run (next section) Cribl\u00c2 Stream as the same Linux user. For\u00c2 details on creating a new user (addressing both systemd and initd distro\u2019s), see Enabling\u00c2 Start\u00c2 on\u00c2 Boot. Download the latest version of Cribl\u00c2 Stream at https://cribl.io/download/. Un-tar the resulting .tgz file in a directory of your choice (e.g., /opt/). Here\u2019s general syntax, then a specific example: You\u2019ll now have Cribl\u00c2 Stream installed in a cribl subdirectory, by default: /opt/cribl/. We\u2019ll\u00c2 refer to this cribl subdirectory throughout this documentation as $CRIBL_HOME. In your terminal, switch to the $CRIBL_HOME/bin directory (e.g,: /opt/cribl/bin). Here, you can start, stop, and verify the Cribl\u00c2 Stream server using these basic ./cribl CLI commands: For other available commands, see CLI\u00c2 Reference. Next, in your browser, open http://<hostname>:9000 (e.g., http://localhost:9000) and log in with default credentials (admin, admin). Register your copy of Cribl\u00c2 Stream when prompted. After registering, you\u2019ll be prompted to change the default password. That\u2019s it! With Cribl\u00c2 Stream now running \u00e2\u0080\u0093 either in Cribl.Cloud or in your self-hosted copy \u00e2\u0080\u0093 you\u2019re ready to configure a working Cribl\u00c2 Stream deployment. You\u2019ll set up a Source, Destination, Pipeline, and Route, and will assemble several built-in Functions to refine sample log data. Each Cribl\u00c2 Stream Source represents a data input. Options include Splunk, Elastic Beats, Kinesis, Kafka, syslog, HTTP, TCP\u00c2 JSON, and others. For this tutorial, we\u2019ll enable a Cribl\u00c2 Stream built-in datagen (i.e., data generator) that generates a stream of realistic sample log data. If you\u2019re on Cribl.Cloud or any other distributed mode, first click the top nav\u2019s Manage tab to select the default (or another) Worker Group. From Cribl\u00c2 Stream\u2019s Manage submenu, select Data > Sources. From the Data\u00c2 Sources page\u2019s tiles or left menu, select Datagen. (You can use the search box to jump to the Datagen tile.) Click Add\u00c2 Source to open the New\u00c2 Source modal. In the Input ID field, name this Source businessevent. In the Data Generator File drop-down, select businessevent.log. This generates\u2026log events for a business scenario. We\u2019ll look at their structure shortly, in Capture\u00c2 and Filter\u00c2 Sample Data. Click Save. If you\u2019re on Cribl.Cloud or any other distributed mode, click Commit\u00c2 &\u00c2 Deploy at Cribl\u00c2 Stream\u2019s upper right before proceeding. Then, in the resulting dialog box, click Commit\u00c2 &\u00c2 Deploy to confirm. You\u2019ll see a Commit\u00c2 successful message. The Yes toggle in the Enabled column indicates that your Datagen Source has started generating sample data. Each Cribl\u00c2 Stream Destination represents a data output. Options include Splunk, Kafka, Kinesis, InfluxDB, Snowflake, Databricks, TCP JSON, and others. For this tutorial, we\u2019ll use Cribl\u00c2 Stream\u2019s built-in DevNull Destination. This simply discards events \u00e2\u0080\u0093 not very exciting! But it simulates a real output, so it provides a configuration-free quick start for testing Cribl\u00c2 Stream setups. It\u2019s ideal for our purposes. To verify that DevNull is enabled, let\u2019s walk through setting up a Destination, then setting it up as Cribl\u00c2 Stream\u2019s default output: On Cribl.Cloud or any other distributed mode, first click the top nav\u2019s Manage tab to select the default (or another) Worker Group. From Cribl\u00c2 Stream\u2019s top menu, select Data > Destinations. From the Data\u00c2 Destinations page\u2019s tiles or left menu, select DevNull. (You can use the search box to jump to the DevNull tile.) On the resulting devnull row, look for the Live indicator under Status. This confirms that the DevNull Destination is ready to accept events. From the Data\u00c2 Destinations page\u2019s left nav, select the Default Destination at the top. On the resulting Manage Default Destination page, verify that the Default\u00c2 Output ID drop-down points to the devnull Destination we just examined. If you\u2019re on Cribl.Cloud or any other distributed mode, click Commit\u00c2 &\u00c2 Deploy at Cribl\u00c2 Stream\u2019s upper right (if available) before proceeding. Then, in the resulting dialog box, click Commit\u00c2 &\u00c2 Deploy to confirm. You\u2019ll see a Commit\u00c2 successful message. We\u2019ve now set up data flow on both sides. Is data flowing? Let\u2019s check. Click the top nav\u2019s Monitoring tab. This opens a summary dashboard, where you should see a steady flow of data in and out of Cribl\u00c2 Stream. The left graph shows events in/out. The right graph shows bytes in/out. Monitoring displays data from the preceding 24 hours. You can use the Monitoring submenu to open detailed displays of Cribl\u00c2 Stream components, collection jobs and tasks, and Cribl\u00c2 Stream\u2019s own internal logs. Click Sources on the lower submenu to switch to this view: This is a compact display of each Source\u2019s inbound events and bytes as a sparkline. You can click each Source\u2019s Expand button (highlighted at right) to zoom up detailed graphs. Click Destinations on the lower submenu. This displays a similar sparklines view, where you can confirm data flow out to the devnull Destination: With confidence that we\u2019ve got data flowing, let\u2019s send it through a Cribl\u00c2 Stream Pipeline, where we can add Functions to refine the raw data. A Pipeline is a stack of Cribl\u00c2 Stream Functions that process data. Pipelines are central to refining your data, and also provide a central Cribl\u00c2 Stream workspace \u00e2\u0080\u0093 so let\u2019s get one going. On Cribl.Cloud or any other distributed mode, first click the top nav\u2019s Manage tab to select the default (or another) Worker Group. From the top menu, select Processing > Pipelines. You now have a two-pane view, with a\u00c2 Pipelines list on the left and Sample\u00c2 Data controls on the right. (We\u2019ll capture some sample data momentarily.) At the Pipelines pane\u2019s upper right, click Add\u00c2 Pipeline, then select Create\u00c2 Pipeline. In the new Pipeline\u2019s ID field, enter a unique identifier. (For this tutorial, you might use slicendice.) Optionally, enter a **Description ** of this Pipeline\u2019s purpose. Click Save. Now scroll through the right Preview pane. Depending on your data sample, you should now see multiple events struck out and faded \u00e2\u0080\u0093 indicating that Cribl\u00c2 Stream will drop them before forwarding the data. If you\u2019re on Cribl.Cloud or any other distributed mode, click Commit\u00c2 &\u00c2 Deploy at Cribl\u00c2 Stream\u2019s upper right before proceeding. Then, in the resulting dialog box, click Commit\u00c2 &\u00c2 Deploy to confirm. You\u2019ll see a Commit\u00c2 successful message. Your empty Pipeline now prompts you to preview data, add Functions, and attach a Route. So\u00c2 let\u2019s capture some data to preview. The right Sample\u00c2 Data pane provides multiple tools for grabbing data from multiple places (inbound streams, copy/paste, and uploaded files); for previewing and testing data transformations as you build them; and for saving and reloading sample files. Since we\u2019ve already got live (simulated) data flowing in from the datagen Source we built, let\u2019s grab some of that data. In the right pane, click Capture\u00c2 Data. Click Capture, then accept the drop-down\u2019s defaults \u00e2\u0080\u0093 click Start. When the modal finishes populating with events, click Save\u00c2 as Sample File. In the SAMPLE\u00c2 FILE SETTINGS fly-out, change the generated File Name to a name you\u2019ll recognize, like be_raw.log. Click Save. This saves to the File\u00c2 Name you entered above, and closes the modal. You\u2019re now previewing the captured events in the right pane. (Note that this pane\u2019s Simple\u00c2 Preview tab now has focus.) Click the Show\u00c2 more link to expand one or more events. By skimming the key-value pairs within the data\u2019s _raw fields, you\u2019ll notice the scenario underlying this preview data (provided by the businessevents.log datagen): these are business logs from a mobile-phone provider. To set up our next step, find at least one marketState K=V pair. Having captured and examined this raw data, let\u2019s use this K=V pair to crack open Cribl\u00c2 Stream\u2019s most basic data-transformation tool, Filtering. Click the right pane\u2019s Sample Data tab. Again click Capture\u00c2 New. In the Capture\u00c2 Sample Data modal, replace the Filter\u00c2 Expression field\u2019s default true value with this simple regex: _raw.match(/marketState=TX/) We\u2019re going to Texas! If you type this in, rather than pasting it, notice how Cribl\u00c2 Stream provides typeahead assist to complete a well-formed JavaScript expression. You can also click the Expand button at the Filter\u00c2 Expression field\u2019s right edge to open a modal to validate your expression. The adjacent drop-down enables you to restore previously used expressions. Click Capture, then Start. Using the Capture drop-down\u2019s default limits of 10 seconds and 10 events, you\u2019ll notice that with this filter applied, it takes much longer for Cribl\u00c2 Stream to capture 10 matching events. Click Cancel to discard this filtered data and close the modal. On the right pane\u2019s Sample\u00c2"
    },
    {
        "question_answer": {
            "question": "How long does it take to set up Cribl Stream?",
            "answer": "Assuming a cold start, setting up a basic deployment of Cribl Stream might take about an hour. However, you can work through it in chunks, and Cribl Stream will persist your work between sessions.",
            "follow_up_question_1": "What are the steps involved in setting up Cribl Stream?",
            "follow_up_answer_1": "First, you need to plan your deployment, which involves deciding on your Sources, Routes, Pipelines, and Destinations. Then, you can optionally install Cribl Stream on your own hardware or virtual machine, or you can use a free Cribl.Cloud instance hosted by Cribl. Once you've installed Cribl Stream, you can configure it by setting up your Sources, Routes, Pipelines, and Destinations.",
            "follow_up_question_2": "What if I want to use Cribl Stream in a distributed mode?",
            "follow_up_answer_2": "Cribl Stream supports distributed mode, which allows you to handle larger data streams. If you're using a Cribl.Cloud instance, it always runs in distributed mode. If you're self-hosting Cribl Stream, you can switch your installation to distributed mode to fully experience a real-world, self-hosted production setup.",
            "follow_up_question_3": "What are the requirements for running Cribl Stream in distributed mode?",
            "follow_up_answer_3": "The requirements for running Cribl Stream in distributed mode are a bit more extensive than for a single-instance deployment. You'll need to consider factors like the number of physical cores or virtual CPUs, the operating system, and the network ports that Cribl Stream will listen on. You can find more detailed information on the Cribl documentation.",
            "url": "https://docs.cribl.io/docs/getting-started-guide"
        },
        "reference": "url,title,text_chunks https://docs.cribl.io/docs/getting-started-guide,Getting Started Guide | Cribl Docs,\"This guide walks you through planning, (optionally) installing, and configuring a basic deployment of Cribl\u00c2 Stream. You\u2019ll capture some realistic sample log data, and then use Cribl\u00c2 Stream\u2019s built-in Functions to redact, parse, refine, and shrink the data. By the end of this guide, you\u2019ll have assembled all of Cribl\u00c2 Stream\u2019s basic building blocks: a Source, Route, Pipeline, several Functions, and a Destination. You can complete this tutorial using Cribl\u00c2 Stream\u2019s included sample data, without connections to \u00e2\u0080\u0093 or licenses on \u00e2\u0080\u0093 any inbound or outbound services. Assuming a cold start (from first-time setup of a Cribl.Cloud or self-hosted instance), this guide might take about an hour. But you\u00c2 can work through it in chunks, and Cribl\u00c2 Stream will persist your work between sessions. If you\u2019ve already launched a Cribl\u00c2 Stream instance (either Cribl.Cloud or self-hosted), skip ahead to Get\u00c2 Data\u00c2 Flowing. If you\u2019d prefer a similar tutorial based on Cribl\u00c2 Stream\u2019s simplified, visual QuickConnect UI, see our CrowdStream setup guide. All the steps shown there work in Cribl.Cloud as well. Once you\u2019ve mastered all the techniques in this tutorial, check out its Distributed\u00c2 Quick\u00c2 Start successor. To quote Robert Frost and Robert Plant (of Led\u00c2 Zeppelin), there are two paths that you can go by: Do this tutorial with a free Cribl.Cloud instance, hosted by Cribl. Follow the registration instructions in the section just below. This\u00c2 skips you past all the requirements and installation sections below \u00e2\u0080\u0093 you\u2019ll have nothing to install or host. Because Cribl.Cloud always runs in distributed\u00c2 mode, this will require a few extra clicks (all clearly labeled) later in the tutorial\u2019s body. But it will give you immediate experience with Cribl\u00c2 Stream\u2019s typical production mode. Do this tutorial by downloading and installing Cribl\u00c2 Stream software on your own hardware or virtual machine. Follow the Requirements and Download and Install instructions below. You\u2019ll need to provide your own infrastructure. But if you\u2019re planning to use self-hosted/on-prem Cribl\u00c2 Stream in production, this will walk you through its realistic setup. To fully experience a real-world, self-hosted production setup, you can switch your Cribl\u00c2 Stream installation to distributed\u00c2 mode. And then chase this tutorial with our Distributed\u00c2 Quick\u00c2 Start, which fully exercises distributed mode\u2019s multiple Workers and Worker\u00c2 Groups. As indicated just above, you can skip installing Cribl\u00c2 Stream software \u00e2\u0080\u0093 and skip this tutorial\u2019s next several sections \u00e2\u0080\u0093 by registering a free Cribl.Cloud instance. Cribl will quickly spin up a fully functioning copy of Cribl\u00c2 Stream for you, and manage it on your behalf. To use this fastest option: Really, skip all the other sections of the linked Cloud Guide, and all this tutorial\u2019s sections between here and Get\u00c2 Data\u00c2 Flowing. We\u00c2 told you this was a quick start! The minimum requirements for running this tutorial are the same as for a Cribl\u00c2 Stream production single-instance deployment. Example distributions: Ubuntu\u00c2 16.04, Debian\u00c2 9, RHEL\u00c2 7, CentOS\u00c2 Linux 7, 8, or CentOS Stream 9, SUSE\u00c2 Linux Enterprise\u00c2 Server 12, Amazon\u00c2 Linux 2. Tested so far on Ubuntu (14.04, 16.04, 18.04, and 20.04), CentOS 7.9, and Amazon Linux 2. We assume that 1 physical core is equivalent to 2\u00c2 virtual/\u00e2\u0080\u008bhyperthreaded CPUs (vCPUs) on Intel/Xeon or AMD processors; and to 1\u00c2 (higher-throughput) vCPU on Graviton2/ARM64 processors. By default, Cribl\u00c2 Stream listens on the following ports: You can override these defaults as needed. For higher processing volumes, users typically enable Cribl\u00c2 Stream\u2019s Distributed\u00c2 Deployment option. While beyond the scope of this tutorial, that option has a few additional requirements, which we list here for planning purposes: See Sizing and Scaling for further details about configuring Cribl\u00c2 Stream to handle large data streams. To avoid permissions errors, you should both install and run (next section) Cribl\u00c2 Stream as the same Linux user. For\u00c2 details on creating a new user (addressing both systemd and initd distro\u2019s), see Enabling\u00c2 Start\u00c2 on\u00c2 Boot. Download the latest version of Cribl\u00c2 Stream at https://cribl.io/download/. Un-tar the resulting .tgz file in a directory of your choice (e.g., /opt/). Here\u2019s general syntax, then a specific example: You\u2019ll now have Cribl\u00c2 Stream installed in a cribl subdirectory, by default: /opt/cribl/. We\u2019ll\u00c2 refer to this cribl subdirectory throughout this documentation as $CRIBL_HOME. In your terminal, switch to the $CRIBL_HOME/bin directory (e.g,: /opt/cribl/bin). Here, you can start, stop, and verify the Cribl\u00c2 Stream server using these basic ./cribl CLI commands: For other available commands, see CLI\u00c2 Reference. Next, in your browser, open http://<hostname>:9000 (e.g., http://localhost:9000) and log in with default credentials (admin, admin). Register your copy of Cribl\u00c2 Stream when prompted. After registering, you\u2019ll be prompted to change the default password. That\u2019s it! With Cribl\u00c2 Stream now running \u00e2\u0080\u0093 either in Cribl.Cloud or in your self-hosted copy \u00e2\u0080\u0093 you\u2019re ready to configure a working Cribl\u00c2 Stream deployment. You\u2019ll set up a Source, Destination, Pipeline, and Route, and will assemble several built-in Functions to refine sample log data. Each Cribl\u00c2 Stream Source represents a data input. Options include Splunk, Elastic Beats, Kinesis, Kafka, syslog, HTTP, TCP\u00c2 JSON, and others. For this tutorial, we\u2019ll enable a Cribl\u00c2 Stream built-in datagen (i.e., data generator) that generates a stream of realistic sample log data. If you\u2019re on Cribl.Cloud or any other distributed mode, first click the top nav\u2019s Manage tab to select the default (or another) Worker Group. From Cribl\u00c2 Stream\u2019s Manage submenu, select Data > Sources. From the Data\u00c2 Sources page\u2019s tiles or left menu, select Datagen. (You can use the search box to jump to the Datagen tile.) Click Add\u00c2 Source to open the New\u00c2 Source modal. In the Input ID field, name this Source businessevent. In the Data Generator File drop-down, select businessevent.log. This generates\u2026log events for a business scenario. We\u2019ll look at their structure shortly, in Capture\u00c2 and Filter\u00c2 Sample Data. Click Save. If you\u2019re on Cribl.Cloud or any other distributed mode, click Commit\u00c2 &\u00c2 Deploy at Cribl\u00c2 Stream\u2019s upper right before proceeding. Then, in the resulting dialog box, click Commit\u00c2 &\u00c2 Deploy to confirm. You\u2019ll see a Commit\u00c2 successful message. The Yes toggle in the Enabled column indicates that your Datagen Source has started generating sample data. Each Cribl\u00c2 Stream Destination represents a data output. Options include Splunk, Kafka, Kinesis, InfluxDB, Snowflake, Databricks, TCP JSON, and others. For this tutorial, we\u2019ll use Cribl\u00c2 Stream\u2019s built-in DevNull Destination. This simply discards events \u00e2\u0080\u0093 not very exciting! But it simulates a real output, so it provides a configuration-free quick start for testing Cribl\u00c2 Stream setups. It\u2019s ideal for our purposes. To verify that DevNull is enabled, let\u2019s walk through setting up a Destination, then setting it up as Cribl\u00c2 Stream\u2019s default output: On Cribl.Cloud or any other distributed mode, first click the top nav\u2019s Manage tab to select the default (or another) Worker Group. From Cribl\u00c2 Stream\u2019s top menu, select Data > Destinations. From the Data\u00c2 Destinations page\u2019s tiles or left menu, select DevNull. (You can use the search box to jump to the DevNull tile.) On the resulting devnull row, look for the Live indicator under Status. This confirms that the DevNull Destination is ready to accept events. From the Data\u00c2 Destinations page\u2019s left nav, select the Default Destination at the top. On the resulting Manage Default Destination page, verify that the Default\u00c2 Output ID drop-down points to the devnull Destination we just examined. If you\u2019re on Cribl.Cloud or any other distributed mode, click Commit\u00c2 &\u00c2 Deploy at Cribl\u00c2 Stream\u2019s upper right (if available) before proceeding. Then, in the resulting dialog box, click Commit\u00c2 &\u00c2 Deploy to confirm. You\u2019ll see a Commit\u00c2 successful message. We\u2019ve now set up data flow on both sides. Is data flowing? Let\u2019s check. Click the top nav\u2019s Monitoring tab. This opens a summary dashboard, where you should see a steady flow of data in and out of Cribl\u00c2 Stream. The left graph shows events in/out. The right graph shows bytes in/out. Monitoring displays data from the preceding 24 hours. You can use the Monitoring submenu to open detailed displays of Cribl\u00c2 Stream components, collection jobs and tasks, and Cribl\u00c2 Stream\u2019s own internal logs. Click Sources on the lower submenu to switch to this view: This is a compact display of each Source\u2019s inbound events and bytes as a sparkline. You can click each Source\u2019s Expand button (highlighted at right) to zoom up detailed graphs. Click Destinations on the lower submenu. This displays a similar sparklines view, where you can confirm data flow out to the devnull Destination: With confidence that we\u2019ve got data flowing, let\u2019s send it through a Cribl\u00c2 Stream Pipeline, where we can add Functions to refine the raw data. A Pipeline is a stack of Cribl\u00c2 Stream Functions that process data. Pipelines are central to refining your data, and also provide a central Cribl\u00c2 Stream workspace \u00e2\u0080\u0093 so let\u2019s get one going. On Cribl.Cloud or any other distributed mode, first click the top nav\u2019s Manage tab to select the default (or another) Worker Group. From the top menu, select Processing > Pipelines. You now have a two-pane view, with a\u00c2 Pipelines list on the left and Sample\u00c2 Data controls on the right. (We\u2019ll capture some sample data momentarily.) At the Pipelines pane\u2019s upper right, click Add\u00c2 Pipeline, then select Create\u00c2 Pipeline. In the new Pipeline\u2019s ID field, enter a unique identifier. (For this tutorial, you might use slicendice.) Optionally, enter a **Description ** of this Pipeline\u2019s purpose. Click Save. Now scroll through the right Preview pane. Depending on your data sample, you should now see multiple events struck out and faded \u00e2\u0080\u0093 indicating that Cribl\u00c2 Stream will drop them before forwarding the data. If you\u2019re on Cribl.Cloud or any other distributed mode, click Commit\u00c2 &\u00c2 Deploy at Cribl\u00c2 Stream\u2019s upper right before proceeding. Then, in the resulting dialog box, click Commit\u00c2 &\u00c2 Deploy to confirm. You\u2019ll see a Commit\u00c2 successful message. Your empty Pipeline now prompts you to preview data, add Functions, and attach a Route. So\u00c2 let\u2019s capture some data to preview. The right Sample\u00c2 Data pane provides multiple tools for grabbing data from multiple places (inbound streams, copy/paste, and uploaded files); for previewing and testing data transformations as you build them; and for saving and reloading sample files. Since we\u2019ve already got live (simulated) data flowing in from the datagen Source we built, let\u2019s grab some of that data. In the right pane, click Capture\u00c2 Data. Click Capture, then accept the drop-down\u2019s defaults \u00e2\u0080\u0093 click Start. When the modal finishes populating with events, click Save\u00c2 as Sample File. In the SAMPLE\u00c2 FILE SETTINGS fly-out, change the generated File Name to a name you\u2019ll recognize, like be_raw.log. Click Save. This saves to the File\u00c2 Name you entered above, and closes the modal. You\u2019re now previewing the captured events in the right pane. (Note that this pane\u2019s Simple\u00c2 Preview tab now has focus.) Click the Show\u00c2 more link to expand one or more events. By skimming the key-value pairs within the data\u2019s _raw fields, you\u2019ll notice the scenario underlying this preview data (provided by the businessevents.log datagen): these are business logs from a mobile-phone provider. To set up our next step, find at least one marketState K=V pair. Having captured and examined this raw data, let\u2019s use this K=V pair to crack open Cribl\u00c2 Stream\u2019s most basic data-transformation tool, Filtering. Click the right pane\u2019s Sample Data tab. Again click Capture\u00c2 New. In the Capture\u00c2 Sample Data modal, replace the Filter\u00c2 Expression field\u2019s default true value with this simple regex: _raw.match(/marketState=TX/) We\u2019re going to Texas! If you type this in, rather than pasting it, notice how Cribl\u00c2 Stream provides typeahead assist to complete a well-formed JavaScript expression. You can also click the Expand button at the Filter\u00c2 Expression field\u2019s right edge to open a modal to validate your expression. The adjacent drop-down enables you to restore previously used expressions. Click Capture, then Start. Using the Capture drop-down\u2019s default limits of 10 seconds and 10 events, you\u2019ll notice that with this filter applied, it takes much longer for Cribl\u00c2 Stream to capture 10 matching events. Click Cancel to discard this filtered data and close the modal. On the right pane\u2019s Sample\u00c2"
    },
    {
        "question_answer": {
            "question": "What is the purpose of the DevNull Destination in Cribl Stream?",
            "answer": "The DevNull Destination in Cribl Stream is a built-in feature that simply discards events. While it might not seem very exciting, it's actually very useful for testing Cribl Stream setups. It simulates a real output without requiring any configuration, making it ideal for getting started quickly.",
            "follow_up_question_1": "How can I verify that the DevNull Destination is enabled?",
            "follow_up_answer_1": "You can verify that the DevNull Destination is enabled by going to the Data Destinations page in Cribl Stream. There, you should see a 'Live' indicator under the Status column for the DevNull Destination. This confirms that it's ready to accept events.",
            "follow_up_question_2": "What happens after I've confirmed that the DevNull Destination is enabled?",
            "follow_up_answer_2": "Once you've confirmed that the DevNull Destination is enabled, you can proceed with setting up your data flow. You can check that data is flowing by clicking on the Monitoring tab in Cribl Stream, which shows a summary dashboard with a steady flow of data in and out.",
            "follow_up_question_3": "Can I use other Destinations besides DevNull in Cribl Stream?",
            "follow_up_answer_3": "Absolutely! While the DevNull Destination is useful for testing, Cribl Stream supports a wide range of Destinations for actual data output. These include platforms like Splunk, Kafka, Kinesis, InfluxDB, Snowflake, Databricks, TCP JSON, and others. You can choose the Destination that best suits your needs.",
            "url": "https://docs.cribl.io/docs/getting-started-guide"
        },
        "reference": "url,title,text_chunks https://docs.cribl.io/docs/getting-started-guide,Getting Started Guide | Cribl Docs,\"This guide walks you through planning, (optionally) installing, and configuring a basic deployment of Cribl\u00c2 Stream. You\u2019ll capture some realistic sample log data, and then use Cribl\u00c2 Stream\u2019s built-in Functions to redact, parse, refine, and shrink the data. By the end of this guide, you\u2019ll have assembled all of Cribl\u00c2 Stream\u2019s basic building blocks: a Source, Route, Pipeline, several Functions, and a Destination. You can complete this tutorial using Cribl\u00c2 Stream\u2019s included sample data, without connections to \u00e2\u0080\u0093 or licenses on \u00e2\u0080\u0093 any inbound or outbound services. Assuming a cold start (from first-time setup of a Cribl.Cloud or self-hosted instance), this guide might take about an hour. But you\u00c2 can work through it in chunks, and Cribl\u00c2 Stream will persist your work between sessions. If you\u2019ve already launched a Cribl\u00c2 Stream instance (either Cribl.Cloud or self-hosted), skip ahead to Get\u00c2 Data\u00c2 Flowing. If you\u2019d prefer a similar tutorial based on Cribl\u00c2 Stream\u2019s simplified, visual QuickConnect UI, see our CrowdStream setup guide. All the steps shown there work in Cribl.Cloud as well. Once you\u2019ve mastered all the techniques in this tutorial, check out its Distributed\u00c2 Quick\u00c2 Start successor. To quote Robert Frost and Robert Plant (of Led\u00c2 Zeppelin), there are two paths that you can go by: Do this tutorial with a free Cribl.Cloud instance, hosted by Cribl. Follow the registration instructions in the section just below. This\u00c2 skips you past all the requirements and installation sections below \u00e2\u0080\u0093 you\u2019ll have nothing to install or host. Because Cribl.Cloud always runs in distributed\u00c2 mode, this will require a few extra clicks (all clearly labeled) later in the tutorial\u2019s body. But it will give you immediate experience with Cribl\u00c2 Stream\u2019s typical production mode. Do this tutorial by downloading and installing Cribl\u00c2 Stream software on your own hardware or virtual machine. Follow the Requirements and Download and Install instructions below. You\u2019ll need to provide your own infrastructure. But if you\u2019re planning to use self-hosted/on-prem Cribl\u00c2 Stream in production, this will walk you through its realistic setup. To fully experience a real-world, self-hosted production setup, you can switch your Cribl\u00c2 Stream installation to distributed\u00c2 mode. And then chase this tutorial with our Distributed\u00c2 Quick\u00c2 Start, which fully exercises distributed mode\u2019s multiple Workers and Worker\u00c2 Groups. As indicated just above, you can skip installing Cribl\u00c2 Stream software \u00e2\u0080\u0093 and skip this tutorial\u2019s next several sections \u00e2\u0080\u0093 by registering a free Cribl.Cloud instance. Cribl will quickly spin up a fully functioning copy of Cribl\u00c2 Stream for you, and manage it on your behalf. To use this fastest option: Really, skip all the other sections of the linked Cloud Guide, and all this tutorial\u2019s sections between here and Get\u00c2 Data\u00c2 Flowing. We\u00c2 told you this was a quick start! The minimum requirements for running this tutorial are the same as for a Cribl\u00c2 Stream production single-instance deployment. Example distributions: Ubuntu\u00c2 16.04, Debian\u00c2 9, RHEL\u00c2 7, CentOS\u00c2 Linux 7, 8, or CentOS Stream 9, SUSE\u00c2 Linux Enterprise\u00c2 Server 12, Amazon\u00c2 Linux 2. Tested so far on Ubuntu (14.04, 16.04, 18.04, and 20.04), CentOS 7.9, and Amazon Linux 2. We assume that 1 physical core is equivalent to 2\u00c2 virtual/\u00e2\u0080\u008bhyperthreaded CPUs (vCPUs) on Intel/Xeon or AMD processors; and to 1\u00c2 (higher-throughput) vCPU on Graviton2/ARM64 processors. By default, Cribl\u00c2 Stream listens on the following ports: You can override these defaults as needed. For higher processing volumes, users typically enable Cribl\u00c2 Stream\u2019s Distributed\u00c2 Deployment option. While beyond the scope of this tutorial, that option has a few additional requirements, which we list here for planning purposes: See Sizing and Scaling for further details about configuring Cribl\u00c2 Stream to handle large data streams. To avoid permissions errors, you should both install and run (next section) Cribl\u00c2 Stream as the same Linux user. For\u00c2 details on creating a new user (addressing both systemd and initd distro\u2019s), see Enabling\u00c2 Start\u00c2 on\u00c2 Boot. Download the latest version of Cribl\u00c2 Stream at https://cribl.io/download/. Un-tar the resulting .tgz file in a directory of your choice (e.g., /opt/). Here\u2019s general syntax, then a specific example: You\u2019ll now have Cribl\u00c2 Stream installed in a cribl subdirectory, by default: /opt/cribl/. We\u2019ll\u00c2 refer to this cribl subdirectory throughout this documentation as $CRIBL_HOME. In your terminal, switch to the $CRIBL_HOME/bin directory (e.g,: /opt/cribl/bin). Here, you can start, stop, and verify the Cribl\u00c2 Stream server using these basic ./cribl CLI commands: For other available commands, see CLI\u00c2 Reference. Next, in your browser, open http://<hostname>:9000 (e.g., http://localhost:9000) and log in with default credentials (admin, admin). Register your copy of Cribl\u00c2 Stream when prompted. After registering, you\u2019ll be prompted to change the default password. That\u2019s it! With Cribl\u00c2 Stream now running \u00e2\u0080\u0093 either in Cribl.Cloud or in your self-hosted copy \u00e2\u0080\u0093 you\u2019re ready to configure a working Cribl\u00c2 Stream deployment. You\u2019ll set up a Source, Destination, Pipeline, and Route, and will assemble several built-in Functions to refine sample log data. Each Cribl\u00c2 Stream Source represents a data input. Options include Splunk, Elastic Beats, Kinesis, Kafka, syslog, HTTP, TCP\u00c2 JSON, and others. For this tutorial, we\u2019ll enable a Cribl\u00c2 Stream built-in datagen (i.e., data generator) that generates a stream of realistic sample log data. If you\u2019re on Cribl.Cloud or any other distributed mode, first click the top nav\u2019s Manage tab to select the default (or another) Worker Group. From Cribl\u00c2 Stream\u2019s Manage submenu, select Data > Sources. From the Data\u00c2 Sources page\u2019s tiles or left menu, select Datagen. (You can use the search box to jump to the Datagen tile.) Click Add\u00c2 Source to open the New\u00c2 Source modal. In the Input ID field, name this Source businessevent. In the Data Generator File drop-down, select businessevent.log. This generates\u2026log events for a business scenario. We\u2019ll look at their structure shortly, in Capture\u00c2 and Filter\u00c2 Sample Data. Click Save. If you\u2019re on Cribl.Cloud or any other distributed mode, click Commit\u00c2 &\u00c2 Deploy at Cribl\u00c2 Stream\u2019s upper right before proceeding. Then, in the resulting dialog box, click Commit\u00c2 &\u00c2 Deploy to confirm. You\u2019ll see a Commit\u00c2 successful message. The Yes toggle in the Enabled column indicates that your Datagen Source has started generating sample data. Each Cribl\u00c2 Stream Destination represents a data output. Options include Splunk, Kafka, Kinesis, InfluxDB, Snowflake, Databricks, TCP JSON, and others. For this tutorial, we\u2019ll use Cribl\u00c2 Stream\u2019s built-in DevNull Destination. This simply discards events \u00e2\u0080\u0093 not very exciting! But it simulates a real output, so it provides a configuration-free quick start for testing Cribl\u00c2 Stream setups. It\u2019s ideal for our purposes. To verify that DevNull is enabled, let\u2019s walk through setting up a Destination, then setting it up as Cribl\u00c2 Stream\u2019s default output: On Cribl.Cloud or any other distributed mode, first click the top nav\u2019s Manage tab to select the default (or another) Worker Group. From Cribl\u00c2 Stream\u2019s top menu, select Data > Destinations. From the Data\u00c2 Destinations page\u2019s tiles or left menu, select DevNull. (You can use the search box to jump to the DevNull tile.) On the resulting devnull row, look for the Live indicator under Status. This confirms that the DevNull Destination is ready to accept events. From the Data\u00c2 Destinations page\u2019s left nav, select the Default Destination at the top. On the resulting Manage Default Destination page, verify that the Default\u00c2 Output ID drop-down points to the devnull Destination we just examined. If you\u2019re on Cribl.Cloud or any other distributed mode, click Commit\u00c2 &\u00c2 Deploy at Cribl\u00c2 Stream\u2019s upper right (if available) before proceeding. Then, in the resulting dialog box, click Commit\u00c2 &\u00c2 Deploy to confirm. You\u2019ll see a Commit\u00c2 successful message. We\u2019ve now set up data flow on both sides. Is data flowing? Let\u2019s check. Click the top nav\u2019s Monitoring tab. This opens a summary dashboard, where you should see a steady flow of data in and out of Cribl\u00c2 Stream. The left graph shows events in/out. The right graph shows bytes in/out. Monitoring displays data from the preceding 24 hours. You can use the Monitoring submenu to open detailed displays of Cribl\u00c2 Stream components, collection jobs and tasks, and Cribl\u00c2 Stream\u2019s own internal logs. Click Sources on the lower submenu to switch to this view: This is a compact display of each Source\u2019s inbound events and bytes as a sparkline. You can click each Source\u2019s Expand button (highlighted at right) to zoom up detailed graphs. Click Destinations on the lower submenu. This displays a similar sparklines view, where you can confirm data flow out to the devnull Destination: With confidence that we\u2019ve got data flowing, let\u2019s send it through a Cribl\u00c2 Stream Pipeline, where we can add Functions to refine the raw data. A Pipeline is a stack of Cribl\u00c2 Stream Functions that process data. Pipelines are central to refining your data, and also provide a central Cribl\u00c2 Stream workspace \u00e2\u0080\u0093 so let\u2019s get one going. On Cribl.Cloud or any other distributed mode, first click the top nav\u2019s Manage tab to select the default (or another) Worker Group. From the top menu, select Processing > Pipelines. You now have a two-pane view, with a\u00c2 Pipelines list on the left and Sample\u00c2 Data controls on the right. (We\u2019ll capture some sample data momentarily.) At the Pipelines pane\u2019s upper right, click Add\u00c2 Pipeline, then select Create\u00c2 Pipeline. In the new Pipeline\u2019s ID field, enter a unique identifier. (For this tutorial, you might use slicendice.) Optionally, enter a **Description ** of this Pipeline\u2019s purpose. Click Save. Now scroll through the right Preview pane. Depending on your data sample, you should now see multiple events struck out and faded \u00e2\u0080\u0093 indicating that Cribl\u00c2 Stream will drop them before forwarding the data. If you\u2019re on Cribl.Cloud or any other distributed mode, click Commit\u00c2 &\u00c2 Deploy at Cribl\u00c2 Stream\u2019s upper right before proceeding. Then, in the resulting dialog box, click Commit\u00c2 &\u00c2 Deploy to confirm. You\u2019ll see a Commit\u00c2 successful message. Your empty Pipeline now prompts you to preview data, add Functions, and attach a Route. So\u00c2 let\u2019s capture some data to preview. The right Sample\u00c2 Data pane provides multiple tools for grabbing data from multiple places (inbound streams, copy/paste, and uploaded files); for previewing and testing data transformations as you build them; and for saving and reloading sample files. Since we\u2019ve already got live (simulated) data flowing in from the datagen Source we built, let\u2019s grab some of that data. In the right pane, click Capture\u00c2 Data. Click Capture, then accept the drop-down\u2019s defaults \u00e2\u0080\u0093 click Start. When the modal finishes populating with events, click Save\u00c2 as Sample File. In the SAMPLE\u00c2 FILE SETTINGS fly-out, change the generated File Name to a name you\u2019ll recognize, like be_raw.log. Click Save. This saves to the File\u00c2 Name you entered above, and closes the modal. You\u2019re now previewing the captured events in the right pane. (Note that this pane\u2019s Simple\u00c2 Preview tab now has focus.) Click the Show\u00c2 more link to expand one or more events. By skimming the key-value pairs within the data\u2019s _raw fields, you\u2019ll notice the scenario underlying this preview data (provided by the businessevents.log datagen): these are business logs from a mobile-phone provider. To set up our next step, find at least one marketState K=V pair. Having captured and examined this raw data, let\u2019s use this K=V pair to crack open Cribl\u00c2 Stream\u2019s most basic data-transformation tool, Filtering. Click the right pane\u2019s Sample Data tab. Again click Capture\u00c2 New. In the Capture\u00c2 Sample Data modal, replace the Filter\u00c2 Expression field\u2019s default true value with this simple regex: _raw.match(/marketState=TX/) We\u2019re going to Texas! If you type this in, rather than pasting it, notice how Cribl\u00c2 Stream provides typeahead assist to complete a well-formed JavaScript expression. You can also click the Expand button at the Filter\u00c2 Expression field\u2019s right edge to open a modal to validate your expression. The adjacent drop-down enables you to restore previously used expressions. Click Capture, then Start. Using the Capture drop-down\u2019s default limits of 10 seconds and 10 events, you\u2019ll notice that with this filter applied, it takes much longer for Cribl\u00c2 Stream to capture 10 matching events. Click Cancel to discard this filtered data and close the modal. On the right pane\u2019s Sample\u00c2"
    },
    {
        "question_answer": {
            "question": "What is Cribl Stream and what does it do?",
            "answer": "Cribl Stream is a vendor-agnostic observability pipeline that provides the flexibility to collect, reduce, enrich, normalize, and route data from any source to any destination within your existing data infrastructure.",
            "follow_up_question_1": "What does it mean when you say it's 'vendor-agnostic'?",
            "follow_up_answer_1": "Being 'vendor-agnostic' means that Cribl Stream can work with any data source or destination, regardless of the vendor. It's not tied to a specific manufacturer or provider, giving users more flexibility and choice.",
            "follow_up_question_2": "How does this flexibility benefit the users?",
            "follow_up_answer_2": "This flexibility allows users to leverage the best tools and services for their specific needs, without being locked into a single vendor's ecosystem. It also enables them to easily integrate new data sources or destinations as their needs evolve.",
            "follow_up_question_3": "Can you give an example of how Cribl Stream has been used effectively?",
            "follow_up_answer_3": "Sure, Vodafone, for instance, used Cribl Stream to dial up their business insights. It helped them manage and route their data more effectively, leading to more informed decision-making.",
            "url": "https://cribl.io/support"
        },
        "reference": "url,title,text_chunks https://cribl.io/support,Cribl Stream and Edge Observability Pipeline Support | Cribl,\"New to observability? Find out everything you need to know. Telemetry 101 Understanding the Basics of Telemetry and Its Benefits Cribl Stream is a vendor-agnostic observability pipeline that gives you the flexibility to collect, reduce, enrich, normalize, and route data from any source to any destination within your existing data infrastructure. Vodafone Case Study Vodafone Dials up Business Insights with Cribl Stream Cribl Edge provides an intelligent, highly scalable edge-based data collection system for logs, metrics, and application data. SpyCloud Edge Story Listen to how SpyCloud uses Cribl Edge at scale. Cribl Search turns the traditional search process on its head, allowing users to search data in place without having to collect/store first. Happy 1st Birthday Cribl Search! The Cribl.Cloud platform gets you up and running fast without the hassle of running infrastructure. Cribl.Cloud Solution Brief The fastest and easiest way to realize the value of an observability ecosystem. AppScope gives operators the visibility they need into application behavior, metrics and events with no configuration and no agent required. Sandbox Launch an AppScope Sandbox today! Explore Cribl\u2019s Solutions by Use Cases: Explore Cribl\u2019s Solutions by Integrations: Seamless Integrations for Your Observability Data Explore Cribl\u2019s Solutions by Industry: Get this Gartner\u00ae report and learn why telemetry pipeline solutions represent a robust and largely untapped source of business insight beyond event and incident response. CriblCon 2023 Escaping Data Lock-In Amidst Industry Takeovers Learn how IT & Security engineers increase resilience & provide more options for analysis to make decisions faster with better data. Try Your Own Cribl Sandbox Experience a full version of Cribl Stream and Cribl Edge in the cloud. Download Cribl\u2019s suite of products for free to get started. Get inspired by how our customers are innovating IT, security and observability. They inspire us daily! Sally Beauty Holdings Sally Beauty Swaps LogStash and Syslog-ng with Cribl.Cloud for a Resilient Security and Observability Pipeline Professional Services Check out our new Professional Services offering. Experience a full version of Cribl Stream and Cribl Edge in the cloud. Take Control of Your Observability Data with Cribl Cribl Corporate Overview Cribl makes open observability a reality, giving you the freedom and flexibility to make choices instead of compromises. Stay up to date on all things Cribl and observability. Press Releases Read our most recent press releases. Cribl\u2019s leadership team has built and launched category-defining products for some of the most innovative companies in the technology sector, and is supported by the world\u2019s most elite investors. Join the Cribl herd! The smartest, funniest, most passionate goats you\u2019ll ever meet. Cribl Named to the Inc. 5000 List of Fastest Growing Private Companies Want to learn more about Cribl from our sales experts? Send us your contact information and we\u2019ll be in touch. Guides to help you get started with Cribl Stream, Edge, and Search When you want to do it yourselfaka RTFM A Cribl Community Slack channel so you can connect with other users. Need a login? FREE Cribl training and official certification center The place to ask questions, find answers, and share best practices Step-by-step guidance to help prove the value of your use cases An intro to basic concepts and commonly used functions for Cribl Stream GitHub repos for updated demo and training content Docker hub for the latest published Cribl Stream containers Cribl\u2019s Customer Success team is designed to guide, shape, and advise you throughout your entire journey with Cribl. Exchange ideas and craft a flight path toward success. Throttle up as we accelerate through implementation. Destination is reached when you unlock the value of all your observability data with Cribl. \u00a92018 \u2013 2023 Cribl, Inc. | Legal\""
    },
    {
        "question_answer": {
            "question": "What is Cribl Edge and how does it differ from Cribl Stream?",
            "answer": "Cribl Edge is an intelligent, highly scalable edge-based data collection system for logs, metrics, and application data. While Cribl Stream focuses on managing and routing data, Cribl Edge is designed to collect data at the 'edge' of the network, closer to where it's generated.",
            "follow_up_question_1": "What are the advantages of collecting data at the 'edge'?",
            "follow_up_answer_1": "Collecting data at the 'edge' can reduce latency, as data doesn't have to travel as far to be processed. It can also reduce bandwidth usage, as only relevant data may need to be sent back to the central system. This can lead to more efficient and timely data analysis.",
            "follow_up_question_2": "Can you provide an example of a company that has benefited from using Cribl Edge?",
            "follow_up_answer_2": "Yes, SpyCloud is a company that has effectively used Cribl Edge at scale. They were able to collect and process their data more efficiently, leading to improved operational performance.",
            "follow_up_question_3": "Does Cribl Edge require a lot of configuration or setup?",
            "follow_up_answer_3": "Cribl Edge is designed to be highly scalable and intelligent, which means it can handle large volumes of data with minimal configuration. This makes it a practical solution for businesses of all sizes.",
            "url": "https://cribl.io/support"
        },
        "reference": "url,title,text_chunks https://cribl.io/support,Cribl Stream and Edge Observability Pipeline Support | Cribl,\"New to observability? Find out everything you need to know. Telemetry 101 Understanding the Basics of Telemetry and Its Benefits Cribl Stream is a vendor-agnostic observability pipeline that gives you the flexibility to collect, reduce, enrich, normalize, and route data from any source to any destination within your existing data infrastructure. Vodafone Case Study Vodafone Dials up Business Insights with Cribl Stream Cribl Edge provides an intelligent, highly scalable edge-based data collection system for logs, metrics, and application data. SpyCloud Edge Story Listen to how SpyCloud uses Cribl Edge at scale. Cribl Search turns the traditional search process on its head, allowing users to search data in place without having to collect/store first. Happy 1st Birthday Cribl Search! The Cribl.Cloud platform gets you up and running fast without the hassle of running infrastructure. Cribl.Cloud Solution Brief The fastest and easiest way to realize the value of an observability ecosystem. AppScope gives operators the visibility they need into application behavior, metrics and events with no configuration and no agent required. Sandbox Launch an AppScope Sandbox today! Explore Cribl\u2019s Solutions by Use Cases: Explore Cribl\u2019s Solutions by Integrations: Seamless Integrations for Your Observability Data Explore Cribl\u2019s Solutions by Industry: Get this Gartner\u00ae report and learn why telemetry pipeline solutions represent a robust and largely untapped source of business insight beyond event and incident response. CriblCon 2023 Escaping Data Lock-In Amidst Industry Takeovers Learn how IT & Security engineers increase resilience & provide more options for analysis to make decisions faster with better data. Try Your Own Cribl Sandbox Experience a full version of Cribl Stream and Cribl Edge in the cloud. Download Cribl\u2019s suite of products for free to get started. Get inspired by how our customers are innovating IT, security and observability. They inspire us daily! Sally Beauty Holdings Sally Beauty Swaps LogStash and Syslog-ng with Cribl.Cloud for a Resilient Security and Observability Pipeline Professional Services Check out our new Professional Services offering. Experience a full version of Cribl Stream and Cribl Edge in the cloud. Take Control of Your Observability Data with Cribl Cribl Corporate Overview Cribl makes open observability a reality, giving you the freedom and flexibility to make choices instead of compromises. Stay up to date on all things Cribl and observability. Press Releases Read our most recent press releases. Cribl\u2019s leadership team has built and launched category-defining products for some of the most innovative companies in the technology sector, and is supported by the world\u2019s most elite investors. Join the Cribl herd! The smartest, funniest, most passionate goats you\u2019ll ever meet. Cribl Named to the Inc. 5000 List of Fastest Growing Private Companies Want to learn more about Cribl from our sales experts? Send us your contact information and we\u2019ll be in touch. Guides to help you get started with Cribl Stream, Edge, and Search When you want to do it yourselfaka RTFM A Cribl Community Slack channel so you can connect with other users. Need a login? FREE Cribl training and official certification center The place to ask questions, find answers, and share best practices Step-by-step guidance to help prove the value of your use cases An intro to basic concepts and commonly used functions for Cribl Stream GitHub repos for updated demo and training content Docker hub for the latest published Cribl Stream containers Cribl\u2019s Customer Success team is designed to guide, shape, and advise you throughout your entire journey with Cribl. Exchange ideas and craft a flight path toward success. Throttle up as we accelerate through implementation. Destination is reached when you unlock the value of all your observability data with Cribl. \u00a92018 \u2013 2023 Cribl, Inc. | Legal\""
    },
    {
        "question_answer": {
            "question": "What is Cribl Search and how does it work?",
            "answer": "Cribl Search is a tool that turns the traditional search process on its head, allowing users to search data in place without having to collect or store it first.",
            "follow_up_question_1": "How does this differ from traditional search methods?",
            "follow_up_answer_1": "Traditional search methods often require data to be collected and stored before it can be searched. With Cribl Search, you can search the data where it resides, eliminating the need for costly and time-consuming data collection and storage processes.",
            "follow_up_question_2": "What are the benefits of this approach?",
            "follow_up_answer_2": "This approach can save significant time and resources, as it eliminates the need to collect and store data before it can be searched. It also allows for real-time search results, as data can be searched as soon as it's generated.",
            "follow_up_question_3": "Is Cribl Search a standalone product or does it integrate with other Cribl solutions?",
            "follow_up_answer_3": "Cribl Search is designed to integrate seamlessly with other Cribl solutions, such as Cribl Stream and Cribl Edge. This allows for a comprehensive and cohesive data management and analysis solution.",
            "url": "https://cribl.io/support"
        },
        "reference": "url,title,text_chunks https://cribl.io/support,Cribl Stream and Edge Observability Pipeline Support | Cribl,\"New to observability? Find out everything you need to know. Telemetry 101 Understanding the Basics of Telemetry and Its Benefits Cribl Stream is a vendor-agnostic observability pipeline that gives you the flexibility to collect, reduce, enrich, normalize, and route data from any source to any destination within your existing data infrastructure. Vodafone Case Study Vodafone Dials up Business Insights with Cribl Stream Cribl Edge provides an intelligent, highly scalable edge-based data collection system for logs, metrics, and application data. SpyCloud Edge Story Listen to how SpyCloud uses Cribl Edge at scale. Cribl Search turns the traditional search process on its head, allowing users to search data in place without having to collect/store first. Happy 1st Birthday Cribl Search! The Cribl.Cloud platform gets you up and running fast without the hassle of running infrastructure. Cribl.Cloud Solution Brief The fastest and easiest way to realize the value of an observability ecosystem. AppScope gives operators the visibility they need into application behavior, metrics and events with no configuration and no agent required. Sandbox Launch an AppScope Sandbox today! Explore Cribl\u2019s Solutions by Use Cases: Explore Cribl\u2019s Solutions by Integrations: Seamless Integrations for Your Observability Data Explore Cribl\u2019s Solutions by Industry: Get this Gartner\u00ae report and learn why telemetry pipeline solutions represent a robust and largely untapped source of business insight beyond event and incident response. CriblCon 2023 Escaping Data Lock-In Amidst Industry Takeovers Learn how IT & Security engineers increase resilience & provide more options for analysis to make decisions faster with better data. Try Your Own Cribl Sandbox Experience a full version of Cribl Stream and Cribl Edge in the cloud. Download Cribl\u2019s suite of products for free to get started. Get inspired by how our customers are innovating IT, security and observability. They inspire us daily! Sally Beauty Holdings Sally Beauty Swaps LogStash and Syslog-ng with Cribl.Cloud for a Resilient Security and Observability Pipeline Professional Services Check out our new Professional Services offering. Experience a full version of Cribl Stream and Cribl Edge in the cloud. Take Control of Your Observability Data with Cribl Cribl Corporate Overview Cribl makes open observability a reality, giving you the freedom and flexibility to make choices instead of compromises. Stay up to date on all things Cribl and observability. Press Releases Read our most recent press releases. Cribl\u2019s leadership team has built and launched category-defining products for some of the most innovative companies in the technology sector, and is supported by the world\u2019s most elite investors. Join the Cribl herd! The smartest, funniest, most passionate goats you\u2019ll ever meet. Cribl Named to the Inc. 5000 List of Fastest Growing Private Companies Want to learn more about Cribl from our sales experts? Send us your contact information and we\u2019ll be in touch. Guides to help you get started with Cribl Stream, Edge, and Search When you want to do it yourselfaka RTFM A Cribl Community Slack channel so you can connect with other users. Need a login? FREE Cribl training and official certification center The place to ask questions, find answers, and share best practices Step-by-step guidance to help prove the value of your use cases An intro to basic concepts and commonly used functions for Cribl Stream GitHub repos for updated demo and training content Docker hub for the latest published Cribl Stream containers Cribl\u2019s Customer Success team is designed to guide, shape, and advise you throughout your entire journey with Cribl. Exchange ideas and craft a flight path toward success. Throttle up as we accelerate through implementation. Destination is reached when you unlock the value of all your observability data with Cribl. \u00a92018 \u2013 2023 Cribl, Inc. | Legal\""
    },
    {
        "question_answer": {
            "question": "What is a distributed deployment in the context of Cribl Stream?",
            "answer": "A distributed deployment in Cribl Stream is a typical deployment type for production where Cribl Stream scales out to handle higher data volumes, load-balancing with failover, and parallel data processing based on conditional mapping and routing. It involves a single Leader Node managing multiple Worker Nodes. The Leader distributes and updates configuration on the Workers, handles version control, and monitors the Workers\u2019 health and activity metrics. The Workers do all the data processing.",
            "follow_up_question_1": "What is the role of the Leader Node in this setup?",
            "follow_up_answer_1": "The Leader Node in a distributed deployment has a crucial role. It manages multiple Worker Nodes, distributes and updates configuration on the Workers, handles version control, and monitors the Workers\u2019 health and activity metrics.",
            "follow_up_question_2": "And what about the Worker Nodes, what do they do?",
            "follow_up_answer_2": "The Worker Nodes in a distributed deployment do all the data processing. They receive instructions from the Leader Node and carry out the necessary tasks based on those instructions.",
            "follow_up_question_3": "Can you explain more about the process of configuring a Leader and two Workers?",
            "follow_up_answer_3": "Sure, the configuration process involves setting up three Cribl Stream instances \u2013 one Leader Node and two Worker Nodes. The Leader Node is set up in Leader mode and a unique, secure value is set for the Auth token. The Worker Nodes are then configured to report to the Leader Node to receive processing instructions. Once the Workers are configured and visible to the Leader, additional resources can be configured to get data flowing through the setup.",
            "url": "https://docs.cribl.io/stream/distributed-guide"
        },
        "reference": "url,title,text_chunks https://docs.cribl.io/stream/distributed-guide,Distributed\u00c2 Quick\u00c2 Start | Cribl Docs,\"This tutorial builds on our Getting Started Guide by walking you through a distributed deployment \u00e2\u0080\u0093 Cribl\u00c2 Stream\u2019s typical deployment type for production. For concepts and deeper details underlying the techniques presented here, see Distributed\u00c2 Deployment. To exercise these distributed features, you\u2019ll need the following prerequisites. This tutorial is tiered to accommodate different Cribl\u00c2 Stream license types: With a Cribl\u00c2 Stream Free, One, or Standard license, you\u2019ll install three Cribl\u00c2 Stream instances on one or multiple physical or virtual machines \u00e2\u0080\u0093 one\u00c2 Leader\u00c2 Node, and two Worker Nodes. To do the optional section on adding and managing multiple Worker Groups, you\u2019ll need an Enterprise or Sales\u00c2 Trial license/plan. Basic building blocks are identical to the Getting Started Guide. Please\u00c2 refer to the following sections of that tutorial for details, as needed: To set up the multiple instances you\u2019ll need, choose among the options below in Three\u00c2 Instances, Pick\u00c2 Any Medium. But first, let\u2019s lay out the division of labor in a single Worker\u00c2 Group. A distributed deployment enables Cribl\u00c2 Stream to scale out to handle higher data volumes, load-balancing with failover, and parallel data processing based on conditional mapping and routing. A single Leader Node manages multiple Worker\u00c2 Nodes. The Leader distributes and updates configuration on the Workers, handles version control, and monitors the Workers\u2019 health and activity metrics. The\u00c2 Workers do all the data processing. Here, we\u2019ll show how this works by configuring a Leader and two Workers. This\u00c2 configuration is compact, and can be demonstrated without an Enterprise (or other paid) license/plan. But\u00c2 you can extrapolate the same technique to setting up enterprise-scale deployments of hundreds of Workers, to handle petabytes of data. Cribl\u00c2 Stream Free, One, and Standard licenses support only a single Worker\u00c2 Group (named default), so in this example, all Workers share identical configuration. With\u00c2 an Enterprise license/plan, you can organize Workers into multiple Groups, with varying configurations to handle scenarios like on-premises versus cloud tech stacks, or data centers in different geographic locations. For\u00c2 details, see Worker Groups \u00e2\u0080\u0093 What Are They and Why You Should Care. You\u2019ll need to deploy three Cribl\u00c2 Stream instances \u00e2\u0080\u0093 with SSH access \u00e2\u0080\u0093 on one or more physical machines, virtual\u00c2 machines, or containers. If you haven\u2019t already provisioned this infrastructure, you have several alternatives, listed in the following subsections: Pick whichever approach will make it easiest for you to get the infrastructure launched \u00e2\u0080\u0093 based on familiarity, preference, or availability. Cribl\u2019s Docker containers come with Cribl\u00c2 Stream preinstalled. If you select any other option, be sure to both install and run Cribl\u00c2 Stream as the same Linux user. (For\u00c2 details on creating a new user \u00e2\u0080\u0093 addressing both systemd and initd distro\u2019s \u00e2\u0080\u0093 see Enabling\u00c2 Start\u00c2 on\u00c2 Boot.) You can use the Docker Compose (docker-compose.yml) file below to easily stand up a Cribl\u00c2 Stream distributed deployment of a Leader and multiple Workers on one machine. Before you use the Docker Compose file This uses a local directory, ~/cribl-config, as the configuration store for Cribl\u00c2 Stream. You must create this directory (it can be empty) before you run the docker\u00e2\u0080\u0091compose command. If you prefer to use ephemeral storage, you can delete line 8 (the CRIBL_VOLUME_DIR definition) and lines 11\u00e2\u0080\u009312 (the\u00c2 volumes configuration) before running the docker\u00e2\u0080\u0091compose command. But this will make it hard to stop and restart the same infrastructure, if you want to do the tutorial in chunks. To deploy a Leader Node, plus (e.g.) two Workers already configured and wired up to the Leader, use this command: To deploy a different number of Workers, just change the workers=2 value. By\u00c2 default, the above command pulls the freshest stable image (tagged cribl/cribl:latest) from Cribl\u2019s\u00c2 Docker\u00c2 Hub. It\u00c2 defaults to the following URLs and ports: If you\u2019re running the container itself on a virtual machine, replace localhost with the VM\u2019s IP address. The\u00c2 automatic assignment of available host-OS ports to the Workers prevents port collisions. Within\u00c2 the Docker container, these ports will forward over TCP to port 9000. To see the ports assigned on the OS, enter: docker ps You should see results like these: The PORTS column shows the host-OS ports on the left, forwarding to the container-internal ports on the right. You can use the docker_workers_N ports if you want to log directly into Workers. In the above example: If your Leader is crashing with two Workers, make sure you are allocating enough memory to Docker. Once your three instances are running, proceed to Configure Leader Instance. Use our Download page\u2019s curl command to directly install Cribl\u00c2 Stream onto your chosen infrastructure. For x64 processors, use: curl -Lso - $(curl https://cdn.cribl.io/dl/latest-x64) | tar zxv For ARM64 processors: curl -Lso - $(curl https://cdn.cribl.io/dl/latest-arm64) | tar zxv Once you\u2019ve configured the first Cribl\u00c2 Stream instance as a Leader, you can bootstrap Workers from the Leader. Or you can create Workers (with tags) from the Leader, using a curl command of this form: curl 'http://<leader-ip-or-hostname>:9000/init/install-worker.sh?token=criblmaster&tag=<tag1>&tag=<tag2>' E.g.: curl 'http://localhost:9000/init/install-worker.sh?token=criblmaster&tag=dev&tag=test'' Once your three instances are running, proceed to Configure Leader Instance. Amazon Lightsail provides a quick, simple way to deploy Cribl\u00c2 Stream to AWS instances. Amazon\u2019s Get\u00c2 Started with Linux/Unix-based Instances in Amazon Lightsail tutorial walks you through setting up your instances and connecting via SSH. Once your three instances are running, proceed to Configure Leader Instance. You can deploy Cribl\u00c2 Stream\u2019s AWS EC2 instances using Cribl\u2019s CloudFormation template, see our AWS/EC2 Quick Start Guide on GitHub. Follow the Cribl\u00c2 Stream Distributed instructions. (You\u2019ll be responsible for the costs of your AWS infrastructure.) If you prefer to deploy your own EC2 instances, the free tier is fine for this tutorial. Cribl recommends selecting Amazon\u00c2 Linux 2 (default\u00c2 CentOS) AMIs. Relevant instructions are linked below. See any of these AWS Docs for EC2 deployment: See these AWS instructions for SSH access: Once your three instances are running, proceed to Configure Leader Instance. Use Cribl\u2019s Helm charts to deploy Kubernetes pods, then proceed to the next section: Once you\u2019ve set up your Leader and Worker instances via your chosen approach above, you\u2019re ready to configure these instances and their communication. If you used an installation option like Docker Containers above, it has already preconfigured all three instances for you. This section and the next Configure\u00c2 Workers section will show you how to verify, and/or modify, these preset configurations. If you want to jump ahead, your next required configuration step is Add\u00c2 a\u00c2 Source, further down. Configure the first instance in Leader mode, and open port 4200. Log into the Leader. Depending on the deployment\u00c2 method you chose, this will be at http://<localhost\u00e2\u0080\u0091or\u00e2\u0080\u0091IP\u00e2\u0080\u0091address>:9000 or http://<localhost\u00e2\u0080\u0091or\u00e2\u0080\u0091IP\u00e2\u0080\u0091address>:19000. Use the default admin/admin credentials. Complete the registration form. From the UI\u2019s Settings > Global Settings > System > Distributed\u00c2 Settings > General\u00c2 Settings, select Mode:\u00c2 Leader. Click the Leader\u00c2 Settings left tab, and make sure the Port is set to 4200.(This\u00c2 port must be open for Workers to communicate with the Leader.) Click Save to restart the Cribl server in Leader mode. Keep the Leader\u2019s tab open, so that we can verify connectivity with the Workers after configuring them (in the next section). Change the value of Auth token from the default to a unique, secure value. Do this before adding any Workers. Optional but recommended: From Cribl\u00c2 Stream\u2019s top nav, click Manage\u00c2 > Groups. Then enable the UI\u00c2 access toggle for your Group(s). This\u00c2 way, you will be able to click through from the Leader\u2019s Manage\u00c2 > Workers tab to view and manage each Worker\u2019s UI, distinguished by a purple border. (This option handles authentication for you, so you don\u2019t need to manage or enter the Workers\u2019 credentials.) Next, configure the two other instances as Workers that report to the Leader to receive processing instructions. We\u2019ll configure one instance through the UI, and (optionally) bootstrap the other from the Leader. Log into the first Worker instance. Depending on the deployment\u00c2 method you chose, this will be at http://<localhost-or-IP-address>:9000 or at: http://<localhost-or-IP-address>:<automatically-assigned-host-port>. Use the default admin/admin credentials. Complete the registration form, if displayed. From the UI\u2019s Settings > Global\u00c2 Settings > Distributed\u00c2 Settings > Distributed\u00c2 Management > General\u00c2 Settings, select Mode:\u00c2 Stream: Managed\u00c2 Worker (managed by Leader). Leave other settings on this tab unchanged, including Default\u00c2 Group: default.(This is the literal name of the single Worker\u00c2 Group available with free licenses.) On the Leader\u00c2 Settings tab, make sure the Address matches the domain of the Leader you previously\u00c2 configured. On the same Leader\u00c2 Settings tab, display the Auth\u00c2 token value.(Optionally, you can change this from the default criblmaster.) Leave other settings unchanged, and click Save to restart this instance as a managed Worker. You can (if you choose) configure the second Worker instance using exactly the same procedure you used just above. But here, we\u2019ll offer you a simplified version of Cribl\u00c2 Stream\u2019s Bootstrapping\u00c2 Workers from Leader procedure for downloading the config from the Leader to the second Worker: First, switch to the terminal/console of the instance you\u2019ve reserved for this second Worker. Next, if you didn\u2019t change the default Auth\u00c2 token value when you previously\u00c2 configured the Leader, run this command as root user: If you cannot run as root, insert sudo as you pipe to the shell: To instead pipe to a bash shell: If you substituted a custom Auth\u00c2 token value on the Leader, enter: Or, for bash: The bootstrap script will install Cribl\u00c2 Stream into /opt/cribl on the target instance. With both Workers configured, next make sure they\u2019re visible to the Leader. Switch back to Cribl\u00c2 Stream\u2019s UI on the Leader instance you previously\u00c2 configured. From Cribl\u00c2 Stream\u2019s top nav, click Manage. Below that, click the Workers tab. On the resulting Manage\u00c2 Workers page, you should now see both Workers, mapped to the default Group. If one or both Workers are missing, repeat the preceding Configure\u00c2 Worker via\u00c2 UI and/or Bootstrap\u00c2 Worker from Leader procedures until the Workers show up. Otherwise, if both Workers are present, we can now configure a few more resources to get data flowing through this distributed setup. If you\u2019re interested in details about the communication among Cribl\u00c2 Stream instances, see How\u00c2 Do Workers and Leader Work Together. Once you\u2019ve configured a Worker to point to the Leader\u00c2 Node, the Leader will assign the Worker a new, random admin password. This secures each Worker from unintended access. If you need to reconfigure the Worker later, you can either: To minimize dependencies, this section walks you through enabling a Cribl\u00c2 Stream built-in Datagen Source to get some (fake) events flowing into your Workers. (This is the same approach used in our single-instance Getting Started Guide tutorial, but using a different datagen.) If you prefer to configure Cribl\u00c2 Stream to receive events from a real data input that you\u2019ve already set up, see our Sources topic for a link to the appropriate instructions. In the Leader UI\u2019s top nav, click Manage\u00c2 > Groups. From the resulting Groups page, click the Group name.(Working with a free license, we\u2019re implicitly configuring this Source on the default Group.) From the top nav, select Manage > Data > Sources. From the Data\u00c2 Sources page\u2019s tiles or left menu, select Datagen.(You can use the search box to jump to the Datagen tile.) Click Add\u00c2 Source to open a New\u00c2 Source modal. In the Input ID field, name this Source weblog (or any unique name). In the Data Generator File drop-down, select weblog.log.This generates\u2026simulated log events for a website. Keep the Events per Second per Worker Node at the default 10\u00c2 EPS for now. Click Save. In the Enabled column, the toggle set to Yes indicates that your Datagen Source has started generating sample data. If you\u2019d like to verify that the Datagen is sending events, wait about a minute for them to start accumulating. Then, on the Manage\u00c2 Sources / Datagen page, click the Live button beside your configured Source. On the resulting Live\u00c2 Data tab, you should see events arrive within the default 10-second capture. On the output side, choose any of these options: To configure a realistic output, but"
    },
    {
        "question_answer": {
            "question": "What are the prerequisites for exercising these distributed features?",
            "answer": "To exercise these distributed features, you\u2019ll need to install three Cribl Stream instances on one or multiple physical or virtual machines \u2013 one Leader Node, and two Worker Nodes. If you want to do the optional section on adding and managing multiple Worker Groups, you\u2019ll need an Enterprise or Sales Trial license/plan.",
            "follow_up_question_1": "What are the different Cribl Stream license types?",
            "follow_up_answer_1": "Cribl Stream offers different license types including Free, One, Standard, and Enterprise. The Free, One, and Standard licenses support only a single Worker Group, while the Enterprise license allows you to organize Workers into multiple Groups with varying configurations.",
            "follow_up_question_2": "What are the benefits of having an Enterprise license?",
            "follow_up_answer_2": "With an Enterprise license, you can organize Workers into multiple Groups, with varying configurations to handle scenarios like on-premises versus cloud tech stacks, or data centers in different geographic locations. This provides more flexibility and scalability for larger deployments.",
            "follow_up_question_3": "What if I only have a Free, One, or Standard license?",
            "follow_up_answer_3": "With a Free, One, or Standard license, you can still install three Cribl Stream instances on one or multiple physical or virtual machines \u2013 one Leader Node, and two Worker Nodes. However, all Workers will share identical configuration as these licenses support only a single Worker Group.",
            "url": "https://docs.cribl.io/stream/distributed-guide"
        },
        "reference": "url,title,text_chunks https://docs.cribl.io/stream/distributed-guide,Distributed\u00c2 Quick\u00c2 Start | Cribl Docs,\"This tutorial builds on our Getting Started Guide by walking you through a distributed deployment \u00e2\u0080\u0093 Cribl\u00c2 Stream\u2019s typical deployment type for production. For concepts and deeper details underlying the techniques presented here, see Distributed\u00c2 Deployment. To exercise these distributed features, you\u2019ll need the following prerequisites. This tutorial is tiered to accommodate different Cribl\u00c2 Stream license types: With a Cribl\u00c2 Stream Free, One, or Standard license, you\u2019ll install three Cribl\u00c2 Stream instances on one or multiple physical or virtual machines \u00e2\u0080\u0093 one\u00c2 Leader\u00c2 Node, and two Worker Nodes. To do the optional section on adding and managing multiple Worker Groups, you\u2019ll need an Enterprise or Sales\u00c2 Trial license/plan. Basic building blocks are identical to the Getting Started Guide. Please\u00c2 refer to the following sections of that tutorial for details, as needed: To set up the multiple instances you\u2019ll need, choose among the options below in Three\u00c2 Instances, Pick\u00c2 Any Medium. But first, let\u2019s lay out the division of labor in a single Worker\u00c2 Group. A distributed deployment enables Cribl\u00c2 Stream to scale out to handle higher data volumes, load-balancing with failover, and parallel data processing based on conditional mapping and routing. A single Leader Node manages multiple Worker\u00c2 Nodes. The Leader distributes and updates configuration on the Workers, handles version control, and monitors the Workers\u2019 health and activity metrics. The\u00c2 Workers do all the data processing. Here, we\u2019ll show how this works by configuring a Leader and two Workers. This\u00c2 configuration is compact, and can be demonstrated without an Enterprise (or other paid) license/plan. But\u00c2 you can extrapolate the same technique to setting up enterprise-scale deployments of hundreds of Workers, to handle petabytes of data. Cribl\u00c2 Stream Free, One, and Standard licenses support only a single Worker\u00c2 Group (named default), so in this example, all Workers share identical configuration. With\u00c2 an Enterprise license/plan, you can organize Workers into multiple Groups, with varying configurations to handle scenarios like on-premises versus cloud tech stacks, or data centers in different geographic locations. For\u00c2 details, see Worker Groups \u00e2\u0080\u0093 What Are They and Why You Should Care. You\u2019ll need to deploy three Cribl\u00c2 Stream instances \u00e2\u0080\u0093 with SSH access \u00e2\u0080\u0093 on one or more physical machines, virtual\u00c2 machines, or containers. If you haven\u2019t already provisioned this infrastructure, you have several alternatives, listed in the following subsections: Pick whichever approach will make it easiest for you to get the infrastructure launched \u00e2\u0080\u0093 based on familiarity, preference, or availability. Cribl\u2019s Docker containers come with Cribl\u00c2 Stream preinstalled. If you select any other option, be sure to both install and run Cribl\u00c2 Stream as the same Linux user. (For\u00c2 details on creating a new user \u00e2\u0080\u0093 addressing both systemd and initd distro\u2019s \u00e2\u0080\u0093 see Enabling\u00c2 Start\u00c2 on\u00c2 Boot.) You can use the Docker Compose (docker-compose.yml) file below to easily stand up a Cribl\u00c2 Stream distributed deployment of a Leader and multiple Workers on one machine. Before you use the Docker Compose file This uses a local directory, ~/cribl-config, as the configuration store for Cribl\u00c2 Stream. You must create this directory (it can be empty) before you run the docker\u00e2\u0080\u0091compose command. If you prefer to use ephemeral storage, you can delete line 8 (the CRIBL_VOLUME_DIR definition) and lines 11\u00e2\u0080\u009312 (the\u00c2 volumes configuration) before running the docker\u00e2\u0080\u0091compose command. But this will make it hard to stop and restart the same infrastructure, if you want to do the tutorial in chunks. To deploy a Leader Node, plus (e.g.) two Workers already configured and wired up to the Leader, use this command: To deploy a different number of Workers, just change the workers=2 value. By\u00c2 default, the above command pulls the freshest stable image (tagged cribl/cribl:latest) from Cribl\u2019s\u00c2 Docker\u00c2 Hub. It\u00c2 defaults to the following URLs and ports: If you\u2019re running the container itself on a virtual machine, replace localhost with the VM\u2019s IP address. The\u00c2 automatic assignment of available host-OS ports to the Workers prevents port collisions. Within\u00c2 the Docker container, these ports will forward over TCP to port 9000. To see the ports assigned on the OS, enter: docker ps You should see results like these: The PORTS column shows the host-OS ports on the left, forwarding to the container-internal ports on the right. You can use the docker_workers_N ports if you want to log directly into Workers. In the above example: If your Leader is crashing with two Workers, make sure you are allocating enough memory to Docker. Once your three instances are running, proceed to Configure Leader Instance. Use our Download page\u2019s curl command to directly install Cribl\u00c2 Stream onto your chosen infrastructure. For x64 processors, use: curl -Lso - $(curl https://cdn.cribl.io/dl/latest-x64) | tar zxv For ARM64 processors: curl -Lso - $(curl https://cdn.cribl.io/dl/latest-arm64) | tar zxv Once you\u2019ve configured the first Cribl\u00c2 Stream instance as a Leader, you can bootstrap Workers from the Leader. Or you can create Workers (with tags) from the Leader, using a curl command of this form: curl 'http://<leader-ip-or-hostname>:9000/init/install-worker.sh?token=criblmaster&tag=<tag1>&tag=<tag2>' E.g.: curl 'http://localhost:9000/init/install-worker.sh?token=criblmaster&tag=dev&tag=test'' Once your three instances are running, proceed to Configure Leader Instance. Amazon Lightsail provides a quick, simple way to deploy Cribl\u00c2 Stream to AWS instances. Amazon\u2019s Get\u00c2 Started with Linux/Unix-based Instances in Amazon Lightsail tutorial walks you through setting up your instances and connecting via SSH. Once your three instances are running, proceed to Configure Leader Instance. You can deploy Cribl\u00c2 Stream\u2019s AWS EC2 instances using Cribl\u2019s CloudFormation template, see our AWS/EC2 Quick Start Guide on GitHub. Follow the Cribl\u00c2 Stream Distributed instructions. (You\u2019ll be responsible for the costs of your AWS infrastructure.) If you prefer to deploy your own EC2 instances, the free tier is fine for this tutorial. Cribl recommends selecting Amazon\u00c2 Linux 2 (default\u00c2 CentOS) AMIs. Relevant instructions are linked below. See any of these AWS Docs for EC2 deployment: See these AWS instructions for SSH access: Once your three instances are running, proceed to Configure Leader Instance. Use Cribl\u2019s Helm charts to deploy Kubernetes pods, then proceed to the next section: Once you\u2019ve set up your Leader and Worker instances via your chosen approach above, you\u2019re ready to configure these instances and their communication. If you used an installation option like Docker Containers above, it has already preconfigured all three instances for you. This section and the next Configure\u00c2 Workers section will show you how to verify, and/or modify, these preset configurations. If you want to jump ahead, your next required configuration step is Add\u00c2 a\u00c2 Source, further down. Configure the first instance in Leader mode, and open port 4200. Log into the Leader. Depending on the deployment\u00c2 method you chose, this will be at http://<localhost\u00e2\u0080\u0091or\u00e2\u0080\u0091IP\u00e2\u0080\u0091address>:9000 or http://<localhost\u00e2\u0080\u0091or\u00e2\u0080\u0091IP\u00e2\u0080\u0091address>:19000. Use the default admin/admin credentials. Complete the registration form. From the UI\u2019s Settings > Global Settings > System > Distributed\u00c2 Settings > General\u00c2 Settings, select Mode:\u00c2 Leader. Click the Leader\u00c2 Settings left tab, and make sure the Port is set to 4200.(This\u00c2 port must be open for Workers to communicate with the Leader.) Click Save to restart the Cribl server in Leader mode. Keep the Leader\u2019s tab open, so that we can verify connectivity with the Workers after configuring them (in the next section). Change the value of Auth token from the default to a unique, secure value. Do this before adding any Workers. Optional but recommended: From Cribl\u00c2 Stream\u2019s top nav, click Manage\u00c2 > Groups. Then enable the UI\u00c2 access toggle for your Group(s). This\u00c2 way, you will be able to click through from the Leader\u2019s Manage\u00c2 > Workers tab to view and manage each Worker\u2019s UI, distinguished by a purple border. (This option handles authentication for you, so you don\u2019t need to manage or enter the Workers\u2019 credentials.) Next, configure the two other instances as Workers that report to the Leader to receive processing instructions. We\u2019ll configure one instance through the UI, and (optionally) bootstrap the other from the Leader. Log into the first Worker instance. Depending on the deployment\u00c2 method you chose, this will be at http://<localhost-or-IP-address>:9000 or at: http://<localhost-or-IP-address>:<automatically-assigned-host-port>. Use the default admin/admin credentials. Complete the registration form, if displayed. From the UI\u2019s Settings > Global\u00c2 Settings > Distributed\u00c2 Settings > Distributed\u00c2 Management > General\u00c2 Settings, select Mode:\u00c2 Stream: Managed\u00c2 Worker (managed by Leader). Leave other settings on this tab unchanged, including Default\u00c2 Group: default.(This is the literal name of the single Worker\u00c2 Group available with free licenses.) On the Leader\u00c2 Settings tab, make sure the Address matches the domain of the Leader you previously\u00c2 configured. On the same Leader\u00c2 Settings tab, display the Auth\u00c2 token value.(Optionally, you can change this from the default criblmaster.) Leave other settings unchanged, and click Save to restart this instance as a managed Worker. You can (if you choose) configure the second Worker instance using exactly the same procedure you used just above. But here, we\u2019ll offer you a simplified version of Cribl\u00c2 Stream\u2019s Bootstrapping\u00c2 Workers from Leader procedure for downloading the config from the Leader to the second Worker: First, switch to the terminal/console of the instance you\u2019ve reserved for this second Worker. Next, if you didn\u2019t change the default Auth\u00c2 token value when you previously\u00c2 configured the Leader, run this command as root user: If you cannot run as root, insert sudo as you pipe to the shell: To instead pipe to a bash shell: If you substituted a custom Auth\u00c2 token value on the Leader, enter: Or, for bash: The bootstrap script will install Cribl\u00c2 Stream into /opt/cribl on the target instance. With both Workers configured, next make sure they\u2019re visible to the Leader. Switch back to Cribl\u00c2 Stream\u2019s UI on the Leader instance you previously\u00c2 configured. From Cribl\u00c2 Stream\u2019s top nav, click Manage. Below that, click the Workers tab. On the resulting Manage\u00c2 Workers page, you should now see both Workers, mapped to the default Group. If one or both Workers are missing, repeat the preceding Configure\u00c2 Worker via\u00c2 UI and/or Bootstrap\u00c2 Worker from Leader procedures until the Workers show up. Otherwise, if both Workers are present, we can now configure a few more resources to get data flowing through this distributed setup. If you\u2019re interested in details about the communication among Cribl\u00c2 Stream instances, see How\u00c2 Do Workers and Leader Work Together. Once you\u2019ve configured a Worker to point to the Leader\u00c2 Node, the Leader will assign the Worker a new, random admin password. This secures each Worker from unintended access. If you need to reconfigure the Worker later, you can either: To minimize dependencies, this section walks you through enabling a Cribl\u00c2 Stream built-in Datagen Source to get some (fake) events flowing into your Workers. (This is the same approach used in our single-instance Getting Started Guide tutorial, but using a different datagen.) If you prefer to configure Cribl\u00c2 Stream to receive events from a real data input that you\u2019ve already set up, see our Sources topic for a link to the appropriate instructions. In the Leader UI\u2019s top nav, click Manage\u00c2 > Groups. From the resulting Groups page, click the Group name.(Working with a free license, we\u2019re implicitly configuring this Source on the default Group.) From the top nav, select Manage > Data > Sources. From the Data\u00c2 Sources page\u2019s tiles or left menu, select Datagen.(You can use the search box to jump to the Datagen tile.) Click Add\u00c2 Source to open a New\u00c2 Source modal. In the Input ID field, name this Source weblog (or any unique name). In the Data Generator File drop-down, select weblog.log.This generates\u2026simulated log events for a website. Keep the Events per Second per Worker Node at the default 10\u00c2 EPS for now. Click Save. In the Enabled column, the toggle set to Yes indicates that your Datagen Source has started generating sample data. If you\u2019d like to verify that the Datagen is sending events, wait about a minute for them to start accumulating. Then, on the Manage\u00c2 Sources / Datagen page, click the Live button beside your configured Source. On the resulting Live\u00c2 Data tab, you should see events arrive within the default 10-second capture. On the output side, choose any of these options: To configure a realistic output, but"
    },
    {
        "question_answer": {
            "question": "What are the options for deploying the Cribl Stream instances?",
            "answer": "You have several alternatives for deploying the Cribl Stream instances. You can use Docker containers, which come with Cribl Stream preinstalled. You can also directly install Cribl Stream onto your chosen infrastructure using a curl command. Other options include deploying to AWS instances using Amazon Lightsail or Cribl\u2019s CloudFormation template, or deploying Kubernetes pods using Cribl\u2019s Helm charts.",
            "follow_up_question_1": "What are the advantages of using Docker containers for deployment?",
            "follow_up_answer_1": "Cribl\u2019s Docker containers come with Cribl Stream preinstalled, which can simplify the deployment process. They also allow you to easily stand up a Cribl Stream distributed deployment of a Leader and multiple Workers on one machine.",
            "follow_up_question_2": "What if I choose to deploy to AWS instances?",
            "follow_up_answer_2": "If you choose to deploy to AWS instances, you can use Amazon Lightsail, which provides a quick, simple way to deploy Cribl Stream. Alternatively, you can use Cribl\u2019s CloudFormation template for deploying Cribl Stream\u2019s AWS EC2 instances. However, you\u2019ll be responsible for the costs of your AWS infrastructure.",
            "follow_up_question_3": "And what about deploying Kubernetes pods using Cribl\u2019s Helm charts?",
            "follow_up_answer_3": "Deploying Kubernetes pods using Cribl\u2019s Helm charts is another option. This allows you to deploy Cribl Stream in a Kubernetes environment, which can provide benefits in terms of scalability and management of your Cribl Stream instances.",
            "url": "https://docs.cribl.io/stream/distributed-guide"
        },
        "reference": "url,title,text_chunks https://docs.cribl.io/stream/distributed-guide,Distributed\u00c2 Quick\u00c2 Start | Cribl Docs,\"This tutorial builds on our Getting Started Guide by walking you through a distributed deployment \u00e2\u0080\u0093 Cribl\u00c2 Stream\u2019s typical deployment type for production. For concepts and deeper details underlying the techniques presented here, see Distributed\u00c2 Deployment. To exercise these distributed features, you\u2019ll need the following prerequisites. This tutorial is tiered to accommodate different Cribl\u00c2 Stream license types: With a Cribl\u00c2 Stream Free, One, or Standard license, you\u2019ll install three Cribl\u00c2 Stream instances on one or multiple physical or virtual machines \u00e2\u0080\u0093 one\u00c2 Leader\u00c2 Node, and two Worker Nodes. To do the optional section on adding and managing multiple Worker Groups, you\u2019ll need an Enterprise or Sales\u00c2 Trial license/plan. Basic building blocks are identical to the Getting Started Guide. Please\u00c2 refer to the following sections of that tutorial for details, as needed: To set up the multiple instances you\u2019ll need, choose among the options below in Three\u00c2 Instances, Pick\u00c2 Any Medium. But first, let\u2019s lay out the division of labor in a single Worker\u00c2 Group. A distributed deployment enables Cribl\u00c2 Stream to scale out to handle higher data volumes, load-balancing with failover, and parallel data processing based on conditional mapping and routing. A single Leader Node manages multiple Worker\u00c2 Nodes. The Leader distributes and updates configuration on the Workers, handles version control, and monitors the Workers\u2019 health and activity metrics. The\u00c2 Workers do all the data processing. Here, we\u2019ll show how this works by configuring a Leader and two Workers. This\u00c2 configuration is compact, and can be demonstrated without an Enterprise (or other paid) license/plan. But\u00c2 you can extrapolate the same technique to setting up enterprise-scale deployments of hundreds of Workers, to handle petabytes of data. Cribl\u00c2 Stream Free, One, and Standard licenses support only a single Worker\u00c2 Group (named default), so in this example, all Workers share identical configuration. With\u00c2 an Enterprise license/plan, you can organize Workers into multiple Groups, with varying configurations to handle scenarios like on-premises versus cloud tech stacks, or data centers in different geographic locations. For\u00c2 details, see Worker Groups \u00e2\u0080\u0093 What Are They and Why You Should Care. You\u2019ll need to deploy three Cribl\u00c2 Stream instances \u00e2\u0080\u0093 with SSH access \u00e2\u0080\u0093 on one or more physical machines, virtual\u00c2 machines, or containers. If you haven\u2019t already provisioned this infrastructure, you have several alternatives, listed in the following subsections: Pick whichever approach will make it easiest for you to get the infrastructure launched \u00e2\u0080\u0093 based on familiarity, preference, or availability. Cribl\u2019s Docker containers come with Cribl\u00c2 Stream preinstalled. If you select any other option, be sure to both install and run Cribl\u00c2 Stream as the same Linux user. (For\u00c2 details on creating a new user \u00e2\u0080\u0093 addressing both systemd and initd distro\u2019s \u00e2\u0080\u0093 see Enabling\u00c2 Start\u00c2 on\u00c2 Boot.) You can use the Docker Compose (docker-compose.yml) file below to easily stand up a Cribl\u00c2 Stream distributed deployment of a Leader and multiple Workers on one machine. Before you use the Docker Compose file This uses a local directory, ~/cribl-config, as the configuration store for Cribl\u00c2 Stream. You must create this directory (it can be empty) before you run the docker\u00e2\u0080\u0091compose command. If you prefer to use ephemeral storage, you can delete line 8 (the CRIBL_VOLUME_DIR definition) and lines 11\u00e2\u0080\u009312 (the\u00c2 volumes configuration) before running the docker\u00e2\u0080\u0091compose command. But this will make it hard to stop and restart the same infrastructure, if you want to do the tutorial in chunks. To deploy a Leader Node, plus (e.g.) two Workers already configured and wired up to the Leader, use this command: To deploy a different number of Workers, just change the workers=2 value. By\u00c2 default, the above command pulls the freshest stable image (tagged cribl/cribl:latest) from Cribl\u2019s\u00c2 Docker\u00c2 Hub. It\u00c2 defaults to the following URLs and ports: If you\u2019re running the container itself on a virtual machine, replace localhost with the VM\u2019s IP address. The\u00c2 automatic assignment of available host-OS ports to the Workers prevents port collisions. Within\u00c2 the Docker container, these ports will forward over TCP to port 9000. To see the ports assigned on the OS, enter: docker ps You should see results like these: The PORTS column shows the host-OS ports on the left, forwarding to the container-internal ports on the right. You can use the docker_workers_N ports if you want to log directly into Workers. In the above example: If your Leader is crashing with two Workers, make sure you are allocating enough memory to Docker. Once your three instances are running, proceed to Configure Leader Instance. Use our Download page\u2019s curl command to directly install Cribl\u00c2 Stream onto your chosen infrastructure. For x64 processors, use: curl -Lso - $(curl https://cdn.cribl.io/dl/latest-x64) | tar zxv For ARM64 processors: curl -Lso - $(curl https://cdn.cribl.io/dl/latest-arm64) | tar zxv Once you\u2019ve configured the first Cribl\u00c2 Stream instance as a Leader, you can bootstrap Workers from the Leader. Or you can create Workers (with tags) from the Leader, using a curl command of this form: curl 'http://<leader-ip-or-hostname>:9000/init/install-worker.sh?token=criblmaster&tag=<tag1>&tag=<tag2>' E.g.: curl 'http://localhost:9000/init/install-worker.sh?token=criblmaster&tag=dev&tag=test'' Once your three instances are running, proceed to Configure Leader Instance. Amazon Lightsail provides a quick, simple way to deploy Cribl\u00c2 Stream to AWS instances. Amazon\u2019s Get\u00c2 Started with Linux/Unix-based Instances in Amazon Lightsail tutorial walks you through setting up your instances and connecting via SSH. Once your three instances are running, proceed to Configure Leader Instance. You can deploy Cribl\u00c2 Stream\u2019s AWS EC2 instances using Cribl\u2019s CloudFormation template, see our AWS/EC2 Quick Start Guide on GitHub. Follow the Cribl\u00c2 Stream Distributed instructions. (You\u2019ll be responsible for the costs of your AWS infrastructure.) If you prefer to deploy your own EC2 instances, the free tier is fine for this tutorial. Cribl recommends selecting Amazon\u00c2 Linux 2 (default\u00c2 CentOS) AMIs. Relevant instructions are linked below. See any of these AWS Docs for EC2 deployment: See these AWS instructions for SSH access: Once your three instances are running, proceed to Configure Leader Instance. Use Cribl\u2019s Helm charts to deploy Kubernetes pods, then proceed to the next section: Once you\u2019ve set up your Leader and Worker instances via your chosen approach above, you\u2019re ready to configure these instances and their communication. If you used an installation option like Docker Containers above, it has already preconfigured all three instances for you. This section and the next Configure\u00c2 Workers section will show you how to verify, and/or modify, these preset configurations. If you want to jump ahead, your next required configuration step is Add\u00c2 a\u00c2 Source, further down. Configure the first instance in Leader mode, and open port 4200. Log into the Leader. Depending on the deployment\u00c2 method you chose, this will be at http://<localhost\u00e2\u0080\u0091or\u00e2\u0080\u0091IP\u00e2\u0080\u0091address>:9000 or http://<localhost\u00e2\u0080\u0091or\u00e2\u0080\u0091IP\u00e2\u0080\u0091address>:19000. Use the default admin/admin credentials. Complete the registration form. From the UI\u2019s Settings > Global Settings > System > Distributed\u00c2 Settings > General\u00c2 Settings, select Mode:\u00c2 Leader. Click the Leader\u00c2 Settings left tab, and make sure the Port is set to 4200.(This\u00c2 port must be open for Workers to communicate with the Leader.) Click Save to restart the Cribl server in Leader mode. Keep the Leader\u2019s tab open, so that we can verify connectivity with the Workers after configuring them (in the next section). Change the value of Auth token from the default to a unique, secure value. Do this before adding any Workers. Optional but recommended: From Cribl\u00c2 Stream\u2019s top nav, click Manage\u00c2 > Groups. Then enable the UI\u00c2 access toggle for your Group(s). This\u00c2 way, you will be able to click through from the Leader\u2019s Manage\u00c2 > Workers tab to view and manage each Worker\u2019s UI, distinguished by a purple border. (This option handles authentication for you, so you don\u2019t need to manage or enter the Workers\u2019 credentials.) Next, configure the two other instances as Workers that report to the Leader to receive processing instructions. We\u2019ll configure one instance through the UI, and (optionally) bootstrap the other from the Leader. Log into the first Worker instance. Depending on the deployment\u00c2 method you chose, this will be at http://<localhost-or-IP-address>:9000 or at: http://<localhost-or-IP-address>:<automatically-assigned-host-port>. Use the default admin/admin credentials. Complete the registration form, if displayed. From the UI\u2019s Settings > Global\u00c2 Settings > Distributed\u00c2 Settings > Distributed\u00c2 Management > General\u00c2 Settings, select Mode:\u00c2 Stream: Managed\u00c2 Worker (managed by Leader). Leave other settings on this tab unchanged, including Default\u00c2 Group: default.(This is the literal name of the single Worker\u00c2 Group available with free licenses.) On the Leader\u00c2 Settings tab, make sure the Address matches the domain of the Leader you previously\u00c2 configured. On the same Leader\u00c2 Settings tab, display the Auth\u00c2 token value.(Optionally, you can change this from the default criblmaster.) Leave other settings unchanged, and click Save to restart this instance as a managed Worker. You can (if you choose) configure the second Worker instance using exactly the same procedure you used just above. But here, we\u2019ll offer you a simplified version of Cribl\u00c2 Stream\u2019s Bootstrapping\u00c2 Workers from Leader procedure for downloading the config from the Leader to the second Worker: First, switch to the terminal/console of the instance you\u2019ve reserved for this second Worker. Next, if you didn\u2019t change the default Auth\u00c2 token value when you previously\u00c2 configured the Leader, run this command as root user: If you cannot run as root, insert sudo as you pipe to the shell: To instead pipe to a bash shell: If you substituted a custom Auth\u00c2 token value on the Leader, enter: Or, for bash: The bootstrap script will install Cribl\u00c2 Stream into /opt/cribl on the target instance. With both Workers configured, next make sure they\u2019re visible to the Leader. Switch back to Cribl\u00c2 Stream\u2019s UI on the Leader instance you previously\u00c2 configured. From Cribl\u00c2 Stream\u2019s top nav, click Manage. Below that, click the Workers tab. On the resulting Manage\u00c2 Workers page, you should now see both Workers, mapped to the default Group. If one or both Workers are missing, repeat the preceding Configure\u00c2 Worker via\u00c2 UI and/or Bootstrap\u00c2 Worker from Leader procedures until the Workers show up. Otherwise, if both Workers are present, we can now configure a few more resources to get data flowing through this distributed setup. If you\u2019re interested in details about the communication among Cribl\u00c2 Stream instances, see How\u00c2 Do Workers and Leader Work Together. Once you\u2019ve configured a Worker to point to the Leader\u00c2 Node, the Leader will assign the Worker a new, random admin password. This secures each Worker from unintended access. If you need to reconfigure the Worker later, you can either: To minimize dependencies, this section walks you through enabling a Cribl\u00c2 Stream built-in Datagen Source to get some (fake) events flowing into your Workers. (This is the same approach used in our single-instance Getting Started Guide tutorial, but using a different datagen.) If you prefer to configure Cribl\u00c2 Stream to receive events from a real data input that you\u2019ve already set up, see our Sources topic for a link to the appropriate instructions. In the Leader UI\u2019s top nav, click Manage\u00c2 > Groups. From the resulting Groups page, click the Group name.(Working with a free license, we\u2019re implicitly configuring this Source on the default Group.) From the top nav, select Manage > Data > Sources. From the Data\u00c2 Sources page\u2019s tiles or left menu, select Datagen.(You can use the search box to jump to the Datagen tile.) Click Add\u00c2 Source to open a New\u00c2 Source modal. In the Input ID field, name this Source weblog (or any unique name). In the Data Generator File drop-down, select weblog.log.This generates\u2026simulated log events for a website. Keep the Events per Second per Worker Node at the default 10\u00c2 EPS for now. Click Save. In the Enabled column, the toggle set to Yes indicates that your Datagen Source has started generating sample data. If you\u2019d like to verify that the Datagen is sending events, wait about a minute for them to start accumulating. Then, on the Manage\u00c2 Sources / Datagen page, click the Live button beside your configured Source. On the resulting Live\u00c2 Data tab, you should see events arrive within the default 10-second capture. On the output side, choose any of these options: To configure a realistic output, but"
    },
    {
        "question_answer": {
            "question": "What is the purpose of the Simulated Splunk Destination in Cribl Stream?",
            "answer": "The Simulated Splunk Destination in Cribl Stream is used to configure a typical Splunk Destination, but with the use of netcat to spoof the receiver on port 9997. This allows you to test and simulate data flow without needing a real receiver set up.",
            "follow_up_question_1": "What is the role of netcat in this process?",
            "follow_up_answer_1": "Netcat is a networking utility that reads and writes data across network connections. In this context, it's used to spoof the receiver, essentially acting as a stand-in for the real receiver during the simulation.",
            "follow_up_question_2": "What if netcat is not installed on my Linux instance?",
            "follow_up_answer_2": "If netcat is not installed on your Linux instance, you can follow your Linux distribution's steps for installing netcat. For example, if you're using Ubuntu, you can install netcat using the commands 'apt update' and 'apt install netcat'.",
            "follow_up_question_3": "What happens if data isn't flowing during the simulation?",
            "follow_up_answer_3": "If data isn't flowing during the simulation, you might need to restart Workers. You can do this through Cribl Stream\u2019s UI. If you're using Docker containers, you can use the commands 'docker-compose down', followed by 'docker-compose up'.",
            "url": "https://docs.cribl.io/stream/distributed-guide"
        },
        "reference": "with no real dependencies and no license requirement, follow the Simulated\u00c2 Splunk\u00c2 Destination instructions just below. For a simpler option, jump to the Internal\u00c2 Destination instructions further down. To send data to a real receiver that you\u2019ve already set up, see our Destinations topic for a link to the appropriate instructions. Here, you\u2019ll go through the steps of configuring a typical Splunk Destination, but you\u2019ll use netcat to spoof the receiver on port 9997. In the Cribl\u00c2 Stream Leader\u2019s UI, configure a Splunk\u00c2 Single Instance Destination, following these\u00c2 instructions. For this simulated output: Click Commit at the upper right, enter a commit message, and confirm the commit. Click Deploy at the upper right to deploy this new configuration to your Workers. In the first Worker instance\u2019s terminal/console, shell in, then enter cd\u00c2 /opt/cribl/bin to access Cribl\u00c2 Stream\u2019s CLI. Enter nc\u00c2 -h to check whether netcat is installed on this Linux instance. If the command fails, follow your Linux distro\u2019s steps for installing netcat. (E.g.,\u00c2 for Ubuntu instructions, see Docker\u00c2 Notes below.) If you\u2019re using a container like Docker, before shelling in at step 4 above, you\u2019ll need to first open a shell inside that container: docker exec -it <CONTAINER\u00c2 ID> /bin/bash In the above Docker Containers deployment example, you\u2019d want to open the shell on the docker_workers_1 container, whose <CONTAINER\u00c2 ID> was a3de9ea8f46f. Cribl\u2019s Docker containers currently run Ubuntu\u00c2 20.04.6. You can install netcat with this sequence of commands: apt updateapt install netcat The data you\u2019ll now see displayed in the terminal will be gibberish, because of Splunk\u2019s proprietary data format. If data isn\u2019t flowing, you might need to restart Workers. You can do this through Cribl\u00c2 Stream\u2019s UI. With Docker containers, use docker-compose down, followed by docker-compose up. You can streamline future Commit and Deploy steps by entering a Default\u00c2 Commit\u00c2 Message, and by collapsing\u00c2 actions to a combined Commit\u00c2 and\u00c2 Deploy button. Both options are available at Settings > Global\u00c2 Settings > System > Git\u00c2 Settings > General. As an alternative to the Splunk instructions above, you can configure Cribl\u00c2 Stream\u2019s built-in DevNull Destination to capture events and discard them. (This is the same Destination used in our single-instance Getting Started Guide tutorial.) In the Leader UI\u2019s top nav, click Manage\u00c2 > Groups. From the resulting Groups tab, click the Group name.(Working with a free license, we\u2019re implicitly configuring this Source on the default Group.) From the top nav, select Data\u00c2 > Destinations. Select DevNull from the Data\u00c2 Destinations page\u2019s tiles or left menu.(You can use the search box to jump to the DevNull tile.) On the resulting devnull row, look for the Live indicator under Enabled. This confirms that the DevNull Destination is ready to accept events. From the Data\u00c2 Destinations page\u2019s left nav, select the Default Destination at the top. On the resulting Manage Default Destination page, verify that the Default\u00c2 Output ID drop-down points to the devnull Destination we just examined. Click Commit at the upper right, enter a commit message, and confirm the commit. Click Deploy at the upper right to deploy this new configuration to your Workers. To complete this distributed deployment with realistic infrastructure, let\u2019s set up a Pipeline and Route. If you\u2019ve already done the Getting\u00c2 Started Guide, you\u2019ve already created a slicendice Pipeline. In the following steps, skip ahead to adding an Eval\u00c2 Function. From the current Group\u2019s submenu, select Processing\u00c2 > Pipelines. At the Pipelines pane\u2019s upper right, click Add\u00c2 Pipeline, then select Create\u00c2 Pipeline. In the new Pipeline\u2019s ID field, enter a unique identifier (e.g., slicendice). Optionally, enter a **Description ** of this Pipeline\u2019s purpose. Click Save. Now add an Eval Function to the Pipeline: At the Pipelines pane\u2019s upper right, click Add\u00c2 Function. Search for the Eval Function, and click its link to add it. In the new Function\u2019s Evaluate\u00c2 Fields section, click Add\u00c2 Field. In the new row\u2019s Name column, name the field origin. In the new row\u2019s Value\u00c2 Expression column, enter: host+\"\" \"\"+source This new origin field will concatenate the host and source fields from incoming events. Click Save to store the Function\u2019s configuration. Commit and Deploy the new Pipeline configuration. Optionally, open the right pane and click Capture\u00c2 Data to verify throughput. If you\u2019ve already done the Getting\u00c2 Started Guide, you\u2019ve already created a demo Route, attached to the slicendice Pipeline. In the following steps, just modify the Route to send data to the new Destination you configured above. At the Pipelines page\u2019s top left, click Attach to Route.This displays the Data\u00c2 Routes page. Click Add\u00c2 Route. Enter a unique Route Name, like demo. Leave the Filter field set to its true default, allowing it to deliver all events. Set the Pipeline drop-down to our configured slicendice Pipeline. Set the Output drop-down to the Destination you configured above. If you boldly chose the Simulated\u00c2 Splunk\u00c2 Destination, this will be named something like splunk:splunk9997. You can leave the Description empty, and leave Final set to Yes. Grab the new Route by its left handle, and drag it to the top of the Routing table, so that our new Route will process events first. You should see something like the screenshot below. Click Save to save the new Route configuration. Commit and Deploy your changes. Still assuming you configured a simulated Splunk output, look at the terminal where you\u2019re running netcat. You should now see events arriving. With all our infrastructure in place, let\u2019s look at how a Cribl\u00c2 Stream distributed deployment scales up to balance the incoming event load among multiple Workers. In the Leader UI\u2019s top nav, click Manage\u00c2 > Groups. From the Groups page, click the Group name.(Working with a free license, we\u2019re implicitly configuring this Source on the default Group.) From the top nav, select Manage > Data > Sources, and find the Datagen Source you configured earlier. Click this Datagen\u2019s row to reopen its config modal. Reset the Events per Second per Worker Node from the default 10\u00c2 EPS to a high number, like 200\u00c2 EPS. Click Save, then Commit and Deploy this higher event load. From the Leader\u2019s top nav, also select Global\u00c2 Config\u00c2 > Commit to commit the Leader\u2019s newest config version. In\u00c2 the resulting modal, click Commit again to confirm. This uses Cribl\u00c2 Stream\u2019s git integration to save a global configuration point for your whole deployment, which you can roll back to. This completes your setup of a basic distributed deployment, using a free license, and configuring a single Worker\u00c2 Group of two identically configured Worker Processes. (You\u00c2 can extrapolate these same techniques you\u2019ve just mastered to spin up virtually any number of Workers in the same way.) To see how you can set up multiple Worker\u00c2 Groups \u00e2\u0080\u0093 with separate configurations optimized for separate data flows \u00e2\u0080\u0093 continue to the next section, noting its licensing prerequisites. If you\u2019re deferring or skipping that option, jump ahead to Cleaning\u00c2 Up. To add and manage more than one Worker Group \u00e2\u0080\u0093 everything in this optional section \u00e2\u0080\u0093 you\u2019ll need an Enterprise or Sales\u00c2 Trial license for your on-prem Cribl\u00c2 Stream deployment, or an Enterprise (or equivalent) plan for your Cribl.Cloud Organization. For details on adding multiple Worker Groups on Cribl.Cloud, see Cribl.Cloud\u00c2 Worker\u00c2 Groups. See Licensing for how to acquire and install one of the above license types. Install the license on your Leader instance, and then commit this as a Leader config change (top nav\u2019s Global\u00c2 Config dialog), before you proceed. Here, we\u2019ll build on the infrastructure we\u2019ve created so far to: To keep this Quick\u00c2 Start tutorial focused on techniques, rather than on configuring lots of infrastructure, we\u2019ll assign just one Worker to each Worker\u00c2 Group \u00e2\u0080\u0093 one of the two Workers we launched above. But\u00c2 in production,\u00c2 you\u2019ll be able to apply the same principles to setting up any number of Worker\u00c2 Groups, with any number of Workers. With an Enterprise or Sales Trial license/plan, Cribl\u00c2 Stream\u2019s UI adds some extra features. The Manage\u00c2 > Groups page, shown here, now supports multiple Groups. (For\u00c2 now, you still see only the single default Group \u00e2\u0080\u0093 but we\u2019ll change that a few sections down.) On Cribl.Cloud, the Groups page adds extra columns. You\u2019ll see each Group\u2019s Worker type (hybrid versus Cloud/Cribl-managed) and, for Cloud Groups, the anticipated ingress rate and Provisioned status. For details, see Cribl.Cloud\u00c2 Worker\u00c2 Groups. Note that if you\u2019ve enabled Worker\u00c2 UI access, you can click directly through to each of your Workers. This\u00c2 feature will come in handy just below. To\u00c2 try it out: For the remaining steps, we want to make sure both our Workers (configured earlier) are up. Cribl\u00c2 Stream\u2019s top header should indicate 2\u00c2 WORKERS. You can verify that they\u2019re alive by clicking the top nav\u2019s Manage\u00c2 > Workers tabs, as shown earlier. If so, proceed to Map\u00c2 Groups by Config. If either or both Workers are down, restart them: Make sure you\u2019ve enabled Worker UI access. (It\u2019s time!) To click through to a dormant Worker\u2019s UI, use either of the left-nav options covered just above: either the Workers page, or the Manage\u00c2 > Groups\u00c2 > default\u00c2 submenu of individual Worker IDs. When you see that Worker\u2019s UI (purple header), click\u00c2 Restart at the upper right. Confirm your choice, and wait for a Server\u00c2 has been restarted message to appear for a few seconds. Click the top nav\u2019s Manage option to return to the Leader\u2019s UI. If the other Worker is down, repeat the above steps to restart it as well. With both Workers confirmed up, let\u2019s look at how Cribl\u00c2 Stream has automatically mapped all these existing workers to the default Group. From the Leader\u2019s top nav, click Manage\u00c2 > Mappings. You now see a default Mapping Ruleset, also literally named default. Click it. A Cribl\u00c2 Stream Leader can have multiple Mapping\u00c2 Rulesets configured, but only one can be active at a time. (The default Ruleset and Rule naming are separate from the default Group\u2019s naming. All of these out-of-the-box starting configurations have been named\u2026literally.) Below the Rule\u00c2 Name, a Mapping Rule has two functional fields: So this is a catch-all rule. By following it, the Leader has assigned all (both) registered Workers to the default Group. Let\u2019s make a more-specific rule, mapping a specific Worker (by hostname) to this Group, which receives events from our weblog Datagen Source. Click Add\u00c2 Rule at the upper right. Then configure the new Rule as shown below: Click Save to add this Rule. Confirm the warning that changes will take effect immediately. From the top nav, select Global\u00c2 Config > Commit to commit the Leader\u2019s new config. For more Mapping Rules/Rulesets details and examples, see Distributed\u00c2 Deployment. Next, we\u2019ll add a second Worker\u00c2 Group; add a second Source (relaying Cribl\u00c2 Stream\u2019s internal metrics); and then add another Mapping\u00c2 Rule, to map our second Worker to the new Group. From the Leader\u2019s top nav, click Manage\u00c2 > Groups. On the resulting Groups page, click New\u00c2 Group. Name the new Group CriblMetrics to match its purpose. Toggle UI\u00c2 access to Yes. Save the Group. Click Deploy on the new Group\u2019s row, and confirm your choice. The new Group should deploy immediately. On the new Group, we\u2019ll now enable a Cribl\u00c2 Internal: Metrics Source, representing a second data type. From the Leader\u2019s top nav, click Manage > Groups > CriblMetrics. From the resulting submenu, click Data\u00c2 > Sources. From the Data\u00c2 Sources page\u2019s tiles or left menu, select Cribl\u00c2 Internal. On the Manage\u00c2 Sources / Cribl\u00c2 Internal page, toggle Enable to Yes in the CriblMetrics row. Confirm that you want to enable CriblMetrics. From the top nav, select Global\u00c2 Config\u00c2 > Commit to commit the new config on the Leader and Group. Click Deploy at the upper right to deploy this new configuration to your Workers. Now let\u2019s map this new group to the CriblMetrics Source\u2019s incoming events: From the Leader\u2019s top nav, click Manage\u00c2 > Groups. From the Groups page, click the Mappings tab. Click the default Mapping Ruleset to open it. Click Add\u00c2 Rule at the upper right. Then configure the new Rule as shown"
    },
    {
        "question_answer": {
            "question": "What is the DevNull Destination in Cribl Stream?",
            "answer": "The DevNull Destination in Cribl Stream is a built-in feature that captures events and discards them. It's useful for testing and debugging, as it allows you to see if events are being generated and sent correctly without actually storing or processing them.",
            "follow_up_question_1": "How can I confirm that the DevNull Destination is ready to accept events?",
            "follow_up_answer_1": "You can confirm that the DevNull Destination is ready to accept events by looking for the 'Live' indicator under 'Enabled' on the 'devnull' row in the Data Destinations page.",
            "follow_up_question_2": "What is the purpose of the 'Default Output ID' drop-down?",
            "follow_up_answer_2": "The 'Default Output ID' drop-down is used to specify which Destination should be used by default for output. In this case, you would want to ensure it points to the 'devnull' Destination that we just examined.",
            "follow_up_question_3": "What happens after I commit and deploy this new configuration?",
            "follow_up_answer_3": "After you commit and deploy this new configuration, your Workers will start using the updated settings. Any events they process will be sent to the specified 'devnull' Destination, where they will be discarded.",
            "url": "https://docs.cribl.io/stream/distributed-guide"
        },
        "reference": "with no real dependencies and no license requirement, follow the Simulated\u00c2 Splunk\u00c2 Destination instructions just below. For a simpler option, jump to the Internal\u00c2 Destination instructions further down. To send data to a real receiver that you\u2019ve already set up, see our Destinations topic for a link to the appropriate instructions. Here, you\u2019ll go through the steps of configuring a typical Splunk Destination, but you\u2019ll use netcat to spoof the receiver on port 9997. In the Cribl\u00c2 Stream Leader\u2019s UI, configure a Splunk\u00c2 Single Instance Destination, following these\u00c2 instructions. For this simulated output: Click Commit at the upper right, enter a commit message, and confirm the commit. Click Deploy at the upper right to deploy this new configuration to your Workers. In the first Worker instance\u2019s terminal/console, shell in, then enter cd\u00c2 /opt/cribl/bin to access Cribl\u00c2 Stream\u2019s CLI. Enter nc\u00c2 -h to check whether netcat is installed on this Linux instance. If the command fails, follow your Linux distro\u2019s steps for installing netcat. (E.g.,\u00c2 for Ubuntu instructions, see Docker\u00c2 Notes below.) If you\u2019re using a container like Docker, before shelling in at step 4 above, you\u2019ll need to first open a shell inside that container: docker exec -it <CONTAINER\u00c2 ID> /bin/bash In the above Docker Containers deployment example, you\u2019d want to open the shell on the docker_workers_1 container, whose <CONTAINER\u00c2 ID> was a3de9ea8f46f. Cribl\u2019s Docker containers currently run Ubuntu\u00c2 20.04.6. You can install netcat with this sequence of commands: apt updateapt install netcat The data you\u2019ll now see displayed in the terminal will be gibberish, because of Splunk\u2019s proprietary data format. If data isn\u2019t flowing, you might need to restart Workers. You can do this through Cribl\u00c2 Stream\u2019s UI. With Docker containers, use docker-compose down, followed by docker-compose up. You can streamline future Commit and Deploy steps by entering a Default\u00c2 Commit\u00c2 Message, and by collapsing\u00c2 actions to a combined Commit\u00c2 and\u00c2 Deploy button. Both options are available at Settings > Global\u00c2 Settings > System > Git\u00c2 Settings > General. As an alternative to the Splunk instructions above, you can configure Cribl\u00c2 Stream\u2019s built-in DevNull Destination to capture events and discard them. (This is the same Destination used in our single-instance Getting Started Guide tutorial.) In the Leader UI\u2019s top nav, click Manage\u00c2 > Groups. From the resulting Groups tab, click the Group name.(Working with a free license, we\u2019re implicitly configuring this Source on the default Group.) From the top nav, select Data\u00c2 > Destinations. Select DevNull from the Data\u00c2 Destinations page\u2019s tiles or left menu.(You can use the search box to jump to the DevNull tile.) On the resulting devnull row, look for the Live indicator under Enabled. This confirms that the DevNull Destination is ready to accept events. From the Data\u00c2 Destinations page\u2019s left nav, select the Default Destination at the top. On the resulting Manage Default Destination page, verify that the Default\u00c2 Output ID drop-down points to the devnull Destination we just examined. Click Commit at the upper right, enter a commit message, and confirm the commit. Click Deploy at the upper right to deploy this new configuration to your Workers. To complete this distributed deployment with realistic infrastructure, let\u2019s set up a Pipeline and Route. If you\u2019ve already done the Getting\u00c2 Started Guide, you\u2019ve already created a slicendice Pipeline. In the following steps, skip ahead to adding an Eval\u00c2 Function. From the current Group\u2019s submenu, select Processing\u00c2 > Pipelines. At the Pipelines pane\u2019s upper right, click Add\u00c2 Pipeline, then select Create\u00c2 Pipeline. In the new Pipeline\u2019s ID field, enter a unique identifier (e.g., slicendice). Optionally, enter a **Description ** of this Pipeline\u2019s purpose. Click Save. Now add an Eval Function to the Pipeline: At the Pipelines pane\u2019s upper right, click Add\u00c2 Function. Search for the Eval Function, and click its link to add it. In the new Function\u2019s Evaluate\u00c2 Fields section, click Add\u00c2 Field. In the new row\u2019s Name column, name the field origin. In the new row\u2019s Value\u00c2 Expression column, enter: host+\"\" \"\"+source This new origin field will concatenate the host and source fields from incoming events. Click Save to store the Function\u2019s configuration. Commit and Deploy the new Pipeline configuration. Optionally, open the right pane and click Capture\u00c2 Data to verify throughput. If you\u2019ve already done the Getting\u00c2 Started Guide, you\u2019ve already created a demo Route, attached to the slicendice Pipeline. In the following steps, just modify the Route to send data to the new Destination you configured above. At the Pipelines page\u2019s top left, click Attach to Route.This displays the Data\u00c2 Routes page. Click Add\u00c2 Route. Enter a unique Route Name, like demo. Leave the Filter field set to its true default, allowing it to deliver all events. Set the Pipeline drop-down to our configured slicendice Pipeline. Set the Output drop-down to the Destination you configured above. If you boldly chose the Simulated\u00c2 Splunk\u00c2 Destination, this will be named something like splunk:splunk9997. You can leave the Description empty, and leave Final set to Yes. Grab the new Route by its left handle, and drag it to the top of the Routing table, so that our new Route will process events first. You should see something like the screenshot below. Click Save to save the new Route configuration. Commit and Deploy your changes. Still assuming you configured a simulated Splunk output, look at the terminal where you\u2019re running netcat. You should now see events arriving. With all our infrastructure in place, let\u2019s look at how a Cribl\u00c2 Stream distributed deployment scales up to balance the incoming event load among multiple Workers. In the Leader UI\u2019s top nav, click Manage\u00c2 > Groups. From the Groups page, click the Group name.(Working with a free license, we\u2019re implicitly configuring this Source on the default Group.) From the top nav, select Manage > Data > Sources, and find the Datagen Source you configured earlier. Click this Datagen\u2019s row to reopen its config modal. Reset the Events per Second per Worker Node from the default 10\u00c2 EPS to a high number, like 200\u00c2 EPS. Click Save, then Commit and Deploy this higher event load. From the Leader\u2019s top nav, also select Global\u00c2 Config\u00c2 > Commit to commit the Leader\u2019s newest config version. In\u00c2 the resulting modal, click Commit again to confirm. This uses Cribl\u00c2 Stream\u2019s git integration to save a global configuration point for your whole deployment, which you can roll back to. This completes your setup of a basic distributed deployment, using a free license, and configuring a single Worker\u00c2 Group of two identically configured Worker Processes. (You\u00c2 can extrapolate these same techniques you\u2019ve just mastered to spin up virtually any number of Workers in the same way.) To see how you can set up multiple Worker\u00c2 Groups \u00e2\u0080\u0093 with separate configurations optimized for separate data flows \u00e2\u0080\u0093 continue to the next section, noting its licensing prerequisites. If you\u2019re deferring or skipping that option, jump ahead to Cleaning\u00c2 Up. To add and manage more than one Worker Group \u00e2\u0080\u0093 everything in this optional section \u00e2\u0080\u0093 you\u2019ll need an Enterprise or Sales\u00c2 Trial license for your on-prem Cribl\u00c2 Stream deployment, or an Enterprise (or equivalent) plan for your Cribl.Cloud Organization. For details on adding multiple Worker Groups on Cribl.Cloud, see Cribl.Cloud\u00c2 Worker\u00c2 Groups. See Licensing for how to acquire and install one of the above license types. Install the license on your Leader instance, and then commit this as a Leader config change (top nav\u2019s Global\u00c2 Config dialog), before you proceed. Here, we\u2019ll build on the infrastructure we\u2019ve created so far to: To keep this Quick\u00c2 Start tutorial focused on techniques, rather than on configuring lots of infrastructure, we\u2019ll assign just one Worker to each Worker\u00c2 Group \u00e2\u0080\u0093 one of the two Workers we launched above. But\u00c2 in production,\u00c2 you\u2019ll be able to apply the same principles to setting up any number of Worker\u00c2 Groups, with any number of Workers. With an Enterprise or Sales Trial license/plan, Cribl\u00c2 Stream\u2019s UI adds some extra features. The Manage\u00c2 > Groups page, shown here, now supports multiple Groups. (For\u00c2 now, you still see only the single default Group \u00e2\u0080\u0093 but we\u2019ll change that a few sections down.) On Cribl.Cloud, the Groups page adds extra columns. You\u2019ll see each Group\u2019s Worker type (hybrid versus Cloud/Cribl-managed) and, for Cloud Groups, the anticipated ingress rate and Provisioned status. For details, see Cribl.Cloud\u00c2 Worker\u00c2 Groups. Note that if you\u2019ve enabled Worker\u00c2 UI access, you can click directly through to each of your Workers. This\u00c2 feature will come in handy just below. To\u00c2 try it out: For the remaining steps, we want to make sure both our Workers (configured earlier) are up. Cribl\u00c2 Stream\u2019s top header should indicate 2\u00c2 WORKERS. You can verify that they\u2019re alive by clicking the top nav\u2019s Manage\u00c2 > Workers tabs, as shown earlier. If so, proceed to Map\u00c2 Groups by Config. If either or both Workers are down, restart them: Make sure you\u2019ve enabled Worker UI access. (It\u2019s time!) To click through to a dormant Worker\u2019s UI, use either of the left-nav options covered just above: either the Workers page, or the Manage\u00c2 > Groups\u00c2 > default\u00c2 submenu of individual Worker IDs. When you see that Worker\u2019s UI (purple header), click\u00c2 Restart at the upper right. Confirm your choice, and wait for a Server\u00c2 has been restarted message to appear for a few seconds. Click the top nav\u2019s Manage option to return to the Leader\u2019s UI. If the other Worker is down, repeat the above steps to restart it as well. With both Workers confirmed up, let\u2019s look at how Cribl\u00c2 Stream has automatically mapped all these existing workers to the default Group. From the Leader\u2019s top nav, click Manage\u00c2 > Mappings. You now see a default Mapping Ruleset, also literally named default. Click it. A Cribl\u00c2 Stream Leader can have multiple Mapping\u00c2 Rulesets configured, but only one can be active at a time. (The default Ruleset and Rule naming are separate from the default Group\u2019s naming. All of these out-of-the-box starting configurations have been named\u2026literally.) Below the Rule\u00c2 Name, a Mapping Rule has two functional fields: So this is a catch-all rule. By following it, the Leader has assigned all (both) registered Workers to the default Group. Let\u2019s make a more-specific rule, mapping a specific Worker (by hostname) to this Group, which receives events from our weblog Datagen Source. Click Add\u00c2 Rule at the upper right. Then configure the new Rule as shown below: Click Save to add this Rule. Confirm the warning that changes will take effect immediately. From the top nav, select Global\u00c2 Config > Commit to commit the Leader\u2019s new config. For more Mapping Rules/Rulesets details and examples, see Distributed\u00c2 Deployment. Next, we\u2019ll add a second Worker\u00c2 Group; add a second Source (relaying Cribl\u00c2 Stream\u2019s internal metrics); and then add another Mapping\u00c2 Rule, to map our second Worker to the new Group. From the Leader\u2019s top nav, click Manage\u00c2 > Groups. On the resulting Groups page, click New\u00c2 Group. Name the new Group CriblMetrics to match its purpose. Toggle UI\u00c2 access to Yes. Save the Group. Click Deploy on the new Group\u2019s row, and confirm your choice. The new Group should deploy immediately. On the new Group, we\u2019ll now enable a Cribl\u00c2 Internal: Metrics Source, representing a second data type. From the Leader\u2019s top nav, click Manage > Groups > CriblMetrics. From the resulting submenu, click Data\u00c2 > Sources. From the Data\u00c2 Sources page\u2019s tiles or left menu, select Cribl\u00c2 Internal. On the Manage\u00c2 Sources / Cribl\u00c2 Internal page, toggle Enable to Yes in the CriblMetrics row. Confirm that you want to enable CriblMetrics. From the top nav, select Global\u00c2 Config\u00c2 > Commit to commit the new config on the Leader and Group. Click Deploy at the upper right to deploy this new configuration to your Workers. Now let\u2019s map this new group to the CriblMetrics Source\u2019s incoming events: From the Leader\u2019s top nav, click Manage\u00c2 > Groups. From the Groups page, click the Mappings tab. Click the default Mapping Ruleset to open it. Click Add\u00c2 Rule at the upper right. Then configure the new Rule as shown"
    },
    {
        "question_answer": {
            "question": "What is the purpose of a Pipeline and Route in Cribl Stream?",
            "answer": "In Cribl Stream, a Pipeline is a sequence of processing steps that data passes through. A Route, on the other hand, is a rule that determines which Pipeline the data should be sent to based on certain conditions. Together, they allow you to control how data is processed and where it is sent.",
            "follow_up_question_1": "Can you give an example of how to set up a Pipeline and Route?",
            "follow_up_answer_1": "Sure, for example, you can create a 'slicendice' Pipeline and add an Eval Function to it. This function can create a new field 'origin' by concatenating the 'host' and 'source' fields from incoming events. Then, you can create a 'demo' Route and set its Filter field to 'true', which means it will deliver all events. You can set the Pipeline drop-down to our configured 'slicendice' Pipeline and set the Output drop-down to the Destination you configured.",
            "follow_up_question_2": "What does the 'Final' setting in the Route configuration do?",
            "follow_up_answer_2": "The 'Final' setting in the Route configuration determines whether or not further routing should be attempted after this Route. If 'Final' is set to 'Yes', then no further routing will be attempted after this Route.",
            "follow_up_question_3": "What happens after I commit and deploy the new Route configuration?",
            "follow_up_answer_3": "After you commit and deploy the new Route configuration, the changes will take effect and data will start being processed according to the new Route and Pipeline settings.",
            "url": "https://docs.cribl.io/stream/distributed-guide"
        },
        "reference": "with no real dependencies and no license requirement, follow the Simulated\u00c2 Splunk\u00c2 Destination instructions just below. For a simpler option, jump to the Internal\u00c2 Destination instructions further down. To send data to a real receiver that you\u2019ve already set up, see our Destinations topic for a link to the appropriate instructions. Here, you\u2019ll go through the steps of configuring a typical Splunk Destination, but you\u2019ll use netcat to spoof the receiver on port 9997. In the Cribl\u00c2 Stream Leader\u2019s UI, configure a Splunk\u00c2 Single Instance Destination, following these\u00c2 instructions. For this simulated output: Click Commit at the upper right, enter a commit message, and confirm the commit. Click Deploy at the upper right to deploy this new configuration to your Workers. In the first Worker instance\u2019s terminal/console, shell in, then enter cd\u00c2 /opt/cribl/bin to access Cribl\u00c2 Stream\u2019s CLI. Enter nc\u00c2 -h to check whether netcat is installed on this Linux instance. If the command fails, follow your Linux distro\u2019s steps for installing netcat. (E.g.,\u00c2 for Ubuntu instructions, see Docker\u00c2 Notes below.) If you\u2019re using a container like Docker, before shelling in at step 4 above, you\u2019ll need to first open a shell inside that container: docker exec -it <CONTAINER\u00c2 ID> /bin/bash In the above Docker Containers deployment example, you\u2019d want to open the shell on the docker_workers_1 container, whose <CONTAINER\u00c2 ID> was a3de9ea8f46f. Cribl\u2019s Docker containers currently run Ubuntu\u00c2 20.04.6. You can install netcat with this sequence of commands: apt updateapt install netcat The data you\u2019ll now see displayed in the terminal will be gibberish, because of Splunk\u2019s proprietary data format. If data isn\u2019t flowing, you might need to restart Workers. You can do this through Cribl\u00c2 Stream\u2019s UI. With Docker containers, use docker-compose down, followed by docker-compose up. You can streamline future Commit and Deploy steps by entering a Default\u00c2 Commit\u00c2 Message, and by collapsing\u00c2 actions to a combined Commit\u00c2 and\u00c2 Deploy button. Both options are available at Settings > Global\u00c2 Settings > System > Git\u00c2 Settings > General. As an alternative to the Splunk instructions above, you can configure Cribl\u00c2 Stream\u2019s built-in DevNull Destination to capture events and discard them. (This is the same Destination used in our single-instance Getting Started Guide tutorial.) In the Leader UI\u2019s top nav, click Manage\u00c2 > Groups. From the resulting Groups tab, click the Group name.(Working with a free license, we\u2019re implicitly configuring this Source on the default Group.) From the top nav, select Data\u00c2 > Destinations. Select DevNull from the Data\u00c2 Destinations page\u2019s tiles or left menu.(You can use the search box to jump to the DevNull tile.) On the resulting devnull row, look for the Live indicator under Enabled. This confirms that the DevNull Destination is ready to accept events. From the Data\u00c2 Destinations page\u2019s left nav, select the Default Destination at the top. On the resulting Manage Default Destination page, verify that the Default\u00c2 Output ID drop-down points to the devnull Destination we just examined. Click Commit at the upper right, enter a commit message, and confirm the commit. Click Deploy at the upper right to deploy this new configuration to your Workers. To complete this distributed deployment with realistic infrastructure, let\u2019s set up a Pipeline and Route. If you\u2019ve already done the Getting\u00c2 Started Guide, you\u2019ve already created a slicendice Pipeline. In the following steps, skip ahead to adding an Eval\u00c2 Function. From the current Group\u2019s submenu, select Processing\u00c2 > Pipelines. At the Pipelines pane\u2019s upper right, click Add\u00c2 Pipeline, then select Create\u00c2 Pipeline. In the new Pipeline\u2019s ID field, enter a unique identifier (e.g., slicendice). Optionally, enter a **Description ** of this Pipeline\u2019s purpose. Click Save. Now add an Eval Function to the Pipeline: At the Pipelines pane\u2019s upper right, click Add\u00c2 Function. Search for the Eval Function, and click its link to add it. In the new Function\u2019s Evaluate\u00c2 Fields section, click Add\u00c2 Field. In the new row\u2019s Name column, name the field origin. In the new row\u2019s Value\u00c2 Expression column, enter: host+\"\" \"\"+source This new origin field will concatenate the host and source fields from incoming events. Click Save to store the Function\u2019s configuration. Commit and Deploy the new Pipeline configuration. Optionally, open the right pane and click Capture\u00c2 Data to verify throughput. If you\u2019ve already done the Getting\u00c2 Started Guide, you\u2019ve already created a demo Route, attached to the slicendice Pipeline. In the following steps, just modify the Route to send data to the new Destination you configured above. At the Pipelines page\u2019s top left, click Attach to Route.This displays the Data\u00c2 Routes page. Click Add\u00c2 Route. Enter a unique Route Name, like demo. Leave the Filter field set to its true default, allowing it to deliver all events. Set the Pipeline drop-down to our configured slicendice Pipeline. Set the Output drop-down to the Destination you configured above. If you boldly chose the Simulated\u00c2 Splunk\u00c2 Destination, this will be named something like splunk:splunk9997. You can leave the Description empty, and leave Final set to Yes. Grab the new Route by its left handle, and drag it to the top of the Routing table, so that our new Route will process events first. You should see something like the screenshot below. Click Save to save the new Route configuration. Commit and Deploy your changes. Still assuming you configured a simulated Splunk output, look at the terminal where you\u2019re running netcat. You should now see events arriving. With all our infrastructure in place, let\u2019s look at how a Cribl\u00c2 Stream distributed deployment scales up to balance the incoming event load among multiple Workers. In the Leader UI\u2019s top nav, click Manage\u00c2 > Groups. From the Groups page, click the Group name.(Working with a free license, we\u2019re implicitly configuring this Source on the default Group.) From the top nav, select Manage > Data > Sources, and find the Datagen Source you configured earlier. Click this Datagen\u2019s row to reopen its config modal. Reset the Events per Second per Worker Node from the default 10\u00c2 EPS to a high number, like 200\u00c2 EPS. Click Save, then Commit and Deploy this higher event load. From the Leader\u2019s top nav, also select Global\u00c2 Config\u00c2 > Commit to commit the Leader\u2019s newest config version. In\u00c2 the resulting modal, click Commit again to confirm. This uses Cribl\u00c2 Stream\u2019s git integration to save a global configuration point for your whole deployment, which you can roll back to. This completes your setup of a basic distributed deployment, using a free license, and configuring a single Worker\u00c2 Group of two identically configured Worker Processes. (You\u00c2 can extrapolate these same techniques you\u2019ve just mastered to spin up virtually any number of Workers in the same way.) To see how you can set up multiple Worker\u00c2 Groups \u00e2\u0080\u0093 with separate configurations optimized for separate data flows \u00e2\u0080\u0093 continue to the next section, noting its licensing prerequisites. If you\u2019re deferring or skipping that option, jump ahead to Cleaning\u00c2 Up. To add and manage more than one Worker Group \u00e2\u0080\u0093 everything in this optional section \u00e2\u0080\u0093 you\u2019ll need an Enterprise or Sales\u00c2 Trial license for your on-prem Cribl\u00c2 Stream deployment, or an Enterprise (or equivalent) plan for your Cribl.Cloud Organization. For details on adding multiple Worker Groups on Cribl.Cloud, see Cribl.Cloud\u00c2 Worker\u00c2 Groups. See Licensing for how to acquire and install one of the above license types. Install the license on your Leader instance, and then commit this as a Leader config change (top nav\u2019s Global\u00c2 Config dialog), before you proceed. Here, we\u2019ll build on the infrastructure we\u2019ve created so far to: To keep this Quick\u00c2 Start tutorial focused on techniques, rather than on configuring lots of infrastructure, we\u2019ll assign just one Worker to each Worker\u00c2 Group \u00e2\u0080\u0093 one of the two Workers we launched above. But\u00c2 in production,\u00c2 you\u2019ll be able to apply the same principles to setting up any number of Worker\u00c2 Groups, with any number of Workers. With an Enterprise or Sales Trial license/plan, Cribl\u00c2 Stream\u2019s UI adds some extra features. The Manage\u00c2 > Groups page, shown here, now supports multiple Groups. (For\u00c2 now, you still see only the single default Group \u00e2\u0080\u0093 but we\u2019ll change that a few sections down.) On Cribl.Cloud, the Groups page adds extra columns. You\u2019ll see each Group\u2019s Worker type (hybrid versus Cloud/Cribl-managed) and, for Cloud Groups, the anticipated ingress rate and Provisioned status. For details, see Cribl.Cloud\u00c2 Worker\u00c2 Groups. Note that if you\u2019ve enabled Worker\u00c2 UI access, you can click directly through to each of your Workers. This\u00c2 feature will come in handy just below. To\u00c2 try it out: For the remaining steps, we want to make sure both our Workers (configured earlier) are up. Cribl\u00c2 Stream\u2019s top header should indicate 2\u00c2 WORKERS. You can verify that they\u2019re alive by clicking the top nav\u2019s Manage\u00c2 > Workers tabs, as shown earlier. If so, proceed to Map\u00c2 Groups by Config. If either or both Workers are down, restart them: Make sure you\u2019ve enabled Worker UI access. (It\u2019s time!) To click through to a dormant Worker\u2019s UI, use either of the left-nav options covered just above: either the Workers page, or the Manage\u00c2 > Groups\u00c2 > default\u00c2 submenu of individual Worker IDs. When you see that Worker\u2019s UI (purple header), click\u00c2 Restart at the upper right. Confirm your choice, and wait for a Server\u00c2 has been restarted message to appear for a few seconds. Click the top nav\u2019s Manage option to return to the Leader\u2019s UI. If the other Worker is down, repeat the above steps to restart it as well. With both Workers confirmed up, let\u2019s look at how Cribl\u00c2 Stream has automatically mapped all these existing workers to the default Group. From the Leader\u2019s top nav, click Manage\u00c2 > Mappings. You now see a default Mapping Ruleset, also literally named default. Click it. A Cribl\u00c2 Stream Leader can have multiple Mapping\u00c2 Rulesets configured, but only one can be active at a time. (The default Ruleset and Rule naming are separate from the default Group\u2019s naming. All of these out-of-the-box starting configurations have been named\u2026literally.) Below the Rule\u00c2 Name, a Mapping Rule has two functional fields: So this is a catch-all rule. By following it, the Leader has assigned all (both) registered Workers to the default Group. Let\u2019s make a more-specific rule, mapping a specific Worker (by hostname) to this Group, which receives events from our weblog Datagen Source. Click Add\u00c2 Rule at the upper right. Then configure the new Rule as shown below: Click Save to add this Rule. Confirm the warning that changes will take effect immediately. From the top nav, select Global\u00c2 Config > Commit to commit the Leader\u2019s new config. For more Mapping Rules/Rulesets details and examples, see Distributed\u00c2 Deployment. Next, we\u2019ll add a second Worker\u00c2 Group; add a second Source (relaying Cribl\u00c2 Stream\u2019s internal metrics); and then add another Mapping\u00c2 Rule, to map our second Worker to the new Group. From the Leader\u2019s top nav, click Manage\u00c2 > Groups. On the resulting Groups page, click New\u00c2 Group. Name the new Group CriblMetrics to match its purpose. Toggle UI\u00c2 access to Yes. Save the Group. Click Deploy on the new Group\u2019s row, and confirm your choice. The new Group should deploy immediately. On the new Group, we\u2019ll now enable a Cribl\u00c2 Internal: Metrics Source, representing a second data type. From the Leader\u2019s top nav, click Manage > Groups > CriblMetrics. From the resulting submenu, click Data\u00c2 > Sources. From the Data\u00c2 Sources page\u2019s tiles or left menu, select Cribl\u00c2 Internal. On the Manage\u00c2 Sources / Cribl\u00c2 Internal page, toggle Enable to Yes in the CriblMetrics row. Confirm that you want to enable CriblMetrics. From the top nav, select Global\u00c2 Config\u00c2 > Commit to commit the new config on the Leader and Group. Click Deploy at the upper right to deploy this new configuration to your Workers. Now let\u2019s map this new group to the CriblMetrics Source\u2019s incoming events: From the Leader\u2019s top nav, click Manage\u00c2 > Groups. From the Groups page, click the Mappings tab. Click the default Mapping Ruleset to open it. Click Add\u00c2 Rule at the upper right. Then configure the new Rule as shown"
    },
    {
        "question_answer": {
            "question": "What is the purpose of moving the Default Mappings rule to the bottom of the Ruleset?",
            "answer": "The Default Mappings rule is moved to the bottom of the Ruleset to reflect its catch-all function. This means it will apply only if none of the other rules in the Ruleset apply.",
            "follow_up_question_1": "What happens if this rule is not placed at the bottom?",
            "follow_up_answer_1": "If the Default Mappings rule is not placed at the bottom, it might interfere with the other rules. It could potentially apply to data that should be handled by a different rule, leading to incorrect data mapping.",
            "follow_up_question_2": "So, it's crucial to ensure the order of the rules is correct, right?",
            "follow_up_answer_2": "Yes, the order of the rules in the Ruleset is very important. The system processes the rules from top to bottom, so the placement of a rule can affect how data is mapped.",
            "follow_up_question_3": "What happens after I save the new configuration?",
            "follow_up_answer_3": "After you save the new configuration, the changes will take effect immediately. This means that any new data will be processed according to the updated Ruleset.",
            "url": "https://docs.cribl.io/stream/distributed-guide"
        },
        "reference": "below: Move the Default\u00c2 Mappings rule to the bottom of the Ruleset, reflecting its catch-all function. Click Save to store the new configuration. Confirm the warning that changes will take effect immediately. We now have two data Sources and two Worker\u00c2 Groups \u00e2\u0080\u0093 one each for (Web) logs versus (Cribl Internal) metrics \u00e2\u0080\u0093 along with two Mapping Rules to map data accordingly. To\u00c2 confirm the Workers\u2019 assignment to the two Groups, click the top nav\u2019s Manage tab, then select Groups from the submenu: To confirm further details about the Workers, click the Workers tab, and on the resulting Workers page, click anywhere on the Worker Node\u2019s row to reveal more details: With incoming metrics now mapped to our second Worker\u00c2 Group, we next need to configure this Group\u2019s output. Here, we\u2019ll rely on a metrics-oriented Pipeline and a Destination that ship with Cribl\u00c2 Stream, and create a new Route to connect everything up. From the Leader\u2019s top nav, click Manage\u00c2 > Groups > CriblMetrics. From the resulting submenu, select Processing\u00c2 > Pipelines. On the Pipelines page, find the cribl_metrics_rollup Pipeline, and click it to expand it. Expand this Pipeline\u2019s Functions (including Comments) to see its configuration. It\u2019s preconfigured with a Rollup\u00c2 Metrics Function to aggregate metrics to a 30-second time window. Next is an Eval Function that filters for Cribl (Cribl\u00c2 Stream) internal metrics and tags them on outgoing events with a new field. We\u2019ll connect this existing Pipeline to a new Route: At the Pipelines page\u2019s top left, click Attach to Route.This displays the Data Routes page. Click Add\u00c2 Route. Enter a unique Route Name, like metrics. Leave the Filter field set to its true default, allowing it to deliver all events. Make sure the Pipeline drop-down is set to cribl_metrics_rollup. As the Output (Destination), select our old friend devnull:devnull. This is Cribl\u00c2 Stream\u2019s preconfigured Destination that simulates a downstream service while simply dropping events. You can leave the Description empty, and leave Final set to Yes. Grab the new Route by its handle, and drag it to the top of the Routing table, so that our new Route will process events first. You should see something like the screenshot below. Click Save to save the new Route configuration. Commit and Deploy your changes. From the sparkline on the Route you just configured (see the screenshot above), you can already see that metrics data is flowing all the way \u201cout\u201d of Cribl\u00c2 Stream \u00e2\u0080\u0093 simulated here by the DevNull Destination. To\u00c2 verify the whole configuration you\u2019ve created, click the top nav\u2019s Monitoring tab. On the Monitoring page, toggle the All\u00c2 Groups drop-down (upper right), toggle between the two Worker\u00c2 Groups to see the division of labor: Before a final section where you can tear down your infrastructure, here\u2019s a recap of the simple (but expandable) distributed model we\u2019ve created, with some ideas for expanding it: A distributed deployment enables Cribl\u00c2 Stream to scale out to higher data volumes. This load-balancing occurs even with a single Worker\u00c2 Group, in which all Workers share the same configuration. By adding multiple Worker Groups, you can partition Worker\u00c2 Nodes (and their data flows) by different configurations. In this demonstration, we simply mapped Workers to Groups by the Workers\u2019 hostname. But\u00c2 you can map by a range of arbitrary criteria to meet your production needs. E.g.: Different Groups can be managed by different teams. You can filter on DNS rules to send relevant data to the relevant team\u2019s Group. Different Groups can also maintain configurations for different regions\u2019 data privacy or data retention requirements. You can also Map workers arbitrarily using Tags and other options. Cribl\u00c2 Stream refers to \u201cWorkers\u201d at several levels. Now that you\u2019ve been initiated into building distributed deployments, here\u2019s a guide to the fine points of distinguishing these levels: A Worker\u00c2 Group holds one or multiple Worker Nodes. Each Worker\u00c2 Node functions like an independent Cribl\u00c2 Stream single-instance deployment. Worker\u00c2 Nodes don\u2019t coordinate directly with each other \u00e2\u0080\u0093 they\u2019re coordinated only through communication to/from the Leader\u00c2 Node. Each Worker\u00c2 Node contains a configured number of Worker Processes. Unlike the above grouping abstractions, this is the level that actually processes data. To load-balance among Worker\u00c2 Processes, Cribl\u00c2 Stream\u2019s API\u00c2 Process round-robins incoming connections to them. When deploying on AWS/EKS, Worker Groups should not span Availability\u00c2 Zones. If you have EBS persistent volumes, and a node fails, its replacement won\u2019t be able to access the peer volume across AZs. If and when you choose to shut down the infrastructure you\u2019ve configured for this demonstration: Navigate to Cribl\u00c2 Stream\u2019s default Group\u00c2 > Data\u00c2 > Sources\u00c2 > Datagen, and switch off the toggle beside weblog.log. If you configured a second Worker Group: Navigate to Cribl\u00c2 Stream\u2019s CriblMetrics Group\u00c2 > Data\u00c2 > Sources\u00c2 > Cribl\u00c2 Internal, and disable the toggle beside CriblMetrics. If you have netcat running on a Worker\u2019s terminal/console, ^C it. There\u2019s no need (or way) to switch off the DevNull Destination \u00e2\u0080\u0093 it just is. If desired, Commit and Deploy these changes. If you\u2019re running the Cribl\u00c2 Stream server(s) on cloud instances that will (ultimately) incur charges, you\u2019re now free to shut down those cloud resources. Interested in guided walk-throughs of more-advanced Cribl\u00c2 Stream features? We suggest that you next check out these further resources. Distributed Deployment: All the details behind the deployment approach you just mastered in this tutorial. Cribl\u00c2 Stream Sandboxes: Work through general and specific scenarios in a free, hosted environment, with terminal access and real data inputs and outputs. Use Cases documentation: Bring your own services to build solutions to specific challenges. Cribl Concept: Pipelines \u00e2\u0080\u0093 Video showing how to build and use Pipelines at multiple Cribl\u00c2 Stream stages. Cribl Concept: Routing \u00e2\u0080\u0093 Video about using Routes to send different data through different paths.\""
    },
    {
        "question_answer": {
            "question": "What is the function of the Worker Groups in Cribl Stream?",
            "answer": "Worker Groups in Cribl Stream are used to partition Worker Nodes and their data flows by different configurations. This allows for efficient load-balancing and scalability, even with higher data volumes.",
            "follow_up_question_1": "Can different Worker Groups be managed by different teams?",
            "follow_up_answer_1": "Yes, different Worker Groups can indeed be managed by different teams. This allows for a division of labor and can help ensure that each team is working with the data most relevant to them.",
            "follow_up_question_2": "Can different Groups maintain configurations for different regions\u2019 data privacy or data retention requirements?",
            "follow_up_answer_2": "Absolutely! Different Groups can be configured to meet different regions' data privacy or data retention requirements. This flexibility is one of the strengths of Cribl Stream.",
            "follow_up_question_3": "How can I map workers arbitrarily using Tags and other options?",
            "follow_up_answer_3": "Cribl Stream allows you to map workers arbitrarily using Tags and other options. This means you can assign workers to groups based on any criteria that suits your needs, not just based on their hostname.",
            "url": "https://docs.cribl.io/stream/distributed-guide"
        },
        "reference": "below: Move the Default\u00c2 Mappings rule to the bottom of the Ruleset, reflecting its catch-all function. Click Save to store the new configuration. Confirm the warning that changes will take effect immediately. We now have two data Sources and two Worker\u00c2 Groups \u00e2\u0080\u0093 one each for (Web) logs versus (Cribl Internal) metrics \u00e2\u0080\u0093 along with two Mapping Rules to map data accordingly. To\u00c2 confirm the Workers\u2019 assignment to the two Groups, click the top nav\u2019s Manage tab, then select Groups from the submenu: To confirm further details about the Workers, click the Workers tab, and on the resulting Workers page, click anywhere on the Worker Node\u2019s row to reveal more details: With incoming metrics now mapped to our second Worker\u00c2 Group, we next need to configure this Group\u2019s output. Here, we\u2019ll rely on a metrics-oriented Pipeline and a Destination that ship with Cribl\u00c2 Stream, and create a new Route to connect everything up. From the Leader\u2019s top nav, click Manage\u00c2 > Groups > CriblMetrics. From the resulting submenu, select Processing\u00c2 > Pipelines. On the Pipelines page, find the cribl_metrics_rollup Pipeline, and click it to expand it. Expand this Pipeline\u2019s Functions (including Comments) to see its configuration. It\u2019s preconfigured with a Rollup\u00c2 Metrics Function to aggregate metrics to a 30-second time window. Next is an Eval Function that filters for Cribl (Cribl\u00c2 Stream) internal metrics and tags them on outgoing events with a new field. We\u2019ll connect this existing Pipeline to a new Route: At the Pipelines page\u2019s top left, click Attach to Route.This displays the Data Routes page. Click Add\u00c2 Route. Enter a unique Route Name, like metrics. Leave the Filter field set to its true default, allowing it to deliver all events. Make sure the Pipeline drop-down is set to cribl_metrics_rollup. As the Output (Destination), select our old friend devnull:devnull. This is Cribl\u00c2 Stream\u2019s preconfigured Destination that simulates a downstream service while simply dropping events. You can leave the Description empty, and leave Final set to Yes. Grab the new Route by its handle, and drag it to the top of the Routing table, so that our new Route will process events first. You should see something like the screenshot below. Click Save to save the new Route configuration. Commit and Deploy your changes. From the sparkline on the Route you just configured (see the screenshot above), you can already see that metrics data is flowing all the way \u201cout\u201d of Cribl\u00c2 Stream \u00e2\u0080\u0093 simulated here by the DevNull Destination. To\u00c2 verify the whole configuration you\u2019ve created, click the top nav\u2019s Monitoring tab. On the Monitoring page, toggle the All\u00c2 Groups drop-down (upper right), toggle between the two Worker\u00c2 Groups to see the division of labor: Before a final section where you can tear down your infrastructure, here\u2019s a recap of the simple (but expandable) distributed model we\u2019ve created, with some ideas for expanding it: A distributed deployment enables Cribl\u00c2 Stream to scale out to higher data volumes. This load-balancing occurs even with a single Worker\u00c2 Group, in which all Workers share the same configuration. By adding multiple Worker Groups, you can partition Worker\u00c2 Nodes (and their data flows) by different configurations. In this demonstration, we simply mapped Workers to Groups by the Workers\u2019 hostname. But\u00c2 you can map by a range of arbitrary criteria to meet your production needs. E.g.: Different Groups can be managed by different teams. You can filter on DNS rules to send relevant data to the relevant team\u2019s Group. Different Groups can also maintain configurations for different regions\u2019 data privacy or data retention requirements. You can also Map workers arbitrarily using Tags and other options. Cribl\u00c2 Stream refers to \u201cWorkers\u201d at several levels. Now that you\u2019ve been initiated into building distributed deployments, here\u2019s a guide to the fine points of distinguishing these levels: A Worker\u00c2 Group holds one or multiple Worker Nodes. Each Worker\u00c2 Node functions like an independent Cribl\u00c2 Stream single-instance deployment. Worker\u00c2 Nodes don\u2019t coordinate directly with each other \u00e2\u0080\u0093 they\u2019re coordinated only through communication to/from the Leader\u00c2 Node. Each Worker\u00c2 Node contains a configured number of Worker Processes. Unlike the above grouping abstractions, this is the level that actually processes data. To load-balance among Worker\u00c2 Processes, Cribl\u00c2 Stream\u2019s API\u00c2 Process round-robins incoming connections to them. When deploying on AWS/EKS, Worker Groups should not span Availability\u00c2 Zones. If you have EBS persistent volumes, and a node fails, its replacement won\u2019t be able to access the peer volume across AZs. If and when you choose to shut down the infrastructure you\u2019ve configured for this demonstration: Navigate to Cribl\u00c2 Stream\u2019s default Group\u00c2 > Data\u00c2 > Sources\u00c2 > Datagen, and switch off the toggle beside weblog.log. If you configured a second Worker Group: Navigate to Cribl\u00c2 Stream\u2019s CriblMetrics Group\u00c2 > Data\u00c2 > Sources\u00c2 > Cribl\u00c2 Internal, and disable the toggle beside CriblMetrics. If you have netcat running on a Worker\u2019s terminal/console, ^C it. There\u2019s no need (or way) to switch off the DevNull Destination \u00e2\u0080\u0093 it just is. If desired, Commit and Deploy these changes. If you\u2019re running the Cribl\u00c2 Stream server(s) on cloud instances that will (ultimately) incur charges, you\u2019re now free to shut down those cloud resources. Interested in guided walk-throughs of more-advanced Cribl\u00c2 Stream features? We suggest that you next check out these further resources. Distributed Deployment: All the details behind the deployment approach you just mastered in this tutorial. Cribl\u00c2 Stream Sandboxes: Work through general and specific scenarios in a free, hosted environment, with terminal access and real data inputs and outputs. Use Cases documentation: Bring your own services to build solutions to specific challenges. Cribl Concept: Pipelines \u00e2\u0080\u0093 Video showing how to build and use Pipelines at multiple Cribl\u00c2 Stream stages. Cribl Concept: Routing \u00e2\u0080\u0093 Video about using Routes to send different data through different paths.\""
    },
    {
        "question_answer": {
            "question": "What is the purpose of the DevNull Destination in Cribl Stream?",
            "answer": "The DevNull Destination in Cribl Stream is a preconfigured Destination that simulates a downstream service while simply dropping events. It's useful for testing and development purposes, as it allows you to see how data would flow through the system without actually sending it to a real destination.",
            "follow_up_question_1": "So, it doesn't actually store or process any data?",
            "follow_up_answer_1": "That's correct. The DevNull Destination doesn't store or process any data. It simply receives the data and then discards it.",
            "follow_up_question_2": "Can I switch off the DevNull Destination?",
            "follow_up_answer_2": "No, there's no need or way to switch off the DevNull Destination. It's always available as a destination option in Cribl Stream.",
            "follow_up_question_3": "Can I use the DevNull Destination in a production environment?",
            "follow_up_answer_3": "While you technically could use the DevNull Destination in a production environment, it's not recommended. Since it doesn't store or process any data, it wouldn't provide any value in a real-world use case.",
            "url": "https://docs.cribl.io/stream/distributed-guide"
        },
        "reference": "below: Move the Default\u00c2 Mappings rule to the bottom of the Ruleset, reflecting its catch-all function. Click Save to store the new configuration. Confirm the warning that changes will take effect immediately. We now have two data Sources and two Worker\u00c2 Groups \u00e2\u0080\u0093 one each for (Web) logs versus (Cribl Internal) metrics \u00e2\u0080\u0093 along with two Mapping Rules to map data accordingly. To\u00c2 confirm the Workers\u2019 assignment to the two Groups, click the top nav\u2019s Manage tab, then select Groups from the submenu: To confirm further details about the Workers, click the Workers tab, and on the resulting Workers page, click anywhere on the Worker Node\u2019s row to reveal more details: With incoming metrics now mapped to our second Worker\u00c2 Group, we next need to configure this Group\u2019s output. Here, we\u2019ll rely on a metrics-oriented Pipeline and a Destination that ship with Cribl\u00c2 Stream, and create a new Route to connect everything up. From the Leader\u2019s top nav, click Manage\u00c2 > Groups > CriblMetrics. From the resulting submenu, select Processing\u00c2 > Pipelines. On the Pipelines page, find the cribl_metrics_rollup Pipeline, and click it to expand it. Expand this Pipeline\u2019s Functions (including Comments) to see its configuration. It\u2019s preconfigured with a Rollup\u00c2 Metrics Function to aggregate metrics to a 30-second time window. Next is an Eval Function that filters for Cribl (Cribl\u00c2 Stream) internal metrics and tags them on outgoing events with a new field. We\u2019ll connect this existing Pipeline to a new Route: At the Pipelines page\u2019s top left, click Attach to Route.This displays the Data Routes page. Click Add\u00c2 Route. Enter a unique Route Name, like metrics. Leave the Filter field set to its true default, allowing it to deliver all events. Make sure the Pipeline drop-down is set to cribl_metrics_rollup. As the Output (Destination), select our old friend devnull:devnull. This is Cribl\u00c2 Stream\u2019s preconfigured Destination that simulates a downstream service while simply dropping events. You can leave the Description empty, and leave Final set to Yes. Grab the new Route by its handle, and drag it to the top of the Routing table, so that our new Route will process events first. You should see something like the screenshot below. Click Save to save the new Route configuration. Commit and Deploy your changes. From the sparkline on the Route you just configured (see the screenshot above), you can already see that metrics data is flowing all the way \u201cout\u201d of Cribl\u00c2 Stream \u00e2\u0080\u0093 simulated here by the DevNull Destination. To\u00c2 verify the whole configuration you\u2019ve created, click the top nav\u2019s Monitoring tab. On the Monitoring page, toggle the All\u00c2 Groups drop-down (upper right), toggle between the two Worker\u00c2 Groups to see the division of labor: Before a final section where you can tear down your infrastructure, here\u2019s a recap of the simple (but expandable) distributed model we\u2019ve created, with some ideas for expanding it: A distributed deployment enables Cribl\u00c2 Stream to scale out to higher data volumes. This load-balancing occurs even with a single Worker\u00c2 Group, in which all Workers share the same configuration. By adding multiple Worker Groups, you can partition Worker\u00c2 Nodes (and their data flows) by different configurations. In this demonstration, we simply mapped Workers to Groups by the Workers\u2019 hostname. But\u00c2 you can map by a range of arbitrary criteria to meet your production needs. E.g.: Different Groups can be managed by different teams. You can filter on DNS rules to send relevant data to the relevant team\u2019s Group. Different Groups can also maintain configurations for different regions\u2019 data privacy or data retention requirements. You can also Map workers arbitrarily using Tags and other options. Cribl\u00c2 Stream refers to \u201cWorkers\u201d at several levels. Now that you\u2019ve been initiated into building distributed deployments, here\u2019s a guide to the fine points of distinguishing these levels: A Worker\u00c2 Group holds one or multiple Worker Nodes. Each Worker\u00c2 Node functions like an independent Cribl\u00c2 Stream single-instance deployment. Worker\u00c2 Nodes don\u2019t coordinate directly with each other \u00e2\u0080\u0093 they\u2019re coordinated only through communication to/from the Leader\u00c2 Node. Each Worker\u00c2 Node contains a configured number of Worker Processes. Unlike the above grouping abstractions, this is the level that actually processes data. To load-balance among Worker\u00c2 Processes, Cribl\u00c2 Stream\u2019s API\u00c2 Process round-robins incoming connections to them. When deploying on AWS/EKS, Worker Groups should not span Availability\u00c2 Zones. If you have EBS persistent volumes, and a node fails, its replacement won\u2019t be able to access the peer volume across AZs. If and when you choose to shut down the infrastructure you\u2019ve configured for this demonstration: Navigate to Cribl\u00c2 Stream\u2019s default Group\u00c2 > Data\u00c2 > Sources\u00c2 > Datagen, and switch off the toggle beside weblog.log. If you configured a second Worker Group: Navigate to Cribl\u00c2 Stream\u2019s CriblMetrics Group\u00c2 > Data\u00c2 > Sources\u00c2 > Cribl\u00c2 Internal, and disable the toggle beside CriblMetrics. If you have netcat running on a Worker\u2019s terminal/console, ^C it. There\u2019s no need (or way) to switch off the DevNull Destination \u00e2\u0080\u0093 it just is. If desired, Commit and Deploy these changes. If you\u2019re running the Cribl\u00c2 Stream server(s) on cloud instances that will (ultimately) incur charges, you\u2019re now free to shut down those cloud resources. Interested in guided walk-throughs of more-advanced Cribl\u00c2 Stream features? We suggest that you next check out these further resources. Distributed Deployment: All the details behind the deployment approach you just mastered in this tutorial. Cribl\u00c2 Stream Sandboxes: Work through general and specific scenarios in a free, hosted environment, with terminal access and real data inputs and outputs. Use Cases documentation: Bring your own services to build solutions to specific challenges. Cribl Concept: Pipelines \u00e2\u0080\u0093 Video showing how to build and use Pipelines at multiple Cribl\u00c2 Stream stages. Cribl Concept: Routing \u00e2\u0080\u0093 Video about using Routes to send different data through different paths.\""
    },
    {
        "question_answer": {
            "question": "What is the purpose of the CriblMetrics Group in Cribl Stream?",
            "answer": "The CriblMetrics Group in Cribl Stream is used to handle Cribl internal metrics. It's configured with a metrics-oriented Pipeline and a Destination that ship with Cribl Stream.",
            "follow_up_question_1": "What is the function of the cribl_metrics_rollup Pipeline?",
            "follow_up_answer_1": "The cribl_metrics_rollup Pipeline is preconfigured with a Rollup Metrics Function to aggregate metrics to a 30-second time window. It also includes an Eval Function that filters for Cribl internal metrics and tags them on outgoing events with a new field.",
            "follow_up_question_2": "How can I connect this existing Pipeline to a new Route?",
            "follow_up_answer_2": "To connect the existing Pipeline to a new Route, you can click 'Attach to Route' at the top left of the Pipelines page. This will display the Data Routes page, where you can add a new Route and configure it to use the cribl_metrics_rollup Pipeline.",
            "follow_up_question_3": "What happens after I save the new Route configuration?",
            "follow_up_answer_3": "After you save the new Route configuration, you need to commit and deploy your changes. Once this is done, the new Route will start processing events according to the configuration you've set up.",
            "url": "https://docs.cribl.io/stream/distributed-guide"
        },
        "reference": "below: Move the Default\u00c2 Mappings rule to the bottom of the Ruleset, reflecting its catch-all function. Click Save to store the new configuration. Confirm the warning that changes will take effect immediately. We now have two data Sources and two Worker\u00c2 Groups \u00e2\u0080\u0093 one each for (Web) logs versus (Cribl Internal) metrics \u00e2\u0080\u0093 along with two Mapping Rules to map data accordingly. To\u00c2 confirm the Workers\u2019 assignment to the two Groups, click the top nav\u2019s Manage tab, then select Groups from the submenu: To confirm further details about the Workers, click the Workers tab, and on the resulting Workers page, click anywhere on the Worker Node\u2019s row to reveal more details: With incoming metrics now mapped to our second Worker\u00c2 Group, we next need to configure this Group\u2019s output. Here, we\u2019ll rely on a metrics-oriented Pipeline and a Destination that ship with Cribl\u00c2 Stream, and create a new Route to connect everything up. From the Leader\u2019s top nav, click Manage\u00c2 > Groups > CriblMetrics. From the resulting submenu, select Processing\u00c2 > Pipelines. On the Pipelines page, find the cribl_metrics_rollup Pipeline, and click it to expand it. Expand this Pipeline\u2019s Functions (including Comments) to see its configuration. It\u2019s preconfigured with a Rollup\u00c2 Metrics Function to aggregate metrics to a 30-second time window. Next is an Eval Function that filters for Cribl (Cribl\u00c2 Stream) internal metrics and tags them on outgoing events with a new field. We\u2019ll connect this existing Pipeline to a new Route: At the Pipelines page\u2019s top left, click Attach to Route.This displays the Data Routes page. Click Add\u00c2 Route. Enter a unique Route Name, like metrics. Leave the Filter field set to its true default, allowing it to deliver all events. Make sure the Pipeline drop-down is set to cribl_metrics_rollup. As the Output (Destination), select our old friend devnull:devnull. This is Cribl\u00c2 Stream\u2019s preconfigured Destination that simulates a downstream service while simply dropping events. You can leave the Description empty, and leave Final set to Yes. Grab the new Route by its handle, and drag it to the top of the Routing table, so that our new Route will process events first. You should see something like the screenshot below. Click Save to save the new Route configuration. Commit and Deploy your changes. From the sparkline on the Route you just configured (see the screenshot above), you can already see that metrics data is flowing all the way \u201cout\u201d of Cribl\u00c2 Stream \u00e2\u0080\u0093 simulated here by the DevNull Destination. To\u00c2 verify the whole configuration you\u2019ve created, click the top nav\u2019s Monitoring tab. On the Monitoring page, toggle the All\u00c2 Groups drop-down (upper right), toggle between the two Worker\u00c2 Groups to see the division of labor: Before a final section where you can tear down your infrastructure, here\u2019s a recap of the simple (but expandable) distributed model we\u2019ve created, with some ideas for expanding it: A distributed deployment enables Cribl\u00c2 Stream to scale out to higher data volumes. This load-balancing occurs even with a single Worker\u00c2 Group, in which all Workers share the same configuration. By adding multiple Worker Groups, you can partition Worker\u00c2 Nodes (and their data flows) by different configurations. In this demonstration, we simply mapped Workers to Groups by the Workers\u2019 hostname. But\u00c2 you can map by a range of arbitrary criteria to meet your production needs. E.g.: Different Groups can be managed by different teams. You can filter on DNS rules to send relevant data to the relevant team\u2019s Group. Different Groups can also maintain configurations for different regions\u2019 data privacy or data retention requirements. You can also Map workers arbitrarily using Tags and other options. Cribl\u00c2 Stream refers to \u201cWorkers\u201d at several levels. Now that you\u2019ve been initiated into building distributed deployments, here\u2019s a guide to the fine points of distinguishing these levels: A Worker\u00c2 Group holds one or multiple Worker Nodes. Each Worker\u00c2 Node functions like an independent Cribl\u00c2 Stream single-instance deployment. Worker\u00c2 Nodes don\u2019t coordinate directly with each other \u00e2\u0080\u0093 they\u2019re coordinated only through communication to/from the Leader\u00c2 Node. Each Worker\u00c2 Node contains a configured number of Worker Processes. Unlike the above grouping abstractions, this is the level that actually processes data. To load-balance among Worker\u00c2 Processes, Cribl\u00c2 Stream\u2019s API\u00c2 Process round-robins incoming connections to them. When deploying on AWS/EKS, Worker Groups should not span Availability\u00c2 Zones. If you have EBS persistent volumes, and a node fails, its replacement won\u2019t be able to access the peer volume across AZs. If and when you choose to shut down the infrastructure you\u2019ve configured for this demonstration: Navigate to Cribl\u00c2 Stream\u2019s default Group\u00c2 > Data\u00c2 > Sources\u00c2 > Datagen, and switch off the toggle beside weblog.log. If you configured a second Worker Group: Navigate to Cribl\u00c2 Stream\u2019s CriblMetrics Group\u00c2 > Data\u00c2 > Sources\u00c2 > Cribl\u00c2 Internal, and disable the toggle beside CriblMetrics. If you have netcat running on a Worker\u2019s terminal/console, ^C it. There\u2019s no need (or way) to switch off the DevNull Destination \u00e2\u0080\u0093 it just is. If desired, Commit and Deploy these changes. If you\u2019re running the Cribl\u00c2 Stream server(s) on cloud instances that will (ultimately) incur charges, you\u2019re now free to shut down those cloud resources. Interested in guided walk-throughs of more-advanced Cribl\u00c2 Stream features? We suggest that you next check out these further resources. Distributed Deployment: All the details behind the deployment approach you just mastered in this tutorial. Cribl\u00c2 Stream Sandboxes: Work through general and specific scenarios in a free, hosted environment, with terminal access and real data inputs and outputs. Use Cases documentation: Bring your own services to build solutions to specific challenges. Cribl Concept: Pipelines \u00e2\u0080\u0093 Video showing how to build and use Pipelines at multiple Cribl\u00c2 Stream stages. Cribl Concept: Routing \u00e2\u0080\u0093 Video about using Routes to send different data through different paths.\""
    },
    {
        "question_answer": {
            "question": "What is the purpose of the Worker Processes in Cribl Stream?",
            "answer": "Worker Processes in Cribl Stream are the level that actually processes data. They are contained within each Worker Node and are used to load-balance incoming connections.",
            "follow_up_question_1": "How does Cribl Stream load-balance among Worker Processes?",
            "follow_up_answer_1": "Cribl Stream's API Process round-robins incoming connections to load-balance among Worker Processes. This means it distributes the connections evenly among the available Worker Processes.",
            "follow_up_question_2": "Do Worker Processes coordinate directly with each other?",
            "follow_up_answer_2": "No, Worker Processes don't coordinate directly with each other. They are coordinated only through communication to/from the Leader Node.",
            "follow_up_question_3": "What happens if a Worker Node fails?",
            "follow_up_answer_3": "If a Worker Node fails and you're deploying on AWS/EKS, its replacement won't be able to access the peer volume across Availability Zones. Therefore, it's recommended that Worker Groups should not span Availability Zones.",
            "url": "https://docs.cribl.io/stream/distributed-guide"
        },
        "reference": "below: Move the Default\u00c2 Mappings rule to the bottom of the Ruleset, reflecting its catch-all function. Click Save to store the new configuration. Confirm the warning that changes will take effect immediately. We now have two data Sources and two Worker\u00c2 Groups \u00e2\u0080\u0093 one each for (Web) logs versus (Cribl Internal) metrics \u00e2\u0080\u0093 along with two Mapping Rules to map data accordingly. To\u00c2 confirm the Workers\u2019 assignment to the two Groups, click the top nav\u2019s Manage tab, then select Groups from the submenu: To confirm further details about the Workers, click the Workers tab, and on the resulting Workers page, click anywhere on the Worker Node\u2019s row to reveal more details: With incoming metrics now mapped to our second Worker\u00c2 Group, we next need to configure this Group\u2019s output. Here, we\u2019ll rely on a metrics-oriented Pipeline and a Destination that ship with Cribl\u00c2 Stream, and create a new Route to connect everything up. From the Leader\u2019s top nav, click Manage\u00c2 > Groups > CriblMetrics. From the resulting submenu, select Processing\u00c2 > Pipelines. On the Pipelines page, find the cribl_metrics_rollup Pipeline, and click it to expand it. Expand this Pipeline\u2019s Functions (including Comments) to see its configuration. It\u2019s preconfigured with a Rollup\u00c2 Metrics Function to aggregate metrics to a 30-second time window. Next is an Eval Function that filters for Cribl (Cribl\u00c2 Stream) internal metrics and tags them on outgoing events with a new field. We\u2019ll connect this existing Pipeline to a new Route: At the Pipelines page\u2019s top left, click Attach to Route.This displays the Data Routes page. Click Add\u00c2 Route. Enter a unique Route Name, like metrics. Leave the Filter field set to its true default, allowing it to deliver all events. Make sure the Pipeline drop-down is set to cribl_metrics_rollup. As the Output (Destination), select our old friend devnull:devnull. This is Cribl\u00c2 Stream\u2019s preconfigured Destination that simulates a downstream service while simply dropping events. You can leave the Description empty, and leave Final set to Yes. Grab the new Route by its handle, and drag it to the top of the Routing table, so that our new Route will process events first. You should see something like the screenshot below. Click Save to save the new Route configuration. Commit and Deploy your changes. From the sparkline on the Route you just configured (see the screenshot above), you can already see that metrics data is flowing all the way \u201cout\u201d of Cribl\u00c2 Stream \u00e2\u0080\u0093 simulated here by the DevNull Destination. To\u00c2 verify the whole configuration you\u2019ve created, click the top nav\u2019s Monitoring tab. On the Monitoring page, toggle the All\u00c2 Groups drop-down (upper right), toggle between the two Worker\u00c2 Groups to see the division of labor: Before a final section where you can tear down your infrastructure, here\u2019s a recap of the simple (but expandable) distributed model we\u2019ve created, with some ideas for expanding it: A distributed deployment enables Cribl\u00c2 Stream to scale out to higher data volumes. This load-balancing occurs even with a single Worker\u00c2 Group, in which all Workers share the same configuration. By adding multiple Worker Groups, you can partition Worker\u00c2 Nodes (and their data flows) by different configurations. In this demonstration, we simply mapped Workers to Groups by the Workers\u2019 hostname. But\u00c2 you can map by a range of arbitrary criteria to meet your production needs. E.g.: Different Groups can be managed by different teams. You can filter on DNS rules to send relevant data to the relevant team\u2019s Group. Different Groups can also maintain configurations for different regions\u2019 data privacy or data retention requirements. You can also Map workers arbitrarily using Tags and other options. Cribl\u00c2 Stream refers to \u201cWorkers\u201d at several levels. Now that you\u2019ve been initiated into building distributed deployments, here\u2019s a guide to the fine points of distinguishing these levels: A Worker\u00c2 Group holds one or multiple Worker Nodes. Each Worker\u00c2 Node functions like an independent Cribl\u00c2 Stream single-instance deployment. Worker\u00c2 Nodes don\u2019t coordinate directly with each other \u00e2\u0080\u0093 they\u2019re coordinated only through communication to/from the Leader\u00c2 Node. Each Worker\u00c2 Node contains a configured number of Worker Processes. Unlike the above grouping abstractions, this is the level that actually processes data. To load-balance among Worker\u00c2 Processes, Cribl\u00c2 Stream\u2019s API\u00c2 Process round-robins incoming connections to them. When deploying on AWS/EKS, Worker Groups should not span Availability\u00c2 Zones. If you have EBS persistent volumes, and a node fails, its replacement won\u2019t be able to access the peer volume across AZs. If and when you choose to shut down the infrastructure you\u2019ve configured for this demonstration: Navigate to Cribl\u00c2 Stream\u2019s default Group\u00c2 > Data\u00c2 > Sources\u00c2 > Datagen, and switch off the toggle beside weblog.log. If you configured a second Worker Group: Navigate to Cribl\u00c2 Stream\u2019s CriblMetrics Group\u00c2 > Data\u00c2 > Sources\u00c2 > Cribl\u00c2 Internal, and disable the toggle beside CriblMetrics. If you have netcat running on a Worker\u2019s terminal/console, ^C it. There\u2019s no need (or way) to switch off the DevNull Destination \u00e2\u0080\u0093 it just is. If desired, Commit and Deploy these changes. If you\u2019re running the Cribl\u00c2 Stream server(s) on cloud instances that will (ultimately) incur charges, you\u2019re now free to shut down those cloud resources. Interested in guided walk-throughs of more-advanced Cribl\u00c2 Stream features? We suggest that you next check out these further resources. Distributed Deployment: All the details behind the deployment approach you just mastered in this tutorial. Cribl\u00c2 Stream Sandboxes: Work through general and specific scenarios in a free, hosted environment, with terminal access and real data inputs and outputs. Use Cases documentation: Bring your own services to build solutions to specific challenges. Cribl Concept: Pipelines \u00e2\u0080\u0093 Video showing how to build and use Pipelines at multiple Cribl\u00c2 Stream stages. Cribl Concept: Routing \u00e2\u0080\u0093 Video about using Routes to send different data through different paths.\""
    },
    {
        "question_answer": {
            "question": "What is the purpose of using data cloning functionality on the F5 load balancer?",
            "answer": "The data cloning functionality on the F5 load balancer is being used to duplicate data to Cribl workers. The aim is to route, filter, and reduce production data in Cribl without impacting production operations. This way, when the time comes for a cutover, Cribl will be ready without much impact.",
            "follow_up_question_1": "What do you mean by 'cutover' in this context?",
            "follow_up_answer_1": "A 'cutover' in this context refers to the process of transitioning from using one system to another. In this case, it would be the transition from using syslog servers to using Cribl for data routing, filtering, and reduction.",
            "follow_up_question_2": "So, the data is being cloned from the F5 to Cribl workers, but it's not being picked up by Cribl. Could you explain why this might be happening?",
            "follow_up_answer_2": "The specific problem seems to be that while the data is reaching the correct interface and port on the worker, it's not being picked up by Cribl. The reason for this is not clear, but it could be due to a variety of factors such as the data being sent in the wrong format or a missing Linux configuration.",
            "follow_up_question_3": "Could you elaborate on what you mean by 'the data being sent in the wrong format'?",
            "follow_up_answer_3": "Data can be sent in various formats, and if the receiving system is not configured to interpret the incoming data format, it may not be able to process the data. In this case, it's possible that the F5 is sending data in a format that the Cribl worker is not configured to interpret, hence the data is not being picked up.",
            "url": "https://community.cribl.io/discussion/777/data-cloned-on-f5-and-sent-to-cribl-workers"
        },
        "reference": "url,title,text_chunks https://community.cribl.io/discussion/777/data-cloned-on-f5-and-sent-to-cribl-workers,Data cloned on F5 and sent to Cribl workers \u2014 Curious + Community,\"Posting this to get community feedback and track our progress in case anyone encounters this issue in the future. We are attempting to use the data cloning functionality on our F5 load balancer to duplicate data to our Cribl workers. We want to do this so that we can route, filter, and reduce production data in Cribl without impacting production operations. When the time comes, Cribl will be ready for the cutover without much impact at all, hopefully. Right now, the F5 is sending data to our syslog servers, which then send to Splunk. The end goal is to replace syslog with Cribl, but we want to be able to see all the data in Cribl before then so that we can operate on it. We are successfully cloning data that comes from Splunk UFs and HFs, but the cloned syslog data from the F5 is not being seen by the Cribl worker. The specific problem is this: The data is making it to the to the correct interface and port on the worker, but is not being picked up by Cribl for whatever reason. I have verified that data is coming in by using tcpdump to view the packets. We have turned off the local firewall on the server. There are no iptables rules dropping the traffic. I have verified \"\"net.ipv4.all.rp_filter\"\" is not present in sysctl. I'm not an F5 engineer, so I am mostly looking for feedback from that standpoint. Could the F5 be sending data in the wrong format, if that makes sense? My logic is that if the data is hitting the correct interface/port on the worker, then Cribl should be picking it up, but maybe that logic is flawed. Maybe there is a Linux configuration I am missing. Funny thing is, we can see the UDP port monitoring traffic that the F5 sends to the worker, but not the data that it is cloning and sending. \u00a9 Cribl 2023 | Legal | Code of Conduct\""
    },
    {
        "question_answer": {
            "question": "What is the end goal of this data cloning process?",
            "answer": "The end goal is to replace syslog with Cribl. The idea is to be able to see all the data in Cribl before then so that they can operate on it. This way, they can ensure a smooth transition with minimal impact on production operations.",
            "follow_up_question_1": "Why is it important to see all the data in Cribl before replacing syslog with it?",
            "follow_up_answer_1": "Seeing all the data in Cribl before replacing syslog with it allows for thorough testing and verification. This ensures that Cribl can handle the data load and perform the necessary operations correctly. It's a way to mitigate risks associated with the transition.",
            "follow_up_question_2": "What kind of operations are they planning to perform on the data in Cribl?",
            "follow_up_answer_2": "The operations they plan to perform on the data in Cribl include routing, filtering, and reducing the data. These operations are crucial for managing and optimizing the use of data in production operations.",
            "follow_up_question_3": "How does reducing data help in optimizing production operations?",
            "follow_up_answer_3": "Reducing data can help optimize production operations by minimizing the amount of data that needs to be processed and stored. This can lead to improved performance, lower storage costs, and more efficient use of resources.",
            "url": "https://community.cribl.io/discussion/777/data-cloned-on-f5-and-sent-to-cribl-workers"
        },
        "reference": "url,title,text_chunks https://community.cribl.io/discussion/777/data-cloned-on-f5-and-sent-to-cribl-workers,Data cloned on F5 and sent to Cribl workers \u2014 Curious + Community,\"Posting this to get community feedback and track our progress in case anyone encounters this issue in the future. We are attempting to use the data cloning functionality on our F5 load balancer to duplicate data to our Cribl workers. We want to do this so that we can route, filter, and reduce production data in Cribl without impacting production operations. When the time comes, Cribl will be ready for the cutover without much impact at all, hopefully. Right now, the F5 is sending data to our syslog servers, which then send to Splunk. The end goal is to replace syslog with Cribl, but we want to be able to see all the data in Cribl before then so that we can operate on it. We are successfully cloning data that comes from Splunk UFs and HFs, but the cloned syslog data from the F5 is not being seen by the Cribl worker. The specific problem is this: The data is making it to the to the correct interface and port on the worker, but is not being picked up by Cribl for whatever reason. I have verified that data is coming in by using tcpdump to view the packets. We have turned off the local firewall on the server. There are no iptables rules dropping the traffic. I have verified \"\"net.ipv4.all.rp_filter\"\" is not present in sysctl. I'm not an F5 engineer, so I am mostly looking for feedback from that standpoint. Could the F5 be sending data in the wrong format, if that makes sense? My logic is that if the data is hitting the correct interface/port on the worker, then Cribl should be picking it up, but maybe that logic is flawed. Maybe there is a Linux configuration I am missing. Funny thing is, we can see the UDP port monitoring traffic that the F5 sends to the worker, but not the data that it is cloning and sending. \u00a9 Cribl 2023 | Legal | Code of Conduct\""
    },
    {
        "question_answer": {
            "question": "What is Cribl.Cloud?",
            "answer": "Cribl.Cloud is a cloud-based service that you can use without having to install anything, manage any infrastructure, or require a license. It also doesn't require any payment collection.",
            "follow_up_question_1": "That sounds convenient. So, does this mean I can use it right away without any setup?",
            "follow_up_answer_1": "Yes, that's correct. Since there's nothing to install and no infrastructure to manage, you can start using Cribl.Cloud immediately.",
            "follow_up_question_2": "That's great. But what about the license? Is it not required at all?",
            "follow_up_answer_2": "That's right. Cribl.Cloud doesn't require a license, making it even more accessible and easy to use.",
            "follow_up_question_3": "And what about payment? Is it free to use?",
            "follow_up_answer_3": "The information provided doesn't specify whether Cribl.Cloud is free or not. It only mentions that no payment is collected during the trial period.",
            "url": "https://cribl.io/cribl-cloud/try-cribl-cloud/"
        },
        "reference": "url,title,text_chunks https://cribl.io/cribl-cloud/try-cribl-cloud/,Free Cribl.Cloud Trial - Cribl,\"Check out the success story > Check out the success story > Nothing to install, no infrastructure to manage, no license required, no payment collected.\""
    },
    {
        "question_answer": {
            "question": "What can I expect from the Cribl.Cloud trial?",
            "answer": "The Cribl.Cloud trial allows you to use the service without any installation, infrastructure management, license requirement, or payment collection.",
            "follow_up_question_1": "So, I can just sign up and start using it?",
            "follow_up_answer_1": "Yes, that's correct. The trial is designed to be as hassle-free as possible, allowing you to start using Cribl.Cloud immediately.",
            "follow_up_question_2": "Do I need to provide any payment details for the trial?",
            "follow_up_answer_2": "No, you don't. The trial does not involve any payment collection.",
            "follow_up_question_3": "What happens after the trial period ends?",
            "follow_up_answer_3": "The provided content doesn't specify what happens after the trial period. It's best to check with Cribl for the specific details.",
            "url": "https://cribl.io/cribl-cloud/try-cribl-cloud/"
        },
        "reference": "url,title,text_chunks https://cribl.io/cribl-cloud/try-cribl-cloud/,Free Cribl.Cloud Trial - Cribl,\"Check out the success story > Check out the success story > Nothing to install, no infrastructure to manage, no license required, no payment collected.\""
    },
    {
        "question_answer": {
            "question": "What value would Stream provide to a customer who already has a well-maintained and functioning logstash for routing/transforming data?",
            "answer": "Stream, a product by Cribl, provides several advantages over logstash. It offers an out-of-the-box solution, eliminating the need to build configurations manually. It also provides reduction use cases such as Suppress, Sample, Drop, and log to metrics. Additionally, it allows for the replay of historical data, enrichment with Lookup, and reshaping for Elastic SIEM.",
            "follow_up_question_1": "Can you elaborate on the out-of-the-box solution that Cribl provides?",
            "follow_up_answer_1": "Certainly. An out-of-the-box solution refers to a product or service that meets the needs of the user without requiring any additional configuration or customization. In the case of Cribl, this means that users can start routing and transforming data immediately, without having to manually build configurations like they would with logstash.",
            "follow_up_question_2": "What are these reduction use cases you mentioned?",
            "follow_up_answer_2": "Reduction use cases in Cribl refer to the ability to manage and reduce the volume of data. This includes suppressing unnecessary data, sampling data for analysis, dropping irrelevant data, and converting log data to metrics. These features help in optimizing data processing and storage.",
            "follow_up_question_3": "How does the replay of historical data work?",
            "follow_up_answer_3": "Replaying historical data allows users to reprocess and analyze past data. This can be useful in various scenarios, such as when new insights are needed, when there are changes in data processing rules, or when errors need to be corrected. With Cribl, users can easily replay their historical data.",
            "url": "https://community.cribl.io/discussion/782/what-value-would-stream-provide-to-someone-who-has-logstash-for-routing-transforming"
        },
        "reference": "url,title,text_chunks https://community.cribl.io/discussion/782/what-value-would-stream-provide-to-someone-who-has-logstash-for-routing-transforming,What value would Stream provide to someone who has logstash for routing/transforming? \u2014 Curious + Community,\"So, this question has been bothering me for quite some time now. While I am a big fan of Cribl and I really enjoy working with their products and showing/explaining them to others I still wonder every now and then what value Stream would provide to a customer, who already has a well-maintained and functioning logstash for routing/transforming data. If I think about it the following points come to my mind, but if someone here has more/different reasons I would be glad to hear them! Overall, I would go after these use cases: Build configurations manually (logstash) vs out of the box solution (Cribl) Reduction use cases (Suppress, Sample, Drop, log to metrics) Replay historical data Enrich with Lookup Reshape for Elastic SIEM https://www.elastic.co/blog/elastic-cribl-migrate-siem https://cribl.io/customers/sally-beauty/ https://cribl.io/blog/cribl-logstream-7x-more-efficient-than-logstash-and-fluentd/ Overall, I would go after these use cases: Build configurations manually (logstash) vs out of the box solution (Cribl) Reduction use cases (Suppress, Sample, Drop, log to metrics) Replay historical data Enrich with Lookup Reshape for Elastic SIEM https://www.elastic.co/blog/elastic-cribl-migrate-siem https://cribl.io/customers/sally-beauty/ https://cribl.io/blog/cribl-logstream-7x-more-efficient-than-logstash-and-fluentd/ \u00a9 Cribl 2023 | Legal | Code of Conduct\""
    },
    {
        "question_answer": {
            "question": "How does Cribl's Stream compare to logstash in terms of efficiency?",
            "answer": "According to a blog post by Cribl, Stream is 7x more efficient than logstash and fluentd. This means it can process data more quickly and use fewer resources, which can lead to significant cost savings.",
            "follow_up_question_1": "How does this increased efficiency impact the overall performance?",
            "follow_up_answer_1": "The increased efficiency means that Stream can process larger volumes of data in less time. This can lead to faster insights and decision-making. Additionally, because it uses fewer resources, it can help reduce infrastructure costs and improve the overall performance of your data processing systems.",
            "follow_up_question_2": "Does this efficiency translate to all types of data or specific ones?",
            "follow_up_answer_2": "The efficiency of Stream applies to all types of data it processes. Whether it's log data, metrics, or any other type of data, Stream is designed to handle it efficiently and effectively.",
            "follow_up_question_3": "Are there any case studies or examples of companies that have benefited from this efficiency?",
            "follow_up_answer_3": "Yes, Sally Beauty is one such company that has benefited from using Cribl. You can find more about their experience on the Cribl's customer page.",
            "url": "https://community.cribl.io/discussion/782/what-value-would-stream-provide-to-someone-who-has-logstash-for-routing-transforming"
        },
        "reference": "url,title,text_chunks https://community.cribl.io/discussion/782/what-value-would-stream-provide-to-someone-who-has-logstash-for-routing-transforming,What value would Stream provide to someone who has logstash for routing/transforming? \u2014 Curious + Community,\"So, this question has been bothering me for quite some time now. While I am a big fan of Cribl and I really enjoy working with their products and showing/explaining them to others I still wonder every now and then what value Stream would provide to a customer, who already has a well-maintained and functioning logstash for routing/transforming data. If I think about it the following points come to my mind, but if someone here has more/different reasons I would be glad to hear them! Overall, I would go after these use cases: Build configurations manually (logstash) vs out of the box solution (Cribl) Reduction use cases (Suppress, Sample, Drop, log to metrics) Replay historical data Enrich with Lookup Reshape for Elastic SIEM https://www.elastic.co/blog/elastic-cribl-migrate-siem https://cribl.io/customers/sally-beauty/ https://cribl.io/blog/cribl-logstream-7x-more-efficient-than-logstash-and-fluentd/ Overall, I would go after these use cases: Build configurations manually (logstash) vs out of the box solution (Cribl) Reduction use cases (Suppress, Sample, Drop, log to metrics) Replay historical data Enrich with Lookup Reshape for Elastic SIEM https://www.elastic.co/blog/elastic-cribl-migrate-siem https://cribl.io/customers/sally-beauty/ https://cribl.io/blog/cribl-logstream-7x-more-efficient-than-logstash-and-fluentd/ \u00a9 Cribl 2023 | Legal | Code of Conduct\""
    },
    {
        "question_answer": {
            "question": "What is Cribl Stream and what does it do?",
            "answer": "Cribl Stream is a vendor-agnostic observability pipeline that provides flexibility to collect, reduce, enrich, normalize, and route data from any source to any destination within your existing data infrastructure.",
            "follow_up_question_1": "What does it mean when you say it's 'vendor-agnostic'?",
            "follow_up_answer_1": "Being 'vendor-agnostic' means that Cribl Stream can work with any vendor's products or systems. It's not limited or biased towards a specific vendor's technology or design.",
            "follow_up_question_2": "How does this flexibility benefit the users?",
            "follow_up_answer_2": "This flexibility allows users to integrate Cribl Stream into their existing data infrastructure, regardless of the systems or technologies they're currently using. It also enables them to route data from any source to any destination, providing a seamless data flow.",
            "follow_up_question_3": "Can you give an example of how Cribl Stream has been used effectively?",
            "follow_up_answer_3": "Sure, Vodafone used Cribl Stream to dial up their business insights. It helped them manage their data more effectively and gain valuable insights.",
            "url": "https://cribl.io/cribl-community-code-of-conduct/"
        },
        "reference": "url,title,text_chunks https://cribl.io/cribl-community-code-of-conduct/,Cribl Community Code of Conduct - Cribl,\"New to observability? Find out everything you need to know. Telemetry 101 Understanding the Basics of Telemetry and Its Benefits Cribl Stream is a vendor-agnostic observability pipeline that gives you the flexibility to collect, reduce, enrich, normalize, and route data from any source to any destination within your existing data infrastructure. Vodafone Case Study Vodafone Dials up Business Insights with Cribl Stream Cribl Edge provides an intelligent, highly scalable edge-based data collection system for logs, metrics, and application data. SpyCloud Edge Story Listen to how SpyCloud uses Cribl Edge at scale. Cribl Search turns the traditional search process on its head, allowing users to search data in place without having to collect/store first. Happy 1st Birthday Cribl Search! The Cribl.Cloud platform gets you up and running fast without the hassle of running infrastructure. Cribl.Cloud Solution Brief The fastest and easiest way to realize the value of an observability ecosystem. AppScope gives operators the visibility they need into application behavior, metrics and events with no configuration and no agent required. Sandbox Launch an AppScope Sandbox today! Explore Cribl\u2019s Solutions by Use Cases: Explore Cribl\u2019s Solutions by Integrations: Seamless Integrations for Your Observability Data Explore Cribl\u2019s Solutions by Industry: Get this Gartner\u00ae report and learn why telemetry pipeline solutions represent a robust and largely untapped source of business insight beyond event and incident response. CriblCon 2023 Escaping Data Lock-In Amidst Industry Takeovers Learn how IT & Security engineers increase resilience & provide more options for analysis to make decisions faster with better data. Try Your Own Cribl Sandbox Experience a full version of Cribl Stream and Cribl Edge in the cloud. Download Cribl\u2019s suite of products for free to get started. Get inspired by how our customers are innovating IT, security and observability. They inspire us daily! Sally Beauty Holdings Sally Beauty Swaps LogStash and Syslog-ng with Cribl.Cloud for a Resilient Security and Observability Pipeline Professional Services Check out our new Professional Services offering. Experience a full version of Cribl Stream and Cribl Edge in the cloud. Take Control of Your Observability Data with Cribl Cribl Corporate Overview Cribl makes open observability a reality, giving you the freedom and flexibility to make choices instead of compromises. Stay up to date on all things Cribl and observability. Press Releases Read our most recent press releases. Cribl\u2019s leadership team has built and launched category-defining products for some of the most innovative companies in the technology sector, and is supported by the world\u2019s most elite investors. Join the Cribl herd! The smartest, funniest, most passionate goats you\u2019ll ever meet. Cribl Named to the Inc. 5000 List of Fastest Growing Private Companies Want to learn more about Cribl from our sales experts? Send us your contact information and we\u2019ll be in touch. This community is dedicated to providing a harassment-free experience for everyone. We do not tolerate harassment of participants in any form. All participants in the Cribl User Community are subject to these rules. These rules are enforced by Cribl employees/administrators, aka the Cribl team. (Cribl employees include the word \u201ccribl\u201d in their usernames so you can identify them.) Applicability and scope This code of conduct applies to all Cribl Community spaces and public channels, private channels, and direct messages, both online and off, as well as all Cribl-sponsored in-person events, including user group events and user conferences. When you register for any Cribl-sponsored event, you agree to comply with all rules and requirements provided by venues hosting Cribl-sponsored in-person events. Anyone who violates this code of conduct may be sanctioned or expelled from these spaces at the discretion of the Cribl team. Creating a welcoming and safe environment We hope to create an environment in which diverse individuals can collaborate and interact in a positive and affirming way. Examples of behavior that contributes to creating this sort of environment include: What we mean when we say \u201charassment\u201d The Cribl community will not tolerate harassment of any kind. Examples of harassment include: If you have questions or concerns about these rules, message a Cribl employee or email community@cribl.io. How to report an issue If you are being harassed by a member of our community, notice that someone else is being harassed, or have any other concerns, please notify a Cribl employee via Direct Message. Note: If the person who is harassing you is a Cribl employee, they will not be involved in handling or resolving the incident. The Cribl team will respond to any complaint as promptly as we can. If you do not get a timely response (for example, if no Cribl employees are currently online) then please put your personal safety and well-being first, and consider logging out and/or contacting us by email at community@cribl.io. This code of conduct applies to our community\u2019s spaces, but if you are being harassed by a member of our community outside our spaces, we still want to know about it. We will take all good-faith reports of harassment by our members seriously. This includes harassment outside our spaces and harassment that took place at any point in time. The Cribl team reserves the right to exclude people from the Cribl community based on their past behavior, including behavior outside of our spaces and behavior towards people who are not in this community. In order to protect admins from abuse and burnout, we reserve the right to reject any report we believe to have been made in bad faith. Reports intended to silence legitimate criticism may be deleted or ignored without response. How we will enforce these rules Every code of conduct violation report will be treated with seriousness and care. If a member\u2019s immediate safety and security is threatened, an individual Cribl employee may take any action that they deem appropriate, up to and including temporarily banning the offender from the Cribl community. In less immediate situations, at least two Cribl employees will discuss the offense and mutually arrive at a suitable response, which will be shared with the offender privately. Whatever the resolution that they decide upon, which may include permanent expulsion of the community member, the decision of the Cribl employees involved in a violation case will be considered final. We will respect confidentiality requests for the purpose of protecting victims of abuse. At our discretion, we may publicly name a person about whom we\u2019ve received harassment complaints, or privately warn third parties about them, if we believe that doing so will increase the safety of our members or the general public. We will not name harassment victims without their affirmative consent. Consequences Participants asked to stop any harassing behavior are expected to comply immediately. If a participant engages in harassing behavior, we may take any action they deem appropriate, up to and including permanent expulsion from the Cribl community and identification of the participant as a harasser to other members. At the discretion of the Cribl team or by request, one or more of the parties involved may request to discuss the violation and how to avoid similar situations in the future. Acknowledgements This Code of Conduct is adapted from the Community Covenant ( http://community-covenant.net ) version 1.0, available at http://community-covenant.net/version/1/0/ . The Community Covenant is an open source effort and is built on codes of conduct that came before it, including the Contributor Covenant (http://contributor-covenant.org/) and the LGBTQ in Tech community code of conduct ( http://lgbtq.technology/coc.html ). License Community Covenant by Coraline Ada Ehmke is licensed under a Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/). Based on a work at http://community-covenant.net/. \u00a92018 \u2013 2023 Cribl, Inc. | Legal\""
    },
    {
        "question_answer": {
            "question": "What is Cribl Edge and how does it work?",
            "answer": "Cribl Edge is an intelligent, highly scalable edge-based data collection system for logs, metrics, and application data.",
            "follow_up_question_1": "What are the benefits of using an edge-based data collection system?",
            "follow_up_answer_1": "An edge-based data collection system like Cribl Edge allows for data processing closer to the source of data generation. This can lead to reduced latency, improved performance, and lower costs as less data needs to be sent over the network.",
            "follow_up_question_2": "Can you provide an example of a company that has successfully used Cribl Edge?",
            "follow_up_answer_2": "Yes, SpyCloud is a company that has successfully used Cribl Edge at scale. They've been able to effectively collect and manage their logs, metrics, and application data.",
            "follow_up_question_3": "What makes Cribl Edge scalable?",
            "follow_up_answer_3": "Cribl Edge is designed to handle large volumes of data, making it highly scalable. It can easily adjust to the increasing amounts of data without compromising performance or efficiency.",
            "url": "https://cribl.io/cribl-community-code-of-conduct/"
        },
        "reference": "url,title,text_chunks https://cribl.io/cribl-community-code-of-conduct/,Cribl Community Code of Conduct - Cribl,\"New to observability? Find out everything you need to know. Telemetry 101 Understanding the Basics of Telemetry and Its Benefits Cribl Stream is a vendor-agnostic observability pipeline that gives you the flexibility to collect, reduce, enrich, normalize, and route data from any source to any destination within your existing data infrastructure. Vodafone Case Study Vodafone Dials up Business Insights with Cribl Stream Cribl Edge provides an intelligent, highly scalable edge-based data collection system for logs, metrics, and application data. SpyCloud Edge Story Listen to how SpyCloud uses Cribl Edge at scale. Cribl Search turns the traditional search process on its head, allowing users to search data in place without having to collect/store first. Happy 1st Birthday Cribl Search! The Cribl.Cloud platform gets you up and running fast without the hassle of running infrastructure. Cribl.Cloud Solution Brief The fastest and easiest way to realize the value of an observability ecosystem. AppScope gives operators the visibility they need into application behavior, metrics and events with no configuration and no agent required. Sandbox Launch an AppScope Sandbox today! Explore Cribl\u2019s Solutions by Use Cases: Explore Cribl\u2019s Solutions by Integrations: Seamless Integrations for Your Observability Data Explore Cribl\u2019s Solutions by Industry: Get this Gartner\u00ae report and learn why telemetry pipeline solutions represent a robust and largely untapped source of business insight beyond event and incident response. CriblCon 2023 Escaping Data Lock-In Amidst Industry Takeovers Learn how IT & Security engineers increase resilience & provide more options for analysis to make decisions faster with better data. Try Your Own Cribl Sandbox Experience a full version of Cribl Stream and Cribl Edge in the cloud. Download Cribl\u2019s suite of products for free to get started. Get inspired by how our customers are innovating IT, security and observability. They inspire us daily! Sally Beauty Holdings Sally Beauty Swaps LogStash and Syslog-ng with Cribl.Cloud for a Resilient Security and Observability Pipeline Professional Services Check out our new Professional Services offering. Experience a full version of Cribl Stream and Cribl Edge in the cloud. Take Control of Your Observability Data with Cribl Cribl Corporate Overview Cribl makes open observability a reality, giving you the freedom and flexibility to make choices instead of compromises. Stay up to date on all things Cribl and observability. Press Releases Read our most recent press releases. Cribl\u2019s leadership team has built and launched category-defining products for some of the most innovative companies in the technology sector, and is supported by the world\u2019s most elite investors. Join the Cribl herd! The smartest, funniest, most passionate goats you\u2019ll ever meet. Cribl Named to the Inc. 5000 List of Fastest Growing Private Companies Want to learn more about Cribl from our sales experts? Send us your contact information and we\u2019ll be in touch. This community is dedicated to providing a harassment-free experience for everyone. We do not tolerate harassment of participants in any form. All participants in the Cribl User Community are subject to these rules. These rules are enforced by Cribl employees/administrators, aka the Cribl team. (Cribl employees include the word \u201ccribl\u201d in their usernames so you can identify them.) Applicability and scope This code of conduct applies to all Cribl Community spaces and public channels, private channels, and direct messages, both online and off, as well as all Cribl-sponsored in-person events, including user group events and user conferences. When you register for any Cribl-sponsored event, you agree to comply with all rules and requirements provided by venues hosting Cribl-sponsored in-person events. Anyone who violates this code of conduct may be sanctioned or expelled from these spaces at the discretion of the Cribl team. Creating a welcoming and safe environment We hope to create an environment in which diverse individuals can collaborate and interact in a positive and affirming way. Examples of behavior that contributes to creating this sort of environment include: What we mean when we say \u201charassment\u201d The Cribl community will not tolerate harassment of any kind. Examples of harassment include: If you have questions or concerns about these rules, message a Cribl employee or email community@cribl.io. How to report an issue If you are being harassed by a member of our community, notice that someone else is being harassed, or have any other concerns, please notify a Cribl employee via Direct Message. Note: If the person who is harassing you is a Cribl employee, they will not be involved in handling or resolving the incident. The Cribl team will respond to any complaint as promptly as we can. If you do not get a timely response (for example, if no Cribl employees are currently online) then please put your personal safety and well-being first, and consider logging out and/or contacting us by email at community@cribl.io. This code of conduct applies to our community\u2019s spaces, but if you are being harassed by a member of our community outside our spaces, we still want to know about it. We will take all good-faith reports of harassment by our members seriously. This includes harassment outside our spaces and harassment that took place at any point in time. The Cribl team reserves the right to exclude people from the Cribl community based on their past behavior, including behavior outside of our spaces and behavior towards people who are not in this community. In order to protect admins from abuse and burnout, we reserve the right to reject any report we believe to have been made in bad faith. Reports intended to silence legitimate criticism may be deleted or ignored without response. How we will enforce these rules Every code of conduct violation report will be treated with seriousness and care. If a member\u2019s immediate safety and security is threatened, an individual Cribl employee may take any action that they deem appropriate, up to and including temporarily banning the offender from the Cribl community. In less immediate situations, at least two Cribl employees will discuss the offense and mutually arrive at a suitable response, which will be shared with the offender privately. Whatever the resolution that they decide upon, which may include permanent expulsion of the community member, the decision of the Cribl employees involved in a violation case will be considered final. We will respect confidentiality requests for the purpose of protecting victims of abuse. At our discretion, we may publicly name a person about whom we\u2019ve received harassment complaints, or privately warn third parties about them, if we believe that doing so will increase the safety of our members or the general public. We will not name harassment victims without their affirmative consent. Consequences Participants asked to stop any harassing behavior are expected to comply immediately. If a participant engages in harassing behavior, we may take any action they deem appropriate, up to and including permanent expulsion from the Cribl community and identification of the participant as a harasser to other members. At the discretion of the Cribl team or by request, one or more of the parties involved may request to discuss the violation and how to avoid similar situations in the future. Acknowledgements This Code of Conduct is adapted from the Community Covenant ( http://community-covenant.net ) version 1.0, available at http://community-covenant.net/version/1/0/ . The Community Covenant is an open source effort and is built on codes of conduct that came before it, including the Contributor Covenant (http://contributor-covenant.org/) and the LGBTQ in Tech community code of conduct ( http://lgbtq.technology/coc.html ). License Community Covenant by Coraline Ada Ehmke is licensed under a Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/). Based on a work at http://community-covenant.net/. \u00a92018 \u2013 2023 Cribl, Inc. | Legal\""
    },
    {
        "question_answer": {
            "question": "What is Cribl Search and how does it differ from traditional search processes?",
            "answer": "Cribl Search turns the traditional search process on its head, allowing users to search data in place without having to collect/store first.",
            "follow_up_question_1": "How does this approach improve the search process?",
            "follow_up_answer_1": "This approach can significantly speed up the search process as it eliminates the need to collect and store data before searching. It also reduces the storage requirements and costs associated with traditional search processes.",
            "follow_up_question_2": "Can you give an example of how Cribl Search has been used effectively?",
            "follow_up_answer_2": "While a specific example isn't provided in the content, the fact that Cribl Search celebrated its 1st birthday suggests that it has been well-received and effectively used by customers over the past year.",
            "follow_up_question_3": "What kind of data can be searched using Cribl Search?",
            "follow_up_answer_3": "The content doesn't specify the types of data that can be searched using Cribl Search, but given that it's part of an observability pipeline, it's likely that it can handle a wide range of data types, including logs, metrics, and application data.",
            "url": "https://cribl.io/cribl-community-code-of-conduct/"
        },
        "reference": "url,title,text_chunks https://cribl.io/cribl-community-code-of-conduct/,Cribl Community Code of Conduct - Cribl,\"New to observability? Find out everything you need to know. Telemetry 101 Understanding the Basics of Telemetry and Its Benefits Cribl Stream is a vendor-agnostic observability pipeline that gives you the flexibility to collect, reduce, enrich, normalize, and route data from any source to any destination within your existing data infrastructure. Vodafone Case Study Vodafone Dials up Business Insights with Cribl Stream Cribl Edge provides an intelligent, highly scalable edge-based data collection system for logs, metrics, and application data. SpyCloud Edge Story Listen to how SpyCloud uses Cribl Edge at scale. Cribl Search turns the traditional search process on its head, allowing users to search data in place without having to collect/store first. Happy 1st Birthday Cribl Search! The Cribl.Cloud platform gets you up and running fast without the hassle of running infrastructure. Cribl.Cloud Solution Brief The fastest and easiest way to realize the value of an observability ecosystem. AppScope gives operators the visibility they need into application behavior, metrics and events with no configuration and no agent required. Sandbox Launch an AppScope Sandbox today! Explore Cribl\u2019s Solutions by Use Cases: Explore Cribl\u2019s Solutions by Integrations: Seamless Integrations for Your Observability Data Explore Cribl\u2019s Solutions by Industry: Get this Gartner\u00ae report and learn why telemetry pipeline solutions represent a robust and largely untapped source of business insight beyond event and incident response. CriblCon 2023 Escaping Data Lock-In Amidst Industry Takeovers Learn how IT & Security engineers increase resilience & provide more options for analysis to make decisions faster with better data. Try Your Own Cribl Sandbox Experience a full version of Cribl Stream and Cribl Edge in the cloud. Download Cribl\u2019s suite of products for free to get started. Get inspired by how our customers are innovating IT, security and observability. They inspire us daily! Sally Beauty Holdings Sally Beauty Swaps LogStash and Syslog-ng with Cribl.Cloud for a Resilient Security and Observability Pipeline Professional Services Check out our new Professional Services offering. Experience a full version of Cribl Stream and Cribl Edge in the cloud. Take Control of Your Observability Data with Cribl Cribl Corporate Overview Cribl makes open observability a reality, giving you the freedom and flexibility to make choices instead of compromises. Stay up to date on all things Cribl and observability. Press Releases Read our most recent press releases. Cribl\u2019s leadership team has built and launched category-defining products for some of the most innovative companies in the technology sector, and is supported by the world\u2019s most elite investors. Join the Cribl herd! The smartest, funniest, most passionate goats you\u2019ll ever meet. Cribl Named to the Inc. 5000 List of Fastest Growing Private Companies Want to learn more about Cribl from our sales experts? Send us your contact information and we\u2019ll be in touch. This community is dedicated to providing a harassment-free experience for everyone. We do not tolerate harassment of participants in any form. All participants in the Cribl User Community are subject to these rules. These rules are enforced by Cribl employees/administrators, aka the Cribl team. (Cribl employees include the word \u201ccribl\u201d in their usernames so you can identify them.) Applicability and scope This code of conduct applies to all Cribl Community spaces and public channels, private channels, and direct messages, both online and off, as well as all Cribl-sponsored in-person events, including user group events and user conferences. When you register for any Cribl-sponsored event, you agree to comply with all rules and requirements provided by venues hosting Cribl-sponsored in-person events. Anyone who violates this code of conduct may be sanctioned or expelled from these spaces at the discretion of the Cribl team. Creating a welcoming and safe environment We hope to create an environment in which diverse individuals can collaborate and interact in a positive and affirming way. Examples of behavior that contributes to creating this sort of environment include: What we mean when we say \u201charassment\u201d The Cribl community will not tolerate harassment of any kind. Examples of harassment include: If you have questions or concerns about these rules, message a Cribl employee or email community@cribl.io. How to report an issue If you are being harassed by a member of our community, notice that someone else is being harassed, or have any other concerns, please notify a Cribl employee via Direct Message. Note: If the person who is harassing you is a Cribl employee, they will not be involved in handling or resolving the incident. The Cribl team will respond to any complaint as promptly as we can. If you do not get a timely response (for example, if no Cribl employees are currently online) then please put your personal safety and well-being first, and consider logging out and/or contacting us by email at community@cribl.io. This code of conduct applies to our community\u2019s spaces, but if you are being harassed by a member of our community outside our spaces, we still want to know about it. We will take all good-faith reports of harassment by our members seriously. This includes harassment outside our spaces and harassment that took place at any point in time. The Cribl team reserves the right to exclude people from the Cribl community based on their past behavior, including behavior outside of our spaces and behavior towards people who are not in this community. In order to protect admins from abuse and burnout, we reserve the right to reject any report we believe to have been made in bad faith. Reports intended to silence legitimate criticism may be deleted or ignored without response. How we will enforce these rules Every code of conduct violation report will be treated with seriousness and care. If a member\u2019s immediate safety and security is threatened, an individual Cribl employee may take any action that they deem appropriate, up to and including temporarily banning the offender from the Cribl community. In less immediate situations, at least two Cribl employees will discuss the offense and mutually arrive at a suitable response, which will be shared with the offender privately. Whatever the resolution that they decide upon, which may include permanent expulsion of the community member, the decision of the Cribl employees involved in a violation case will be considered final. We will respect confidentiality requests for the purpose of protecting victims of abuse. At our discretion, we may publicly name a person about whom we\u2019ve received harassment complaints, or privately warn third parties about them, if we believe that doing so will increase the safety of our members or the general public. We will not name harassment victims without their affirmative consent. Consequences Participants asked to stop any harassing behavior are expected to comply immediately. If a participant engages in harassing behavior, we may take any action they deem appropriate, up to and including permanent expulsion from the Cribl community and identification of the participant as a harasser to other members. At the discretion of the Cribl team or by request, one or more of the parties involved may request to discuss the violation and how to avoid similar situations in the future. Acknowledgements This Code of Conduct is adapted from the Community Covenant ( http://community-covenant.net ) version 1.0, available at http://community-covenant.net/version/1/0/ . The Community Covenant is an open source effort and is built on codes of conduct that came before it, including the Contributor Covenant (http://contributor-covenant.org/) and the LGBTQ in Tech community code of conduct ( http://lgbtq.technology/coc.html ). License Community Covenant by Coraline Ada Ehmke is licensed under a Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/). Based on a work at http://community-covenant.net/. \u00a92018 \u2013 2023 Cribl, Inc. | Legal\""
    },
    {
        "question_answer": {
            "question": "What is the purpose of the General Discussions category on the Cribl Community site?",
            "answer": "The General Discussions category on the Cribl Community site is a platform for users to engage in various topics that may not fit into other specific categories. It's a space for open-ended conversations, questions, and sharing of ideas.",
            "follow_up_question_1": "What kind of topics can be discussed in this category?",
            "follow_up_answer_1": "Given its general nature, a wide range of topics can be discussed. These could include questions about Cribl's products, sharing of use-cases, discussions on data processing strategies, or even broader topics related to the data industry.",
            "follow_up_question_2": "Are there any guidelines or rules for posting in the General Discussions category?",
            "follow_up_answer_2": "While the specific rules aren't mentioned in the provided content, most online communities have guidelines to ensure respectful and productive discussions. These typically include rules against spamming, harassment, and off-topic posts. It's always a good idea to check the community's Code of Conduct or similar document before posting.",
            "follow_up_question_3": "What is the benefit of having such a General Discussions category in a community like this?",
            "follow_up_answer_3": "A General Discussions category provides a space for users to engage in conversations that don't necessarily fit into other categories. This can foster a sense of community, encourage knowledge sharing, and provide a platform for users to voice their thoughts and ideas.",
            "url": "https://community.cribl.io/categories/general-discussions"
        },
        "reference": "url,title,text_chunks https://community.cribl.io/categories/general-discussions,General Discussions \u2014 Curious + Community,\u00a9 Cribl 2023 | Legal | Code of Conduct"
    },
    {
        "question_answer": {
            "question": "What is the Code of Conduct mentioned on the Cribl Community site?",
            "answer": "The Code of Conduct is a set of guidelines that outlines the expected behavior of community members. While the specific details aren't provided in the content, it typically includes rules about respect, harassment, and appropriate content.",
            "follow_up_question_1": "Why is it important for an online community to have a Code of Conduct?",
            "follow_up_answer_1": "A Code of Conduct is crucial for maintaining a respectful and inclusive environment in an online community. It sets clear expectations for behavior, helps prevent conflicts, and provides a basis for moderating discussions and dealing with any issues that arise.",
            "follow_up_question_2": "What happens if someone violates the Code of Conduct?",
            "follow_up_answer_2": "While the specific consequences aren't mentioned in the provided content, typically, violations of the Code of Conduct can result in actions ranging from warnings to temporary or permanent bans from the community, depending on the severity and frequency of the violations.",
            "follow_up_question_3": "Is the Code of Conduct applicable to all categories in the community or just the General Discussions?",
            "follow_up_answer_3": "Typically, a community's Code of Conduct applies to all interactions within the community, not just specific categories. This includes posts, comments, private messages, and any other form of communication on the platform.",
            "url": "https://community.cribl.io/categories/general-discussions"
        },
        "reference": "url,title,text_chunks https://community.cribl.io/categories/general-discussions,General Discussions \u2014 Curious + Community,\u00a9 Cribl 2023 | Legal | Code of Conduct"
    },
    {
        "question_answer": {
            "question": "What is Cribl Stream and what does it do?",
            "answer": "Cribl Stream is a vendor-agnostic observability pipeline that provides the flexibility to collect, reduce, enrich, normalize, and route data from any source to any destination within your existing data infrastructure.",
            "follow_up_question_1": "What does it mean when you say it's 'vendor-agnostic'?",
            "follow_up_answer_1": "Being 'vendor-agnostic' means that Cribl Stream can work with any data source or destination, regardless of the vendor. It's not tied to a specific manufacturer or provider, giving users more flexibility and choice.",
            "follow_up_question_2": "How does this flexibility benefit the users?",
            "follow_up_answer_2": "This flexibility allows users to integrate Cribl Stream into their existing data infrastructure without having to change their current systems or processes. It also enables them to route data to any destination they choose, which can be particularly useful for businesses that use multiple data storage or analysis tools.",
            "follow_up_question_3": "Can you give an example of how a company has used Cribl Stream?",
            "follow_up_answer_3": "Sure, Vodafone is a great example. They used Cribl Stream to dial up their business insights. It helped them manage their data more effectively and gain valuable insights to drive their business forward.",
            "url": "https://cribl.io/community/cribl-community-group/"
        },
        "reference": "url,title,text_chunks https://cribl.io/community/cribl-community-group/,Connecting with Monthly Community User Group Meetings | Cribl,\"New to observability? Find out everything you need to know. Telemetry 101 Understanding the Basics of Telemetry and Its Benefits Cribl Stream is a vendor-agnostic observability pipeline that gives you the flexibility to collect, reduce, enrich, normalize, and route data from any source to any destination within your existing data infrastructure. Vodafone Case Study Vodafone Dials up Business Insights with Cribl Stream Cribl Edge provides an intelligent, highly scalable edge-based data collection system for logs, metrics, and application data. SpyCloud Edge Story Listen to how SpyCloud uses Cribl Edge at scale. Cribl Search turns the traditional search process on its head, allowing users to search data in place without having to collect/store first. Happy 1st Birthday Cribl Search! The Cribl.Cloud platform gets you up and running fast without the hassle of running infrastructure. Cribl.Cloud Solution Brief The fastest and easiest way to realize the value of an observability ecosystem. AppScope gives operators the visibility they need into application behavior, metrics and events with no configuration and no agent required. Sandbox Launch an AppScope Sandbox today! Explore Cribl\u2019s Solutions by Use Cases: Explore Cribl\u2019s Solutions by Integrations: Seamless Integrations for Your Observability Data Explore Cribl\u2019s Solutions by Industry: Get this Gartner\u00ae report and learn why telemetry pipeline solutions represent a robust and largely untapped source of business insight beyond event and incident response. CriblCon 2023 Escaping Data Lock-In Amidst Industry Takeovers Learn how IT & Security engineers increase resilience & provide more options for analysis to make decisions faster with better data. Try Your Own Cribl Sandbox Experience a full version of Cribl Stream and Cribl Edge in the cloud. Download Cribl\u2019s suite of products for free to get started. Get inspired by how our customers are innovating IT, security and observability. They inspire us daily! Sally Beauty Holdings Sally Beauty Swaps LogStash and Syslog-ng with Cribl.Cloud for a Resilient Security and Observability Pipeline Professional Services Check out our new Professional Services offering. Experience a full version of Cribl Stream and Cribl Edge in the cloud. Take Control of Your Observability Data with Cribl Cribl Corporate Overview Cribl makes open observability a reality, giving you the freedom and flexibility to make choices instead of compromises. Stay up to date on all things Cribl and observability. Press Releases Read our most recent press releases. Cribl\u2019s leadership team has built and launched category-defining products for some of the most innovative companies in the technology sector, and is supported by the world\u2019s most elite investors. Join the Cribl herd! The smartest, funniest, most passionate goats you\u2019ll ever meet. Cribl Named to the Inc. 5000 List of Fastest Growing Private Companies Want to learn more about Cribl from our sales experts? Send us your contact information and we\u2019ll be in touch. \u00a92018 \u2013 2023 Cribl, Inc. | Legal\""
    },
    {
        "question_answer": {
            "question": "What is Cribl Edge and how does it work?",
            "answer": "Cribl Edge is an intelligent, highly scalable edge-based data collection system for logs, metrics, and application data. It's designed to handle large volumes of data and provide real-time insights.",
            "follow_up_question_1": "What are the benefits of using an edge-based data collection system like this?",
            "follow_up_answer_1": "An edge-based system like Cribl Edge allows for data processing closer to the source, which can reduce latency and bandwidth usage. This can lead to faster, more efficient data collection and analysis.",
            "follow_up_question_2": "Can you provide an example of a company that has used Cribl Edge?",
            "follow_up_answer_2": "Yes, SpyCloud is a company that has used Cribl Edge at scale. They were able to collect and process large amounts of data efficiently, which helped them in their operations.",
            "follow_up_question_3": "How does Cribl Edge integrate with other Cribl products?",
            "follow_up_answer_3": "Cribl Edge integrates seamlessly with other Cribl products like Cribl Stream and Cribl Search. This allows for a unified and efficient data management and observability solution.",
            "url": "https://cribl.io/community/cribl-community-group/"
        },
        "reference": "url,title,text_chunks https://cribl.io/community/cribl-community-group/,Connecting with Monthly Community User Group Meetings | Cribl,\"New to observability? Find out everything you need to know. Telemetry 101 Understanding the Basics of Telemetry and Its Benefits Cribl Stream is a vendor-agnostic observability pipeline that gives you the flexibility to collect, reduce, enrich, normalize, and route data from any source to any destination within your existing data infrastructure. Vodafone Case Study Vodafone Dials up Business Insights with Cribl Stream Cribl Edge provides an intelligent, highly scalable edge-based data collection system for logs, metrics, and application data. SpyCloud Edge Story Listen to how SpyCloud uses Cribl Edge at scale. Cribl Search turns the traditional search process on its head, allowing users to search data in place without having to collect/store first. Happy 1st Birthday Cribl Search! The Cribl.Cloud platform gets you up and running fast without the hassle of running infrastructure. Cribl.Cloud Solution Brief The fastest and easiest way to realize the value of an observability ecosystem. AppScope gives operators the visibility they need into application behavior, metrics and events with no configuration and no agent required. Sandbox Launch an AppScope Sandbox today! Explore Cribl\u2019s Solutions by Use Cases: Explore Cribl\u2019s Solutions by Integrations: Seamless Integrations for Your Observability Data Explore Cribl\u2019s Solutions by Industry: Get this Gartner\u00ae report and learn why telemetry pipeline solutions represent a robust and largely untapped source of business insight beyond event and incident response. CriblCon 2023 Escaping Data Lock-In Amidst Industry Takeovers Learn how IT & Security engineers increase resilience & provide more options for analysis to make decisions faster with better data. Try Your Own Cribl Sandbox Experience a full version of Cribl Stream and Cribl Edge in the cloud. Download Cribl\u2019s suite of products for free to get started. Get inspired by how our customers are innovating IT, security and observability. They inspire us daily! Sally Beauty Holdings Sally Beauty Swaps LogStash and Syslog-ng with Cribl.Cloud for a Resilient Security and Observability Pipeline Professional Services Check out our new Professional Services offering. Experience a full version of Cribl Stream and Cribl Edge in the cloud. Take Control of Your Observability Data with Cribl Cribl Corporate Overview Cribl makes open observability a reality, giving you the freedom and flexibility to make choices instead of compromises. Stay up to date on all things Cribl and observability. Press Releases Read our most recent press releases. Cribl\u2019s leadership team has built and launched category-defining products for some of the most innovative companies in the technology sector, and is supported by the world\u2019s most elite investors. Join the Cribl herd! The smartest, funniest, most passionate goats you\u2019ll ever meet. Cribl Named to the Inc. 5000 List of Fastest Growing Private Companies Want to learn more about Cribl from our sales experts? Send us your contact information and we\u2019ll be in touch. \u00a92018 \u2013 2023 Cribl, Inc. | Legal\""
    },
    {
        "question_answer": {
            "question": "What is Cribl Search and how does it differ from traditional search processes?",
            "answer": "Cribl Search turns the traditional search process on its head by allowing users to search data in place without having to collect or store it first. This can make the search process faster and more efficient.",
            "follow_up_question_1": "How does this in-place search work?",
            "follow_up_answer_1": "In-place search works by querying the data directly where it resides, without the need to move it to a central location for processing. This can significantly reduce the time and resources needed for data collection and storage.",
            "follow_up_question_2": "What are the benefits of this approach?",
            "follow_up_answer_2": "This approach can lead to faster search results, as there's no need to wait for data to be collected and stored before it can be searched. It also reduces the storage requirements, which can lead to cost savings.",
            "follow_up_question_3": "Can you give an example of how a company has benefited from using Cribl Search?",
            "follow_up_answer_3": "While the content doesn't provide a specific example for Cribl Search, it's safe to say that any company dealing with large volumes of data could benefit from its in-place search capabilities. By reducing the need for data collection and storage, companies can save time and resources while still gaining the insights they need.",
            "url": "https://cribl.io/community/cribl-community-group/"
        },
        "reference": "url,title,text_chunks https://cribl.io/community/cribl-community-group/,Connecting with Monthly Community User Group Meetings | Cribl,\"New to observability? Find out everything you need to know. Telemetry 101 Understanding the Basics of Telemetry and Its Benefits Cribl Stream is a vendor-agnostic observability pipeline that gives you the flexibility to collect, reduce, enrich, normalize, and route data from any source to any destination within your existing data infrastructure. Vodafone Case Study Vodafone Dials up Business Insights with Cribl Stream Cribl Edge provides an intelligent, highly scalable edge-based data collection system for logs, metrics, and application data. SpyCloud Edge Story Listen to how SpyCloud uses Cribl Edge at scale. Cribl Search turns the traditional search process on its head, allowing users to search data in place without having to collect/store first. Happy 1st Birthday Cribl Search! The Cribl.Cloud platform gets you up and running fast without the hassle of running infrastructure. Cribl.Cloud Solution Brief The fastest and easiest way to realize the value of an observability ecosystem. AppScope gives operators the visibility they need into application behavior, metrics and events with no configuration and no agent required. Sandbox Launch an AppScope Sandbox today! Explore Cribl\u2019s Solutions by Use Cases: Explore Cribl\u2019s Solutions by Integrations: Seamless Integrations for Your Observability Data Explore Cribl\u2019s Solutions by Industry: Get this Gartner\u00ae report and learn why telemetry pipeline solutions represent a robust and largely untapped source of business insight beyond event and incident response. CriblCon 2023 Escaping Data Lock-In Amidst Industry Takeovers Learn how IT & Security engineers increase resilience & provide more options for analysis to make decisions faster with better data. Try Your Own Cribl Sandbox Experience a full version of Cribl Stream and Cribl Edge in the cloud. Download Cribl\u2019s suite of products for free to get started. Get inspired by how our customers are innovating IT, security and observability. They inspire us daily! Sally Beauty Holdings Sally Beauty Swaps LogStash and Syslog-ng with Cribl.Cloud for a Resilient Security and Observability Pipeline Professional Services Check out our new Professional Services offering. Experience a full version of Cribl Stream and Cribl Edge in the cloud. Take Control of Your Observability Data with Cribl Cribl Corporate Overview Cribl makes open observability a reality, giving you the freedom and flexibility to make choices instead of compromises. Stay up to date on all things Cribl and observability. Press Releases Read our most recent press releases. Cribl\u2019s leadership team has built and launched category-defining products for some of the most innovative companies in the technology sector, and is supported by the world\u2019s most elite investors. Join the Cribl herd! The smartest, funniest, most passionate goats you\u2019ll ever meet. Cribl Named to the Inc. 5000 List of Fastest Growing Private Companies Want to learn more about Cribl from our sales experts? Send us your contact information and we\u2019ll be in touch. \u00a92018 \u2013 2023 Cribl, Inc. | Legal\""
    }
]