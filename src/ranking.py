import nltk
import argparse
import asyncio
import wandb
import logging

from nltk.translate.bleu_score import sentence_bleu
from nltk.translate.bleu_score import SmoothingFunction
from nltk.translate.bleu_score import corpus_bleu
from nltk.translate.bleu_score import sentence_bleu
from nltk.translate.bleu_score import sentence_bleu
from nltk.translate.meteor_score import meteor_score
from nltk.translate.rouge_score import rouge_n, rouge_l, rouge_w
from typing import List
from .utils import read_endpoint_configurations, read_qa_data, get_llm_answer

# Make sure NLTK data is downloaded (required for METEOR and ROUGE)
nltk.download("wordnet")
nltk.download("punkt")

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
formatter = logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")
ch = logging.StreamHandler()
ch.setFormatter(formatter)
logger.addHandler(ch)


async def evaluate_qa_data(
    qa_data_path: str, qa_endpoints: str, wandb_log: bool = False
) -> None:
    """
    Evaluate the quality of answers generated by different endpoints for a given set of questions.

    Args:
        qa_data_path (str): The path to the JSON file containing the QA data.
        qa_endpoints (str): The path to the JSON file containing the endpoint configurations.
        wandb_log (bool): Whether to log the results to Weights & Biases.

    Returns:
        None
    """

    # Read and process endpoint configurations
    endpoint_configs = read_endpoint_configurations(qa_endpoints)

    # Read the qa dataset
    qa_data = read_qa_data(qa_data_path)

    for endpoint_config in endpoint_configs:
        logger.info(f"Endpoint Name: {endpoint_config['name']}")
        logger.info(f"Endpoint URL: {endpoint_config['url']}")
        logger.info()
    for entry in qa_data:
        question = entry.get("question", "")
        answer = entry.get("answer", "")
        logger.info(f"Question: {question}")
        logger.info(f"Answer: {answer}")

    question_ranking = []
    ranked_endpoints = []

    for entry in qa_data:
        question = entry.get("question", "")
        reference_answer = entry.get("answer", "")
        logger.info(f"Question: {question}")
        logger.info(f"Answer: {reference_answer}")
        logger.info()
        for endpoint_config in endpoint_configs:
            logger.info(f"Endpoint Name: {endpoint_config['name']}")
            logger.info(f"Endpoint URL: {endpoint_config['url']}")
            candidate = get_llm_answer(question, endpoint_config)
            logger.info(f"Answer: {candidate}")
            # Tokenize the sentences into lists of words
            reference_answer_tokens = nltk.word_tokenize(reference_answer.lower())
            candidate_tokens = nltk.word_tokenize(candidate.lower())

            # Calculate BLEU score
            bleu_score = sentence_bleu(
                [reference_answer_tokens],
                candidate_tokens,
                smoothing_function=SmoothingFunction().method4,
            )

            # Calculate ROUGE-L score
            rouge_l_score = rouge_l([reference_answer], [candidate])

            # Calculate METEOR score
            meteor_score = meteor_score([reference_answer], candidate)

            question_ranking.append(
                {
                    "endpoint_name": endpoint_config["name"],
                    "url": endpoint_config["url"],
                    "question": question,
                    "answer": reference_answer,
                    "rouge_l_score": rouge_l_score,
                    "bleu_score": bleu_score,
                    "meteor_score": meteor_score,
                }
            )

        # Sort the endpoints based on their scores (highest to lowest)
        question_ranking = sorted(
            question_ranking,
            key=lambda x: (-x["rouge_l_score"], -x["bleu_score"], -x["meteor_score"]),
        )
        ranked_endpoints.append({"question": question, "ranking": question_ranking})

    ranked_endpoints = sorted(
        ranked_endpoints,
        key=lambda x: (
            x["ranking"][0]["rouge_l_score"],
            x["ranking"][0]["bleu_score"],
            x["ranking"][0]["meteor_score"],
        ),
    )

    for ranking in ranked_endpoints:
        logger.info(f"Question: {ranking['question']}")
        for i, endpoint in enumerate(ranking["ranking"]):
            # log in wandb
            if wandb_log:
                wandb.log(
                    {
                        "question": ranking["question"],
                        "endpoint_name": endpoint["endpoint_name"],
                        "url": endpoint["url"],
                        "rouge_l_score": endpoint["rouge_l_score"],
                        "bleu_score": endpoint["bleu_score"],
                        "meteor_score": endpoint["meteor_score"],
                    }
                )
            logger.info(
                {
                    "question": ranking["question"],
                    "endpoint_name": endpoint["endpoint_name"],
                    "url": endpoint["url"],
                    "rouge_l_score": endpoint["rouge_l_score"],
                    "bleu_score": endpoint["bleu_score"],
                    "meteor_score": endpoint["meteor_score"],
                }
            )

        logger.info("Ranking is completed")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Ranking the endpoints for each question"
    )

    parser.add_argument("--qa_data_path", type=str, help="Path to the input data file")

    parser.add_argument("--qa_endpoints", type=str, help="LLM Endpoints")

    parser.add_argument(
        "--wandb_log", type=bool, default=False, help="wandb logging enabled"
    )

    args = parser.parse_args()

    asyncio.run(
        evaluate_qa_data(
            qa_data_path=args.qa_data_path,
            qa_endpoints=args.qa_endpoints,
            wandb_log=args.wandb_log,
        )
    )
